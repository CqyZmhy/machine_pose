{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 创作古诗\n",
    "在这一章中我们了解到循环神经网络非常擅长处理序列和自然语言处理，文本都是由单词或者汉字按照序列顺序组成的，那么如何能够生成文本呢？下面我们来讲一讲原理，需要你根据这个原理来实现整个网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理介绍\n",
    "前面我们介绍过 RNN 的输入和输出存在多种关系，比如多个输入对一个输出，这个时候输入是一个序列，输出是一个分类结果，就像使用 RNN 做图像分类。\n",
    "\n",
    "这里我们使用 RNN 来生成文本，网络的输入是一个序列，同时输出也是一个相同长度的序列，结构如下\n",
    "\n",
    "<img src=https://ws1.sinaimg.cn/large/006tNc79gy1fob5kq3r8jj30mt09dq2r.jpg width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的网络流程中，输入是一个序列 \"床 前 明 月 光\"，输出也是一个序列 \"前 明 月 光 床\"。如果你仔细观察可以发现网络的每一步输出都是下一步的输入，这就是其设计思路。\n",
    "\n",
    "那么对于任意的一段话，比如 \"我喜欢小猫\"，我们可以将其拆分 \"我 喜 欢 小 猫\" 这个长度为 5 的序列，网络的每一步输出就是 \"喜 欢 小 猫 我\"，也就是每个字符的输出就是其**紧跟**的后一个字符。\n",
    "\n",
    "当然对于一个序列，其最后一个字符后面并没有其他的字符，所以有多种方式选择，比如将序列的第一个字符作为其输出，也就是 \"光\" 的输出是 \"床\"，或者将其本身作为输出，也就是 \"光\" 的输出是 \"光\"，这里的选择可以有很多，我们使用一种循环的连接，将第一个字符作为最后一个字符的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成文本\n",
    "这样设计网络的训练流程是为了非常好地生成文本，下面我们说明一下如何进行文本的生成。\n",
    "\n",
    "首先需要输入网络一段初始的序列进行预热，预热的过程并不需要实际的输出结果，只是为了生成拥有记忆效果的隐藏状态，并将隐藏状态保留下来，接着我们开始正式生成文本，每个字符作为输入都可以得到输出，然后将输出作为下一步的输入，这样就可以不断地生成新的句子，这个过程是可以无限循环下去，或者到达我们的要求输出长度，具体可以看看下面的图示\n",
    "\n",
    "<img src=https://ws2.sinaimg.cn/large/006tNc79gy1fob5z06w1uj30qh09m0sl.jpg width=800>\n",
    "\n",
    "讲完了原理之后，下面就该你亲自动手去实现这个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们可以探索一下数据集是什么样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.315656Z",
     "start_time": "2018-02-18T03:28:52.286844Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./dataset/poetry.txt', 'r') as f:\n",
    "    poetry_corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们取得了前100个字符的结果，其中 `\\n` 表示换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.331908Z",
     "start_time": "2018-02-18T03:28:52.317790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'寒随穷律变，春逐鸟声开。\\n初风飘带柳，晚雪间花梅。\\n碧林青旧竹，绿沼翠新苔。\\n芝田初雁去，绮树巧莺来。\\n晚霞聊自怡，初晴弥可喜。\\n日晃百花色，风动千林翠。\\n池鱼跃不同，园鸟声还异。\\n寄言博通者，知予物'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.338277Z",
     "start_time": "2018-02-18T03:28:52.334069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的字符数: 942681\n"
     ]
    }
   ],
   "source": [
    "# 看看字符数\n",
    "print('总的字符数: {}'.format(len(poetry_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了可视化比较方便，我们将换行字符 `\\n` 替换成空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.353185Z",
     "start_time": "2018-02-18T03:28:52.340405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开  初风飘带柳 晚雪间花梅  碧林青旧竹 绿沼翠新苔  芝田初雁去 绮树巧莺来  晚霞聊自怡 初晴弥可喜  日晃百花色 风动千林翠  池鱼跃不同 园鸟声还异  寄言博通者 知予物\n"
     ]
    }
   ],
   "source": [
    "poetry_corpus = poetry_corpus.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "print(poetry_corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数值表示\n",
    "对于每个文字，电脑并不能像人一样能够有效地识别，所以必须做一个转换，将文字转换成电脑能够识别的数字，相当于每个不同的汉字，都用不同的数字去表示，可以对所有非重复的字符，从 0 开始建立索引\n",
    "\n",
    "同时可能古诗中会出现一些生僻的字，这些字可能只会出现几次，甚至只会出现一次，引入这些字会增大模型的复杂度，同时也会影响模型的训练，可以将这些词频比较低的字去掉\n",
    "\n",
    "关于汉字和数字的转换，我们已经为你实现好了一个转换器，感兴趣的同学可以去 `utils.py` 中查看，在之后的练习中，你可以使用这个转换器进行生成文本的转换，下面我们先看看例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.642640Z",
     "start_time": "2018-02-18T03:28:52.355357Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import TextConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.016322Z",
     "start_time": "2018-02-18T03:28:52.645616Z"
    }
   },
   "outputs": [],
   "source": [
    "convert = TextConverter('./dataset/poetry.txt', max_vocab=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们通过数据集建立好了这个转换器 `convert`，下面我们看看如何去调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.025196Z",
     "start_time": "2018-02-18T03:28:53.018514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的文本结果: 寒随穷律变 春逐鸟声开\n",
      "\n",
      "转换成数字之后的结果: [ 40 166 358 935 565   0  10 367 108  63  78]\n",
      "\n",
      "将数字重新转换成文字: 寒随穷律变 春逐鸟声开\n"
     ]
    }
   ],
   "source": [
    "# 得到原始的文本结果\n",
    "txt_char = poetry_corpus[:11]\n",
    "print('原始的文本结果: {}'.format(txt_char))\n",
    "print()\n",
    "\n",
    "# 通过 convert 将文字转换成数字\n",
    "num_char = convert.text_to_arr(txt_char)\n",
    "print('转换成数字之后的结果: {}'.format(num_char))\n",
    "print()\n",
    "\n",
    "# 通过 convert 将数字转换成文字\n",
    "origin_txt_char = convert.arr_to_text(num_char)\n",
    "print('将数字重新转换成文字: {}'.format(origin_txt_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的例子，你可以看到，能够使用 `convert.text_to_arr` 对一个文本进行数字的转换，通过 `convert.arr_to_text` 将数字转换成文本 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造时序样本数据\n",
    "对于一整段文本，并不适合全部输入到循环神经网络中，因为我们前面了解到循环神经网络存在着长时依赖的问题，所以需要将整个文本分成很多个序列文本，然后将这些序列文本输入到循环神经网络中进行训练，只要我们定好每个序列的长度，那么序列个数也就被决定了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.036447Z",
     "start_time": "2018-02-18T03:28:53.027222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列的个数: 47134\n"
     ]
    }
   ],
   "source": [
    "# 每个序列的长度，你可以自行修改\n",
    "n_step = 20\n",
    "\n",
    "# 总的序列个数\n",
    "num_seq = int(len(poetry_corpus) / n_step)\n",
    "\n",
    "# 去掉最后不足一个序列长度的部分\n",
    "text = poetry_corpus[:num_seq*n_step]\n",
    "\n",
    "print('序列的个数: {}'.format(num_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着需要将序列中所有的文字转换成数字表示，同时重新排列成 **$(num\\_seq \\times n\\_step)$** 的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成下面的 `#todo` 的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.921749Z",
     "start_time": "2018-02-18T03:28:53.260507Z"
    }
   },
   "outputs": [],
   "source": [
    "arr = convert.text_to_arr(text) #todo: 使用 convert 将文本 text 转换成数字表示的数组\n",
    "arr = arr.reshape(num_seq, n_step) #todo: 将转换之后的数组重新排列成 (num_seq x n_step) 的形状\n",
    "arr = arr.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "# 不要修改下面的代码\n",
    "# ================== test =================\n",
    "if arr.shape == (num_seq, n_step):\n",
    "    print('Successful!')\n",
    "else:\n",
    "    print('Failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们可以构建 Tensorflow 中的数据读取来训练网络，这里我们将最后一个字符的输出 label 定为输入的第一个字符，也就是\"床前明月光\"的输出是\"前明月光床\"，完成下面 #todo 的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.945768Z",
     "start_time": "2018-02-18T03:28:53.925488Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(object):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        #TODO: 取得 arr 中的 item 这一个序列\n",
    "        x = self.arr[item, :]\n",
    "\n",
    "        #TODO: 构造上述描述的 label\n",
    "        y = np.zeros_like(x)\n",
    "#         y = torch.zeros(x.size())\n",
    "#         y = np.hstack((x[1:20], x[:1]))\n",
    "        y[:-1], y[-1] = x[1:], x[0]\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你构造好了这个数据集类，我们可以将其实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.950296Z",
     "start_time": "2018-02-18T03:28:53.947697Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们可以取出其中一个数据集参看一下是否是我们描述的这样，这个数据集需要像上面描述的一样，请自行检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.957705Z",
     "start_time": "2018-02-18T03:28:53.952232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(y): <class 'numpy.ndarray'>\n",
      "输入的文字序列 x: 寒随穷律变 春逐鸟声开  初风飘带柳 晚\n",
      "输出的文字序列 y: 随穷律变 春逐鸟声开  初风飘带柳 晚寒\n"
     ]
    }
   ],
   "source": [
    "x, y = train_set[0]\n",
    "# for x, y in train_set:\n",
    "#     print('输入的文字序列 x: {}'.format(convert.arr_to_text(x)))\n",
    "#     print('输出的文字序列 y: {}'.format(convert.arr_to_text(y)))\n",
    "print(\"type(y):\", type(y))\n",
    "print('输入的文字序列 x: {}'.format(convert.arr_to_text(x)))\n",
    "print('输出的文字序列 y: {}'.format(convert.arr_to_text(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "下面我们需要构建这个循环神经网路的网络结构，模型可以定义成非常简单的两层\n",
    "- 第一层是 RNN 层, **LSTM (GRU)**\n",
    "- 第二层是线性层，做分类问题，最后输出预测的字符 **slim.fully_connected**\n",
    "\n",
    "只需要按照提示填写下面的 #todo 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造输入\n",
    "\n",
    "    首先构造一些placeholder作为网络的输入,方便之后代入数据, 需要构建的是:\n",
    "    - inputs: placeholder, 接收 `[batch_size, n_step]` 的输入, 是输入的词\n",
    "    - targets: placeholder, 接收 `[batch_size, n_step]` 的输入, 是输入词的对应词, 也就是 label\n",
    "    - keep_prob: placeholder, 用来表示 dropout 的保留概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, n_step):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        batch_size: 一个批次中有多少个序列输入\n",
    "        n_steps: 一个序列中有多少个词\n",
    "        \n",
    "    return:\n",
    "        inputs: 输入的词\n",
    "        targets: 输入词的对应词\n",
    "        keep_prob: dropout 保留概率\n",
    "    '''\n",
    "    inputs = tf.placeholder(shape=(batch_size, n_step), dtype=tf.int32)\n",
    "    targets = tf.placeholder(shape=(batch_size, n_step), dtype=tf.int32)\n",
    "    keep_prob = tf.placeholder(name='keep_prob', dtype=tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造 RNN\n",
    "\n",
    "    然后我们开始构造 RNN, 将一个序列中的每个词产生一个输出词. \n",
    "\n",
    "    这里我们可以构造一个多层的 RNN, 可以使用 LSTM 或者 GRU 作为 RNN 的基本单元."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(hidden_size, num_layers, batch_size, keep_prob):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        keep_prob: dropout 保留概率\n",
    "        hidden_size: RNN 隐藏层大小\n",
    "        num_layers: RNN 隐藏层个数\n",
    "        batch_size: batch size\n",
    "\n",
    "    return:\n",
    "        cell: RNN cell\n",
    "        initial_state: RNN输入时的初始状态\n",
    "    '''\n",
    "    \n",
    "    def build_cell(hidden_size, keep_prob):\n",
    "        #todo: 得到一个 rnn cell, 可以是 rnn 或者 lstm 或者 gru\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "        \n",
    "        #todo: 添加 dropout\n",
    "        lstm_drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        \n",
    "        return lstm_drop\n",
    "    \n",
    "    \n",
    "    #todo: 得到一个多层的 rnn cell\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(hidden_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    #todo: 得到 cell 的初始状态\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造分类层\n",
    "\n",
    "    现在我们要构造以 RNN 输出的结果为输入, 一个词为输出的全连接层, 这是一个分类问题, 有多少个词就是多少分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def build_output(rnn_out, in_size, out_size):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        rnn_out: 上一步rnn的输出\n",
    "        in_size: rnn输出的特征个数\n",
    "        out_size: 词的总个数(分类数)\n",
    "    \n",
    "    return:\n",
    "        out: 输出词的概率向量\n",
    "        logits: softmax之前的结果\n",
    "    '''\n",
    "\n",
    "    #todo: rnn_out 的形状是 (batch, n_step, rnn_size), 将形状改成 (batch x n_step, rnn_size)\n",
    "    # 变成一个2阶矩阵才可以参与到下一步的分类层\n",
    "#     seq_output = tf.concat(rnn_out, axis=1)\n",
    "    x = tf.reshape(rnn_out, [-1, in_size])\n",
    "    \n",
    "    #todo: 一个全连阶层作为分类层\n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    #todo: softmax 得到概率\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits,softmax_w,softmax_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造损失函数\n",
    "\n",
    "    这是一个分类问题, 因此我们使用 softmax_with_logits 作为损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes, softmax_w, softmax_b):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        logits: softmax之前的结果\n",
    "        targets: 目标词\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: 词的总个数(分类数)\n",
    "    \n",
    "    return:\n",
    "        loss: loss tensor.\n",
    "    '''\n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    #todo: softmax 分类损失函数\n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_reshaped, logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造训练过程\n",
    "\n",
    "    接下来我们需要构造训练过程. 前面说到过, RNN 经常会遇到梯度爆炸的问题, 但有一种方法可以避免这类问题, 就是\"梯度裁剪\".\n",
    "    \n",
    "    tensorflow 可以通过**`tf.clip_by_global_norm(tensors, grad_clip)`**函数对`tensors`进行梯度裁剪. \n",
    "    \n",
    "    形象地说, 如果大于`grad_clip`就会将大于的部分剪掉, 这样操作之后, 所有的`tensors`都比`grad_clip`要小, 也就不会存在爆炸的问题了.\n",
    "    \n",
    "    因此在这里我们会用到这个方法, 那么我们就不再使用最简单的`optimizer.minimize`这个函数去构造训练过程了, 需要把这个过程拆开, 具体来说分为下面几步:\n",
    "    \n",
    "        - 计算所有可训练变量的梯度\n",
    "        - 对所有梯度进行裁剪\n",
    "        - 再将梯度应用到原来的变量上去\n",
    "        \n",
    "    第一步和第二步应该都知道如何去做, 第三步需要用到一个全新的函数, **`optimizer.apply_gradients`**. \n",
    "    \n",
    "    `optimizer`就是前面我们定义的比如说梯度下降法方法, Momentum方法, Adam方法等等优化器, 每个优化器都有`apply_gradients`方法, 这里不具体展开如何使用这个函数, 大家可以查看下面的函数说明或者参考[这里](https://tensorflow.google.cn/api_docs/python/tf/train/AdamOptimizer#apply_gradients)\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://image.ibb.co/dx7cRn/apply_gradient.png\" alt=\"apply gradient\" border=\"0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        loss: loss tensor\n",
    "        learning_rate: 学习率\n",
    "    \n",
    "    return:\n",
    "        optimizer: 优化方法\n",
    "    '''\n",
    "\n",
    "    #todo: 获取所有的可训练变量\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    #todo: 获取 loss 对 tvars 的梯度\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "    \n",
    "    #todo: 使用 tf.clip_by_global_norm 进行梯度裁剪\n",
    "    grads_clipped, _ = tf.clip_by_global_norm(grads, grad_clip)\n",
    "#     grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5)\n",
    "    \n",
    "    #todo: 生成一个 Adam 优化器\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    #todo: 使用 apply_gradients 生成一个参数更新 op\n",
    "    optimizer = train_op.apply_gradients(zip(grads_clipped, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建完整的 **CharRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       rnn_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "        '''\n",
    "        \n",
    "        args:\n",
    "            num_classes: 分类数, 也就是字符总数\n",
    "            batch_size: batch size\n",
    "            num_steps: 一个序列中出现的字符个数\n",
    "            rnn_size: rnn 的隐藏层大小\n",
    "            num_layers: rnn 中的隐藏层个数\n",
    "            learning_rate: 学习率\n",
    "            grad_clip: 梯度裁剪常数\n",
    "            sampling: 是否进行采样\n",
    "            \n",
    "        '''\n",
    "        # 之后我们用这个网络进行 inference 的时候, 我们会传入一个字符进来, 而不是训练时候的\n",
    "        # 传入 n_step 个字符, 因此在这里用 sampling 来控制\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.name_scope('Inputs'):\n",
    "        # Build the input placeholder tensors\n",
    "            self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "            \n",
    "        #todo: 构建输入\n",
    "        with tf.name_scope('Lstm'):\n",
    "            cell, self.initial_state = build_rnn(rnn_size, num_layers, batch_size, self.keep_prob)\n",
    "            \n",
    "#         self.inputs, self.targets, self.keep_prob = tf.placeholder(tf.int32, [batch_size, None]),\n",
    "#         tf.placeholder(tf.int32, [batch_size, None]),\n",
    "#         tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "#         #todo: 构建RNN的cell\n",
    "#         lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size,state_is_tuple=True)     \n",
    "#         #todo: 添加 dropout\n",
    "#         lstm = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)   \n",
    "#         #todo: 得到一个多层的 rnn cell\n",
    "#         cell = tf.contrib.rnn.MultiRNNCell([lstm] * num_layers,state_is_tuple=True)\n",
    "#         cell, self.initial_state = cell, cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ### 用RNN跑一遍输入得到输出\n",
    "        # 首先将输入转化成one_hot形式, 相当于给字符编码\n",
    "        # 这里你也可以使用我们之前讲过的 word_embedding, 将字符嵌入到一个向量里面去\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #todo: 运行RNN得到输出和最终状态(提示: 使用 tf.nn.dynamic_rnn)\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        \n",
    "        #todo: 将最后的状态保存在 final_state 中\n",
    "        self.final_state = state\n",
    "        \n",
    "        #todo: 得到分类层的结果\n",
    "        self.prediction, self.logits, self.softmax_w,self.softmax_b = build_output(outputs, rnn_size, num_classes)\n",
    "        \n",
    "        #todo: 得到损失函数\n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        with tf.name_scope('Loss'):\n",
    "            self.loss = build_loss(self.logits, self.targets, rnn_size, num_classes,self.softmax_w,self.softmax_b)\n",
    "        \n",
    "        #todo: 得到优化算子\n",
    "        with tf.name_scope('SGD'):\n",
    "            self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们再定义网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = None        # batch_size\n",
    "# rnn_size = None          # rnn中隐藏层的大小\n",
    "# num_layers = None        # rnn中隐藏层的个数\n",
    "# learning_rate = None     # 学习率\n",
    "# keep_prob = None         # dropout保留概率\n",
    "\n",
    "batch_size = 256    # Sequences per batch\n",
    "num_steps = 20    # Number of sequence steps per batch\n",
    "rnn_size = 512     # Size of hidden layers in LSTMs\n",
    "num_layers = 2         # Number of LSTM layers\n",
    "learning_rate = 0.003    # Learning rate\n",
    "keep_prob = 0.5     # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面的`CharRNN`构造model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "model =CharRNN(convert.vocab_size, batch_size=batch_size, num_steps=num_steps,\n",
    "                rnn_size=rnn_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Print losses every N interations\n",
    "print_every_n = 1\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面构造一个读取数据的`generator`, 也可以自行实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class build_data_generator:\n",
    "    def __init__(self, data, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.nb_of_examples = len(data)\n",
    "    \n",
    "    def __call__(self):\n",
    "        ind = 0\n",
    "        indices = list(range(self.nb_of_examples))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "\n",
    "        while ind + self.batch_size <= self.nb_of_examples:\n",
    "            x, y = self.data[ind: ind + batch_size]\n",
    "            ind += batch_size\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nb_of_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...  Training Step: 1...  Training loss: 8.5918...  1.7820 sec/batch\n",
      "Epoch: 1/100...  Training Step: 2...  Training loss: 8.4878...  0.3435 sec/batch\n",
      "Epoch: 1/100...  Training Step: 3...  Training loss: 9.4271...  0.3306 sec/batch\n",
      "Epoch: 1/100...  Training Step: 4...  Training loss: 7.4339...  0.3334 sec/batch\n",
      "Epoch: 1/100...  Training Step: 5...  Training loss: 7.6500...  0.3351 sec/batch\n",
      "Epoch: 1/100...  Training Step: 6...  Training loss: 6.9588...  0.3350 sec/batch\n",
      "Epoch: 1/100...  Training Step: 7...  Training loss: 6.6767...  0.3340 sec/batch\n",
      "Epoch: 1/100...  Training Step: 8...  Training loss: 6.5810...  0.3349 sec/batch\n",
      "Epoch: 1/100...  Training Step: 9...  Training loss: 6.6424...  0.3346 sec/batch\n",
      "Epoch: 1/100...  Training Step: 10...  Training loss: 6.6177...  0.3337 sec/batch\n",
      "Epoch: 1/100...  Training Step: 11...  Training loss: 6.3776...  0.3323 sec/batch\n",
      "Epoch: 1/100...  Training Step: 12...  Training loss: 6.1868...  0.3315 sec/batch\n",
      "Epoch: 1/100...  Training Step: 13...  Training loss: 6.2084...  0.3317 sec/batch\n",
      "Epoch: 1/100...  Training Step: 14...  Training loss: 6.4283...  0.3334 sec/batch\n",
      "Epoch: 1/100...  Training Step: 15...  Training loss: 6.2691...  0.3325 sec/batch\n",
      "Epoch: 1/100...  Training Step: 16...  Training loss: 6.1726...  0.3340 sec/batch\n",
      "Epoch: 1/100...  Training Step: 17...  Training loss: 6.2607...  0.3328 sec/batch\n",
      "Epoch: 1/100...  Training Step: 18...  Training loss: 6.1255...  0.3363 sec/batch\n",
      "Epoch: 1/100...  Training Step: 19...  Training loss: 6.2071...  0.3329 sec/batch\n",
      "Epoch: 1/100...  Training Step: 20...  Training loss: 6.1411...  0.3363 sec/batch\n",
      "Epoch: 1/100...  Training Step: 21...  Training loss: 6.1659...  0.3321 sec/batch\n",
      "Epoch: 1/100...  Training Step: 22...  Training loss: 6.1115...  0.3335 sec/batch\n",
      "Epoch: 1/100...  Training Step: 23...  Training loss: 6.1267...  0.3342 sec/batch\n",
      "Epoch: 1/100...  Training Step: 24...  Training loss: 6.1652...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 25...  Training loss: 6.1147...  0.3339 sec/batch\n",
      "Epoch: 1/100...  Training Step: 26...  Training loss: 6.0595...  0.3332 sec/batch\n",
      "Epoch: 1/100...  Training Step: 27...  Training loss: 6.1168...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 28...  Training loss: 6.0955...  0.3320 sec/batch\n",
      "Epoch: 1/100...  Training Step: 29...  Training loss: 6.0641...  0.3340 sec/batch\n",
      "Epoch: 1/100...  Training Step: 30...  Training loss: 6.0312...  0.3356 sec/batch\n",
      "Epoch: 1/100...  Training Step: 31...  Training loss: 5.9535...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 32...  Training loss: 5.9990...  0.3331 sec/batch\n",
      "Epoch: 1/100...  Training Step: 33...  Training loss: 5.8527...  0.3343 sec/batch\n",
      "Epoch: 1/100...  Training Step: 34...  Training loss: 5.8859...  0.3310 sec/batch\n",
      "Epoch: 1/100...  Training Step: 35...  Training loss: 5.9749...  0.3344 sec/batch\n",
      "Epoch: 1/100...  Training Step: 36...  Training loss: 6.1115...  0.3336 sec/batch\n",
      "Epoch: 1/100...  Training Step: 37...  Training loss: 6.0795...  0.3334 sec/batch\n",
      "Epoch: 1/100...  Training Step: 38...  Training loss: 6.0860...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 39...  Training loss: 5.9708...  0.3321 sec/batch\n",
      "Epoch: 1/100...  Training Step: 40...  Training loss: 5.9093...  0.3324 sec/batch\n",
      "Epoch: 1/100...  Training Step: 41...  Training loss: 6.0709...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 42...  Training loss: 6.0020...  0.3374 sec/batch\n",
      "Epoch: 1/100...  Training Step: 43...  Training loss: 6.0518...  0.3342 sec/batch\n",
      "Epoch: 1/100...  Training Step: 44...  Training loss: 6.0758...  0.3336 sec/batch\n",
      "Epoch: 1/100...  Training Step: 45...  Training loss: 5.9482...  0.3338 sec/batch\n",
      "Epoch: 1/100...  Training Step: 46...  Training loss: 5.9858...  0.3344 sec/batch\n",
      "Epoch: 1/100...  Training Step: 47...  Training loss: 6.0199...  0.3314 sec/batch\n",
      "Epoch: 1/100...  Training Step: 48...  Training loss: 5.9566...  0.3356 sec/batch\n",
      "Epoch: 1/100...  Training Step: 49...  Training loss: 5.9392...  0.3324 sec/batch\n",
      "Epoch: 1/100...  Training Step: 50...  Training loss: 6.1373...  0.3334 sec/batch\n",
      "Epoch: 1/100...  Training Step: 51...  Training loss: 6.0857...  0.3337 sec/batch\n",
      "Epoch: 1/100...  Training Step: 52...  Training loss: 5.9764...  0.3353 sec/batch\n",
      "Epoch: 1/100...  Training Step: 53...  Training loss: 6.0949...  0.3368 sec/batch\n",
      "Epoch: 1/100...  Training Step: 54...  Training loss: 6.0924...  0.3381 sec/batch\n",
      "Epoch: 1/100...  Training Step: 55...  Training loss: 6.0404...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 56...  Training loss: 5.9523...  0.3336 sec/batch\n",
      "Epoch: 1/100...  Training Step: 57...  Training loss: 5.9041...  0.3345 sec/batch\n",
      "Epoch: 1/100...  Training Step: 58...  Training loss: 5.9794...  0.3374 sec/batch\n",
      "Epoch: 1/100...  Training Step: 59...  Training loss: 5.8278...  0.3346 sec/batch\n",
      "Epoch: 1/100...  Training Step: 60...  Training loss: 5.8596...  0.3371 sec/batch\n",
      "Epoch: 1/100...  Training Step: 61...  Training loss: 5.8347...  0.3331 sec/batch\n",
      "Epoch: 1/100...  Training Step: 62...  Training loss: 5.8929...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 63...  Training loss: 5.9737...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 64...  Training loss: 5.9031...  0.3329 sec/batch\n",
      "Epoch: 1/100...  Training Step: 65...  Training loss: 5.8595...  0.3312 sec/batch\n",
      "Epoch: 1/100...  Training Step: 66...  Training loss: 5.8690...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 67...  Training loss: 5.8647...  0.3366 sec/batch\n",
      "Epoch: 1/100...  Training Step: 68...  Training loss: 5.7634...  0.3326 sec/batch\n",
      "Epoch: 1/100...  Training Step: 69...  Training loss: 5.9371...  0.3321 sec/batch\n",
      "Epoch: 1/100...  Training Step: 70...  Training loss: 5.8443...  0.3358 sec/batch\n",
      "Epoch: 1/100...  Training Step: 71...  Training loss: 5.8953...  0.3347 sec/batch\n",
      "Epoch: 1/100...  Training Step: 72...  Training loss: 5.6731...  0.3338 sec/batch\n",
      "Epoch: 1/100...  Training Step: 73...  Training loss: 5.7330...  0.3315 sec/batch\n",
      "Epoch: 1/100...  Training Step: 74...  Training loss: 5.7736...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 75...  Training loss: 5.6725...  0.3332 sec/batch\n",
      "Epoch: 1/100...  Training Step: 76...  Training loss: 5.6515...  0.3337 sec/batch\n",
      "Epoch: 1/100...  Training Step: 77...  Training loss: 5.7616...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 78...  Training loss: 5.7705...  0.3343 sec/batch\n",
      "Epoch: 1/100...  Training Step: 79...  Training loss: 5.8531...  0.3318 sec/batch\n",
      "Epoch: 1/100...  Training Step: 80...  Training loss: 6.0614...  0.3344 sec/batch\n",
      "Epoch: 1/100...  Training Step: 81...  Training loss: 5.8895...  0.3330 sec/batch\n",
      "Epoch: 1/100...  Training Step: 82...  Training loss: 5.9041...  0.3339 sec/batch\n",
      "Epoch: 1/100...  Training Step: 83...  Training loss: 5.8299...  0.3353 sec/batch\n",
      "Epoch: 1/100...  Training Step: 84...  Training loss: 5.8081...  0.3313 sec/batch\n",
      "Epoch: 1/100...  Training Step: 85...  Training loss: 5.8398...  0.3330 sec/batch\n",
      "Epoch: 1/100...  Training Step: 86...  Training loss: 5.7558...  0.3340 sec/batch\n",
      "Epoch: 1/100...  Training Step: 87...  Training loss: 5.7143...  0.3345 sec/batch\n",
      "Epoch: 1/100...  Training Step: 88...  Training loss: 5.8249...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 89...  Training loss: 5.7920...  0.3357 sec/batch\n",
      "Epoch: 1/100...  Training Step: 90...  Training loss: 5.7554...  0.3350 sec/batch\n",
      "Epoch: 1/100...  Training Step: 91...  Training loss: 5.7494...  0.3346 sec/batch\n",
      "Epoch: 1/100...  Training Step: 92...  Training loss: 5.5741...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 93...  Training loss: 5.8136...  0.3356 sec/batch\n",
      "Epoch: 1/100...  Training Step: 94...  Training loss: 5.8384...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 95...  Training loss: 5.7153...  0.3347 sec/batch\n",
      "Epoch: 1/100...  Training Step: 96...  Training loss: 5.5811...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 97...  Training loss: 5.6422...  0.3371 sec/batch\n",
      "Epoch: 1/100...  Training Step: 98...  Training loss: 5.6629...  0.3375 sec/batch\n",
      "Epoch: 1/100...  Training Step: 99...  Training loss: 5.6898...  0.3383 sec/batch\n",
      "Epoch: 1/100...  Training Step: 100...  Training loss: 5.6661...  0.3372 sec/batch\n",
      "Epoch: 1/100...  Training Step: 101...  Training loss: 5.6640...  0.3372 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100...  Training Step: 102...  Training loss: 5.6661...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 103...  Training loss: 5.8227...  0.3362 sec/batch\n",
      "Epoch: 1/100...  Training Step: 104...  Training loss: 5.7235...  0.3370 sec/batch\n",
      "Epoch: 1/100...  Training Step: 105...  Training loss: 5.7773...  0.3363 sec/batch\n",
      "Epoch: 1/100...  Training Step: 106...  Training loss: 5.7724...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 107...  Training loss: 5.7560...  0.3349 sec/batch\n",
      "Epoch: 1/100...  Training Step: 108...  Training loss: 5.5436...  0.3353 sec/batch\n",
      "Epoch: 1/100...  Training Step: 109...  Training loss: 5.5355...  0.3327 sec/batch\n",
      "Epoch: 1/100...  Training Step: 110...  Training loss: 5.5502...  0.3359 sec/batch\n",
      "Epoch: 1/100...  Training Step: 111...  Training loss: 5.5329...  0.3383 sec/batch\n",
      "Epoch: 1/100...  Training Step: 112...  Training loss: 5.6993...  0.3343 sec/batch\n",
      "Epoch: 1/100...  Training Step: 113...  Training loss: 5.6548...  0.3372 sec/batch\n",
      "Epoch: 1/100...  Training Step: 114...  Training loss: 5.6047...  0.3367 sec/batch\n",
      "Epoch: 1/100...  Training Step: 115...  Training loss: 5.4878...  0.3371 sec/batch\n",
      "Epoch: 1/100...  Training Step: 116...  Training loss: 5.4951...  0.3353 sec/batch\n",
      "Epoch: 1/100...  Training Step: 117...  Training loss: 5.6436...  0.3350 sec/batch\n",
      "Epoch: 1/100...  Training Step: 118...  Training loss: 5.5053...  0.3335 sec/batch\n",
      "Epoch: 1/100...  Training Step: 119...  Training loss: 5.4991...  0.3369 sec/batch\n",
      "Epoch: 1/100...  Training Step: 120...  Training loss: 5.5909...  0.3363 sec/batch\n",
      "Epoch: 1/100...  Training Step: 121...  Training loss: 5.8003...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 122...  Training loss: 5.6780...  0.3338 sec/batch\n",
      "Epoch: 1/100...  Training Step: 123...  Training loss: 5.5359...  0.3339 sec/batch\n",
      "Epoch: 1/100...  Training Step: 124...  Training loss: 5.5721...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 125...  Training loss: 5.5203...  0.3353 sec/batch\n",
      "Epoch: 1/100...  Training Step: 126...  Training loss: 5.4712...  0.3337 sec/batch\n",
      "Epoch: 1/100...  Training Step: 127...  Training loss: 5.6159...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 128...  Training loss: 5.6667...  0.3338 sec/batch\n",
      "Epoch: 1/100...  Training Step: 129...  Training loss: 5.7357...  0.3354 sec/batch\n",
      "Epoch: 1/100...  Training Step: 130...  Training loss: 5.5516...  0.3339 sec/batch\n",
      "Epoch: 1/100...  Training Step: 131...  Training loss: 5.5610...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 132...  Training loss: 5.5656...  0.3385 sec/batch\n",
      "Epoch: 1/100...  Training Step: 133...  Training loss: 5.5870...  0.3347 sec/batch\n",
      "Epoch: 1/100...  Training Step: 134...  Training loss: 5.4049...  0.3332 sec/batch\n",
      "Epoch: 1/100...  Training Step: 135...  Training loss: 5.4834...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 136...  Training loss: 5.3374...  0.3377 sec/batch\n",
      "Epoch: 1/100...  Training Step: 137...  Training loss: 5.5846...  0.3376 sec/batch\n",
      "Epoch: 1/100...  Training Step: 138...  Training loss: 5.4494...  0.3358 sec/batch\n",
      "Epoch: 1/100...  Training Step: 139...  Training loss: 5.5209...  0.3359 sec/batch\n",
      "Epoch: 1/100...  Training Step: 140...  Training loss: 5.8835...  0.3356 sec/batch\n",
      "Epoch: 1/100...  Training Step: 141...  Training loss: 5.8305...  0.3340 sec/batch\n",
      "Epoch: 1/100...  Training Step: 142...  Training loss: 5.7405...  0.3369 sec/batch\n",
      "Epoch: 1/100...  Training Step: 143...  Training loss: 5.5427...  0.3377 sec/batch\n",
      "Epoch: 1/100...  Training Step: 144...  Training loss: 5.4563...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 145...  Training loss: 5.4971...  0.3343 sec/batch\n",
      "Epoch: 1/100...  Training Step: 146...  Training loss: 5.5489...  0.3343 sec/batch\n",
      "Epoch: 1/100...  Training Step: 147...  Training loss: 5.6513...  0.3347 sec/batch\n",
      "Epoch: 1/100...  Training Step: 148...  Training loss: 5.5641...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 149...  Training loss: 5.5509...  0.3349 sec/batch\n",
      "Epoch: 1/100...  Training Step: 150...  Training loss: 5.5335...  0.3350 sec/batch\n",
      "Epoch: 1/100...  Training Step: 151...  Training loss: 5.5720...  0.3374 sec/batch\n",
      "Epoch: 1/100...  Training Step: 152...  Training loss: 5.3463...  0.3364 sec/batch\n",
      "Epoch: 1/100...  Training Step: 153...  Training loss: 5.5802...  0.3358 sec/batch\n",
      "Epoch: 1/100...  Training Step: 154...  Training loss: 5.4585...  0.3346 sec/batch\n",
      "Epoch: 1/100...  Training Step: 155...  Training loss: 5.5948...  0.3357 sec/batch\n",
      "Epoch: 1/100...  Training Step: 156...  Training loss: 5.4828...  0.3383 sec/batch\n",
      "Epoch: 1/100...  Training Step: 157...  Training loss: 5.5665...  0.3370 sec/batch\n",
      "Epoch: 1/100...  Training Step: 158...  Training loss: 5.6372...  0.3354 sec/batch\n",
      "Epoch: 1/100...  Training Step: 159...  Training loss: 5.4985...  0.3348 sec/batch\n",
      "Epoch: 1/100...  Training Step: 160...  Training loss: 5.4983...  0.3352 sec/batch\n",
      "Epoch: 1/100...  Training Step: 161...  Training loss: 5.5172...  0.3375 sec/batch\n",
      "Epoch: 1/100...  Training Step: 162...  Training loss: 5.6177...  0.3381 sec/batch\n",
      "Epoch: 1/100...  Training Step: 163...  Training loss: 5.5460...  0.3375 sec/batch\n",
      "Epoch: 1/100...  Training Step: 164...  Training loss: 5.5896...  0.3370 sec/batch\n",
      "Epoch: 1/100...  Training Step: 165...  Training loss: 5.8303...  0.3333 sec/batch\n",
      "Epoch: 1/100...  Training Step: 166...  Training loss: 5.7446...  0.3360 sec/batch\n",
      "Epoch: 1/100...  Training Step: 167...  Training loss: 5.6436...  0.3379 sec/batch\n",
      "Epoch: 1/100...  Training Step: 168...  Training loss: 5.5220...  0.3365 sec/batch\n",
      "Epoch: 1/100...  Training Step: 169...  Training loss: 5.4953...  0.3359 sec/batch\n",
      "Epoch: 1/100...  Training Step: 170...  Training loss: 5.4332...  0.3364 sec/batch\n",
      "Epoch: 1/100...  Training Step: 171...  Training loss: 5.4213...  0.3383 sec/batch\n",
      "Epoch: 1/100...  Training Step: 172...  Training loss: 5.4311...  0.3376 sec/batch\n",
      "Epoch: 1/100...  Training Step: 173...  Training loss: 5.5562...  0.3379 sec/batch\n",
      "Epoch: 1/100...  Training Step: 174...  Training loss: 5.5371...  0.3384 sec/batch\n",
      "Epoch: 1/100...  Training Step: 175...  Training loss: 5.5438...  0.3393 sec/batch\n",
      "Epoch: 1/100...  Training Step: 176...  Training loss: 5.4385...  0.3357 sec/batch\n",
      "Epoch: 1/100...  Training Step: 177...  Training loss: 5.4940...  0.3372 sec/batch\n",
      "Epoch: 1/100...  Training Step: 178...  Training loss: 5.4673...  0.3349 sec/batch\n",
      "Epoch: 1/100...  Training Step: 179...  Training loss: 5.4439...  0.3389 sec/batch\n",
      "Epoch: 1/100...  Training Step: 180...  Training loss: 5.4399...  0.3380 sec/batch\n",
      "Epoch: 1/100...  Training Step: 181...  Training loss: 5.8263...  0.3394 sec/batch\n",
      "Epoch: 1/100...  Training Step: 182...  Training loss: 5.6988...  0.3356 sec/batch\n",
      "Epoch: 1/100...  Training Step: 183...  Training loss: 5.7655...  0.3376 sec/batch\n",
      "Epoch: 1/100...  Training Step: 184...  Training loss: 5.6992...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 185...  Training loss: 7.7205...  0.3379 sec/batch\n",
      "Epoch: 2/100...  Training Step: 186...  Training loss: 6.8006...  0.3352 sec/batch\n",
      "Epoch: 2/100...  Training Step: 187...  Training loss: 6.1861...  0.3366 sec/batch\n",
      "Epoch: 2/100...  Training Step: 188...  Training loss: 5.9931...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 189...  Training loss: 5.8816...  0.3381 sec/batch\n",
      "Epoch: 2/100...  Training Step: 190...  Training loss: 5.7973...  0.3387 sec/batch\n",
      "Epoch: 2/100...  Training Step: 191...  Training loss: 6.0580...  0.3340 sec/batch\n",
      "Epoch: 2/100...  Training Step: 192...  Training loss: 5.7424...  0.3347 sec/batch\n",
      "Epoch: 2/100...  Training Step: 193...  Training loss: 5.8251...  0.3379 sec/batch\n",
      "Epoch: 2/100...  Training Step: 194...  Training loss: 5.6893...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 195...  Training loss: 5.6935...  0.3349 sec/batch\n",
      "Epoch: 2/100...  Training Step: 196...  Training loss: 5.6745...  0.3354 sec/batch\n",
      "Epoch: 2/100...  Training Step: 197...  Training loss: 5.6465...  0.3399 sec/batch\n",
      "Epoch: 2/100...  Training Step: 198...  Training loss: 5.8004...  0.3397 sec/batch\n",
      "Epoch: 2/100...  Training Step: 199...  Training loss: 5.7420...  0.3382 sec/batch\n",
      "Epoch: 2/100...  Training Step: 200...  Training loss: 5.7000...  0.3346 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100...  Training Step: 201...  Training loss: 5.7605...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 202...  Training loss: 5.5873...  0.3400 sec/batch\n",
      "Epoch: 2/100...  Training Step: 203...  Training loss: 5.6592...  0.3404 sec/batch\n",
      "Epoch: 2/100...  Training Step: 204...  Training loss: 5.6164...  0.3391 sec/batch\n",
      "Epoch: 2/100...  Training Step: 205...  Training loss: 5.6804...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 206...  Training loss: 5.6030...  0.3350 sec/batch\n",
      "Epoch: 2/100...  Training Step: 207...  Training loss: 5.6373...  0.3371 sec/batch\n",
      "Epoch: 2/100...  Training Step: 208...  Training loss: 5.6952...  0.3343 sec/batch\n",
      "Epoch: 2/100...  Training Step: 209...  Training loss: 5.6030...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 210...  Training loss: 5.5529...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 211...  Training loss: 5.5456...  0.3359 sec/batch\n",
      "Epoch: 2/100...  Training Step: 212...  Training loss: 5.5536...  0.3368 sec/batch\n",
      "Epoch: 2/100...  Training Step: 213...  Training loss: 5.5501...  0.3392 sec/batch\n",
      "Epoch: 2/100...  Training Step: 214...  Training loss: 5.5335...  0.3338 sec/batch\n",
      "Epoch: 2/100...  Training Step: 215...  Training loss: 5.4805...  0.3375 sec/batch\n",
      "Epoch: 2/100...  Training Step: 216...  Training loss: 5.4951...  0.3379 sec/batch\n",
      "Epoch: 2/100...  Training Step: 217...  Training loss: 5.2910...  0.3341 sec/batch\n",
      "Epoch: 2/100...  Training Step: 218...  Training loss: 5.3523...  0.3345 sec/batch\n",
      "Epoch: 2/100...  Training Step: 219...  Training loss: 5.4787...  0.3380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 220...  Training loss: 5.6101...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 221...  Training loss: 5.5851...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 222...  Training loss: 5.5785...  0.3346 sec/batch\n",
      "Epoch: 2/100...  Training Step: 223...  Training loss: 5.4557...  0.3340 sec/batch\n",
      "Epoch: 2/100...  Training Step: 224...  Training loss: 5.4240...  0.3399 sec/batch\n",
      "Epoch: 2/100...  Training Step: 225...  Training loss: 5.5586...  0.3364 sec/batch\n",
      "Epoch: 2/100...  Training Step: 226...  Training loss: 5.5018...  0.3361 sec/batch\n",
      "Epoch: 2/100...  Training Step: 227...  Training loss: 5.5634...  0.3378 sec/batch\n",
      "Epoch: 2/100...  Training Step: 228...  Training loss: 5.6023...  0.3397 sec/batch\n",
      "Epoch: 2/100...  Training Step: 229...  Training loss: 5.4832...  0.3361 sec/batch\n",
      "Epoch: 2/100...  Training Step: 230...  Training loss: 5.4944...  0.3378 sec/batch\n",
      "Epoch: 2/100...  Training Step: 231...  Training loss: 5.5233...  0.3348 sec/batch\n",
      "Epoch: 2/100...  Training Step: 232...  Training loss: 5.4648...  0.3356 sec/batch\n",
      "Epoch: 2/100...  Training Step: 233...  Training loss: 5.4617...  0.3387 sec/batch\n",
      "Epoch: 2/100...  Training Step: 234...  Training loss: 5.7001...  0.3343 sec/batch\n",
      "Epoch: 2/100...  Training Step: 235...  Training loss: 5.6430...  0.3348 sec/batch\n",
      "Epoch: 2/100...  Training Step: 236...  Training loss: 5.5147...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 237...  Training loss: 5.6375...  0.3378 sec/batch\n",
      "Epoch: 2/100...  Training Step: 238...  Training loss: 5.6527...  0.3380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 239...  Training loss: 5.6189...  0.3349 sec/batch\n",
      "Epoch: 2/100...  Training Step: 240...  Training loss: 5.4997...  0.3358 sec/batch\n",
      "Epoch: 2/100...  Training Step: 241...  Training loss: 5.4635...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 242...  Training loss: 5.5428...  0.3377 sec/batch\n",
      "Epoch: 2/100...  Training Step: 243...  Training loss: 5.3835...  0.3370 sec/batch\n",
      "Epoch: 2/100...  Training Step: 244...  Training loss: 5.4062...  0.3379 sec/batch\n",
      "Epoch: 2/100...  Training Step: 245...  Training loss: 5.4033...  0.3360 sec/batch\n",
      "Epoch: 2/100...  Training Step: 246...  Training loss: 5.4674...  0.3380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 247...  Training loss: 5.5754...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 248...  Training loss: 5.5434...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 249...  Training loss: 5.4497...  0.3378 sec/batch\n",
      "Epoch: 2/100...  Training Step: 250...  Training loss: 5.4625...  0.3394 sec/batch\n",
      "Epoch: 2/100...  Training Step: 251...  Training loss: 5.4709...  0.3347 sec/batch\n",
      "Epoch: 2/100...  Training Step: 252...  Training loss: 5.3453...  0.3364 sec/batch\n",
      "Epoch: 2/100...  Training Step: 253...  Training loss: 5.5647...  0.3368 sec/batch\n",
      "Epoch: 2/100...  Training Step: 254...  Training loss: 5.4864...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 255...  Training loss: 5.5285...  0.3390 sec/batch\n",
      "Epoch: 2/100...  Training Step: 256...  Training loss: 5.3031...  0.3360 sec/batch\n",
      "Epoch: 2/100...  Training Step: 257...  Training loss: 5.3745...  0.3388 sec/batch\n",
      "Epoch: 2/100...  Training Step: 258...  Training loss: 5.4566...  0.3398 sec/batch\n",
      "Epoch: 2/100...  Training Step: 259...  Training loss: 5.3707...  0.3375 sec/batch\n",
      "Epoch: 2/100...  Training Step: 260...  Training loss: 5.3510...  0.3370 sec/batch\n",
      "Epoch: 2/100...  Training Step: 261...  Training loss: 5.4753...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 262...  Training loss: 5.5133...  0.3352 sec/batch\n",
      "Epoch: 2/100...  Training Step: 263...  Training loss: 5.5794...  0.3394 sec/batch\n",
      "Epoch: 2/100...  Training Step: 264...  Training loss: 5.7698...  0.3347 sec/batch\n",
      "Epoch: 2/100...  Training Step: 265...  Training loss: 5.6127...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 266...  Training loss: 5.6236...  0.3350 sec/batch\n",
      "Epoch: 2/100...  Training Step: 267...  Training loss: 5.5909...  0.3346 sec/batch\n",
      "Epoch: 2/100...  Training Step: 268...  Training loss: 5.5946...  0.3347 sec/batch\n",
      "Epoch: 2/100...  Training Step: 269...  Training loss: 5.6298...  0.3365 sec/batch\n",
      "Epoch: 2/100...  Training Step: 270...  Training loss: 5.5549...  0.3336 sec/batch\n",
      "Epoch: 2/100...  Training Step: 271...  Training loss: 5.4740...  0.3351 sec/batch\n",
      "Epoch: 2/100...  Training Step: 272...  Training loss: 5.5775...  0.3345 sec/batch\n",
      "Epoch: 2/100...  Training Step: 273...  Training loss: 5.5757...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 274...  Training loss: 5.5697...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 275...  Training loss: 5.5656...  0.3348 sec/batch\n",
      "Epoch: 2/100...  Training Step: 276...  Training loss: 5.3327...  0.3350 sec/batch\n",
      "Epoch: 2/100...  Training Step: 277...  Training loss: 5.6124...  0.3338 sec/batch\n",
      "Epoch: 2/100...  Training Step: 278...  Training loss: 5.6183...  0.3329 sec/batch\n",
      "Epoch: 2/100...  Training Step: 279...  Training loss: 5.5299...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 280...  Training loss: 5.3973...  0.3333 sec/batch\n",
      "Epoch: 2/100...  Training Step: 281...  Training loss: 5.4539...  0.3333 sec/batch\n",
      "Epoch: 2/100...  Training Step: 282...  Training loss: 5.4752...  0.3339 sec/batch\n",
      "Epoch: 2/100...  Training Step: 283...  Training loss: 5.5331...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 284...  Training loss: 5.4866...  0.3336 sec/batch\n",
      "Epoch: 2/100...  Training Step: 285...  Training loss: 5.4939...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 286...  Training loss: 5.5163...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 287...  Training loss: 5.6664...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 288...  Training loss: 5.5313...  0.3331 sec/batch\n",
      "Epoch: 2/100...  Training Step: 289...  Training loss: 5.6037...  0.3338 sec/batch\n",
      "Epoch: 2/100...  Training Step: 290...  Training loss: 5.6157...  0.3337 sec/batch\n",
      "Epoch: 2/100...  Training Step: 291...  Training loss: 5.5723...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 292...  Training loss: 5.3643...  0.3361 sec/batch\n",
      "Epoch: 2/100...  Training Step: 293...  Training loss: 5.3782...  0.3340 sec/batch\n",
      "Epoch: 2/100...  Training Step: 294...  Training loss: 5.3811...  0.3356 sec/batch\n",
      "Epoch: 2/100...  Training Step: 295...  Training loss: 5.3641...  0.3382 sec/batch\n",
      "Epoch: 2/100...  Training Step: 296...  Training loss: 5.4970...  0.3369 sec/batch\n",
      "Epoch: 2/100...  Training Step: 297...  Training loss: 5.4534...  0.3364 sec/batch\n",
      "Epoch: 2/100...  Training Step: 298...  Training loss: 5.4363...  0.3375 sec/batch\n",
      "Epoch: 2/100...  Training Step: 299...  Training loss: 5.3082...  0.3345 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100...  Training Step: 300...  Training loss: 5.3303...  0.3370 sec/batch\n",
      "Epoch: 2/100...  Training Step: 301...  Training loss: 5.5105...  0.3354 sec/batch\n",
      "Epoch: 2/100...  Training Step: 302...  Training loss: 5.3467...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 303...  Training loss: 5.3109...  0.3369 sec/batch\n",
      "Epoch: 2/100...  Training Step: 304...  Training loss: 5.4235...  0.3374 sec/batch\n",
      "Epoch: 2/100...  Training Step: 305...  Training loss: 5.6227...  0.3385 sec/batch\n",
      "Epoch: 2/100...  Training Step: 306...  Training loss: 5.5127...  0.3352 sec/batch\n",
      "Epoch: 2/100...  Training Step: 307...  Training loss: 5.4087...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 308...  Training loss: 5.3785...  0.3385 sec/batch\n",
      "Epoch: 2/100...  Training Step: 309...  Training loss: 5.3132...  0.3355 sec/batch\n",
      "Epoch: 2/100...  Training Step: 310...  Training loss: 5.3090...  0.3395 sec/batch\n",
      "Epoch: 2/100...  Training Step: 311...  Training loss: 5.3995...  0.3363 sec/batch\n",
      "Epoch: 2/100...  Training Step: 312...  Training loss: 5.4333...  0.3348 sec/batch\n",
      "Epoch: 2/100...  Training Step: 313...  Training loss: 5.5504...  0.3382 sec/batch\n",
      "Epoch: 2/100...  Training Step: 314...  Training loss: 5.4076...  0.3372 sec/batch\n",
      "Epoch: 2/100...  Training Step: 315...  Training loss: 5.4048...  0.3385 sec/batch\n",
      "Epoch: 2/100...  Training Step: 316...  Training loss: 5.4151...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 317...  Training loss: 5.4408...  0.3372 sec/batch\n",
      "Epoch: 2/100...  Training Step: 318...  Training loss: 5.2103...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 319...  Training loss: 5.2942...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 320...  Training loss: 5.1917...  0.3366 sec/batch\n",
      "Epoch: 2/100...  Training Step: 321...  Training loss: 5.5054...  0.3396 sec/batch\n",
      "Epoch: 2/100...  Training Step: 322...  Training loss: 5.3185...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 323...  Training loss: 5.4086...  0.3364 sec/batch\n",
      "Epoch: 2/100...  Training Step: 324...  Training loss: 5.8452...  0.3376 sec/batch\n",
      "Epoch: 2/100...  Training Step: 325...  Training loss: 5.7729...  0.3365 sec/batch\n",
      "Epoch: 2/100...  Training Step: 326...  Training loss: 5.6438...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 327...  Training loss: 5.4148...  0.3363 sec/batch\n",
      "Epoch: 2/100...  Training Step: 328...  Training loss: 5.4115...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 329...  Training loss: 5.3578...  0.3357 sec/batch\n",
      "Epoch: 2/100...  Training Step: 330...  Training loss: 5.4057...  0.3361 sec/batch\n",
      "Epoch: 2/100...  Training Step: 331...  Training loss: 5.5070...  0.3350 sec/batch\n",
      "Epoch: 2/100...  Training Step: 332...  Training loss: 5.4598...  0.3358 sec/batch\n",
      "Epoch: 2/100...  Training Step: 333...  Training loss: 5.4678...  0.3378 sec/batch\n",
      "Epoch: 2/100...  Training Step: 334...  Training loss: 5.4203...  0.3339 sec/batch\n",
      "Epoch: 2/100...  Training Step: 335...  Training loss: 5.4009...  0.3372 sec/batch\n",
      "Epoch: 2/100...  Training Step: 336...  Training loss: 5.1780...  0.3376 sec/batch\n",
      "Epoch: 2/100...  Training Step: 337...  Training loss: 5.4125...  0.3347 sec/batch\n",
      "Epoch: 2/100...  Training Step: 338...  Training loss: 5.3384...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 339...  Training loss: 5.4593...  0.3393 sec/batch\n",
      "Epoch: 2/100...  Training Step: 340...  Training loss: 5.3750...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 341...  Training loss: 5.4566...  0.3384 sec/batch\n",
      "Epoch: 2/100...  Training Step: 342...  Training loss: 5.4725...  0.3409 sec/batch\n",
      "Epoch: 2/100...  Training Step: 343...  Training loss: 5.3446...  0.3391 sec/batch\n",
      "Epoch: 2/100...  Training Step: 344...  Training loss: 5.3421...  0.3380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 345...  Training loss: 5.3686...  0.3403 sec/batch\n",
      "Epoch: 2/100...  Training Step: 346...  Training loss: 5.4709...  0.3401 sec/batch\n",
      "Epoch: 2/100...  Training Step: 347...  Training loss: 5.4527...  0.3390 sec/batch\n",
      "Epoch: 2/100...  Training Step: 348...  Training loss: 5.4613...  0.3380 sec/batch\n",
      "Epoch: 2/100...  Training Step: 349...  Training loss: 5.6815...  0.3366 sec/batch\n",
      "Epoch: 2/100...  Training Step: 350...  Training loss: 5.5984...  0.3373 sec/batch\n",
      "Epoch: 2/100...  Training Step: 351...  Training loss: 5.4886...  0.3360 sec/batch\n",
      "Epoch: 2/100...  Training Step: 352...  Training loss: 5.3865...  0.3356 sec/batch\n",
      "Epoch: 2/100...  Training Step: 353...  Training loss: 5.3631...  0.3387 sec/batch\n",
      "Epoch: 2/100...  Training Step: 354...  Training loss: 5.3138...  0.3349 sec/batch\n",
      "Epoch: 2/100...  Training Step: 355...  Training loss: 5.3203...  0.3344 sec/batch\n",
      "Epoch: 2/100...  Training Step: 356...  Training loss: 5.3277...  0.3364 sec/batch\n",
      "Epoch: 2/100...  Training Step: 357...  Training loss: 5.4360...  0.3376 sec/batch\n",
      "Epoch: 2/100...  Training Step: 358...  Training loss: 5.3952...  0.3381 sec/batch\n",
      "Epoch: 2/100...  Training Step: 359...  Training loss: 5.4141...  0.3391 sec/batch\n",
      "Epoch: 2/100...  Training Step: 360...  Training loss: 5.3124...  0.3353 sec/batch\n",
      "Epoch: 2/100...  Training Step: 361...  Training loss: 5.3555...  0.3376 sec/batch\n",
      "Epoch: 2/100...  Training Step: 362...  Training loss: 5.3131...  0.3362 sec/batch\n",
      "Epoch: 2/100...  Training Step: 363...  Training loss: 5.2880...  0.3359 sec/batch\n",
      "Epoch: 2/100...  Training Step: 364...  Training loss: 5.3238...  0.3369 sec/batch\n",
      "Epoch: 2/100...  Training Step: 365...  Training loss: 5.7543...  0.3354 sec/batch\n",
      "Epoch: 2/100...  Training Step: 366...  Training loss: 5.5961...  0.3372 sec/batch\n",
      "Epoch: 2/100...  Training Step: 367...  Training loss: 5.6649...  0.3393 sec/batch\n",
      "Epoch: 2/100...  Training Step: 368...  Training loss: 5.5888...  0.3390 sec/batch\n",
      "Epoch: 3/100...  Training Step: 369...  Training loss: 6.0915...  0.3396 sec/batch\n",
      "Epoch: 3/100...  Training Step: 370...  Training loss: 5.8346...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 371...  Training loss: 5.6331...  0.3381 sec/batch\n",
      "Epoch: 3/100...  Training Step: 372...  Training loss: 5.5227...  0.3344 sec/batch\n",
      "Epoch: 3/100...  Training Step: 373...  Training loss: 5.4995...  0.3354 sec/batch\n",
      "Epoch: 3/100...  Training Step: 374...  Training loss: 5.4033...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 375...  Training loss: 5.6441...  0.3394 sec/batch\n",
      "Epoch: 3/100...  Training Step: 376...  Training loss: 5.4254...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 377...  Training loss: 5.5205...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 378...  Training loss: 5.3752...  0.3355 sec/batch\n",
      "Epoch: 3/100...  Training Step: 379...  Training loss: 5.4302...  0.3362 sec/batch\n",
      "Epoch: 3/100...  Training Step: 380...  Training loss: 5.3886...  0.3362 sec/batch\n",
      "Epoch: 3/100...  Training Step: 381...  Training loss: 5.3525...  0.3400 sec/batch\n",
      "Epoch: 3/100...  Training Step: 382...  Training loss: 5.5361...  0.3376 sec/batch\n",
      "Epoch: 3/100...  Training Step: 383...  Training loss: 5.4838...  0.3393 sec/batch\n",
      "Epoch: 3/100...  Training Step: 384...  Training loss: 5.4676...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 385...  Training loss: 5.4801...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 386...  Training loss: 5.3407...  0.3408 sec/batch\n",
      "Epoch: 3/100...  Training Step: 387...  Training loss: 5.4491...  0.3352 sec/batch\n",
      "Epoch: 3/100...  Training Step: 388...  Training loss: 5.3674...  0.3376 sec/batch\n",
      "Epoch: 3/100...  Training Step: 389...  Training loss: 5.4204...  0.3380 sec/batch\n",
      "Epoch: 3/100...  Training Step: 390...  Training loss: 5.3946...  0.3352 sec/batch\n",
      "Epoch: 3/100...  Training Step: 391...  Training loss: 5.3931...  0.3363 sec/batch\n",
      "Epoch: 3/100...  Training Step: 392...  Training loss: 5.4624...  0.3386 sec/batch\n",
      "Epoch: 3/100...  Training Step: 393...  Training loss: 5.4018...  0.3351 sec/batch\n",
      "Epoch: 3/100...  Training Step: 394...  Training loss: 5.3369...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 395...  Training loss: 5.3722...  0.3359 sec/batch\n",
      "Epoch: 3/100...  Training Step: 396...  Training loss: 5.3617...  0.3381 sec/batch\n",
      "Epoch: 3/100...  Training Step: 397...  Training loss: 5.3711...  0.3379 sec/batch\n",
      "Epoch: 3/100...  Training Step: 398...  Training loss: 5.3808...  0.3381 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100...  Training Step: 399...  Training loss: 5.3120...  0.3376 sec/batch\n",
      "Epoch: 3/100...  Training Step: 400...  Training loss: 5.3051...  0.3401 sec/batch\n",
      "Epoch: 3/100...  Training Step: 401...  Training loss: 5.0679...  0.3364 sec/batch\n",
      "Epoch: 3/100...  Training Step: 402...  Training loss: 5.1430...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 403...  Training loss: 5.2891...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 404...  Training loss: 5.4554...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 405...  Training loss: 5.3807...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 406...  Training loss: 5.4038...  0.3359 sec/batch\n",
      "Epoch: 3/100...  Training Step: 407...  Training loss: 5.2884...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 408...  Training loss: 5.2730...  0.3385 sec/batch\n",
      "Epoch: 3/100...  Training Step: 409...  Training loss: 5.3838...  0.3368 sec/batch\n",
      "Epoch: 3/100...  Training Step: 410...  Training loss: 5.3357...  0.3399 sec/batch\n",
      "Epoch: 3/100...  Training Step: 411...  Training loss: 5.4078...  0.3346 sec/batch\n",
      "Epoch: 3/100...  Training Step: 412...  Training loss: 5.4184...  0.3347 sec/batch\n",
      "Epoch: 3/100...  Training Step: 413...  Training loss: 5.3249...  0.3351 sec/batch\n",
      "Epoch: 3/100...  Training Step: 414...  Training loss: 5.3460...  0.3376 sec/batch\n",
      "Epoch: 3/100...  Training Step: 415...  Training loss: 5.3712...  0.3384 sec/batch\n",
      "Epoch: 3/100...  Training Step: 416...  Training loss: 5.2551...  0.3364 sec/batch\n",
      "Epoch: 3/100...  Training Step: 417...  Training loss: 5.2977...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 418...  Training loss: 5.5364...  0.3351 sec/batch\n",
      "Epoch: 3/100...  Training Step: 419...  Training loss: 5.4460...  0.3383 sec/batch\n",
      "Epoch: 3/100...  Training Step: 420...  Training loss: 5.3186...  0.3396 sec/batch\n",
      "Epoch: 3/100...  Training Step: 421...  Training loss: 5.4210...  0.3367 sec/batch\n",
      "Epoch: 3/100...  Training Step: 422...  Training loss: 5.4518...  0.3359 sec/batch\n",
      "Epoch: 3/100...  Training Step: 423...  Training loss: 5.4513...  0.3363 sec/batch\n",
      "Epoch: 3/100...  Training Step: 424...  Training loss: 5.3363...  0.3386 sec/batch\n",
      "Epoch: 3/100...  Training Step: 425...  Training loss: 5.2481...  0.3403 sec/batch\n",
      "Epoch: 3/100...  Training Step: 426...  Training loss: 5.3860...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 427...  Training loss: 5.2001...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 428...  Training loss: 5.2340...  0.3369 sec/batch\n",
      "Epoch: 3/100...  Training Step: 429...  Training loss: 5.2209...  0.3363 sec/batch\n",
      "Epoch: 3/100...  Training Step: 430...  Training loss: 5.2819...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 431...  Training loss: 5.3929...  0.3357 sec/batch\n",
      "Epoch: 3/100...  Training Step: 432...  Training loss: 5.3917...  0.3353 sec/batch\n",
      "Epoch: 3/100...  Training Step: 433...  Training loss: 5.2557...  0.3346 sec/batch\n",
      "Epoch: 3/100...  Training Step: 434...  Training loss: 5.2742...  0.3396 sec/batch\n",
      "Epoch: 3/100...  Training Step: 435...  Training loss: 5.2978...  0.3364 sec/batch\n",
      "Epoch: 3/100...  Training Step: 436...  Training loss: 5.1674...  0.3386 sec/batch\n",
      "Epoch: 3/100...  Training Step: 437...  Training loss: 5.4357...  0.3386 sec/batch\n",
      "Epoch: 3/100...  Training Step: 438...  Training loss: 5.3116...  0.3369 sec/batch\n",
      "Epoch: 3/100...  Training Step: 439...  Training loss: 5.3502...  0.3380 sec/batch\n",
      "Epoch: 3/100...  Training Step: 440...  Training loss: 5.1538...  0.3345 sec/batch\n",
      "Epoch: 3/100...  Training Step: 441...  Training loss: 5.2017...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 442...  Training loss: 5.2641...  0.3360 sec/batch\n",
      "Epoch: 3/100...  Training Step: 443...  Training loss: 5.2069...  0.3348 sec/batch\n",
      "Epoch: 3/100...  Training Step: 444...  Training loss: 5.2063...  0.3355 sec/batch\n",
      "Epoch: 3/100...  Training Step: 445...  Training loss: 5.3181...  0.3381 sec/batch\n",
      "Epoch: 3/100...  Training Step: 446...  Training loss: 5.3565...  0.3399 sec/batch\n",
      "Epoch: 3/100...  Training Step: 447...  Training loss: 5.4126...  0.3385 sec/batch\n",
      "Epoch: 3/100...  Training Step: 448...  Training loss: 5.6202...  0.3390 sec/batch\n",
      "Epoch: 3/100...  Training Step: 449...  Training loss: 5.5030...  0.3363 sec/batch\n",
      "Epoch: 3/100...  Training Step: 450...  Training loss: 5.4787...  0.3369 sec/batch\n",
      "Epoch: 3/100...  Training Step: 451...  Training loss: 5.4560...  0.3360 sec/batch\n",
      "Epoch: 3/100...  Training Step: 452...  Training loss: 5.4609...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 453...  Training loss: 5.5389...  0.3364 sec/batch\n",
      "Epoch: 3/100...  Training Step: 454...  Training loss: 5.4164...  0.3355 sec/batch\n",
      "Epoch: 3/100...  Training Step: 455...  Training loss: 5.3489...  0.3385 sec/batch\n",
      "Epoch: 3/100...  Training Step: 456...  Training loss: 5.4795...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 457...  Training loss: 5.5049...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 458...  Training loss: 5.4565...  0.3411 sec/batch\n",
      "Epoch: 3/100...  Training Step: 459...  Training loss: 5.4653...  0.3373 sec/batch\n",
      "Epoch: 3/100...  Training Step: 460...  Training loss: 5.2146...  0.3349 sec/batch\n",
      "Epoch: 3/100...  Training Step: 461...  Training loss: 5.5245...  0.3395 sec/batch\n",
      "Epoch: 3/100...  Training Step: 462...  Training loss: 5.5309...  0.3383 sec/batch\n",
      "Epoch: 3/100...  Training Step: 463...  Training loss: 5.4651...  0.3350 sec/batch\n",
      "Epoch: 3/100...  Training Step: 464...  Training loss: 5.3634...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 465...  Training loss: 5.3905...  0.3379 sec/batch\n",
      "Epoch: 3/100...  Training Step: 466...  Training loss: 5.3564...  0.3393 sec/batch\n",
      "Epoch: 3/100...  Training Step: 467...  Training loss: 5.4179...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 468...  Training loss: 5.3616...  0.3348 sec/batch\n",
      "Epoch: 3/100...  Training Step: 469...  Training loss: 5.3975...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 470...  Training loss: 5.4135...  0.3399 sec/batch\n",
      "Epoch: 3/100...  Training Step: 471...  Training loss: 5.5822...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 472...  Training loss: 5.4563...  0.3380 sec/batch\n",
      "Epoch: 3/100...  Training Step: 473...  Training loss: 5.5240...  0.3360 sec/batch\n",
      "Epoch: 3/100...  Training Step: 474...  Training loss: 5.5180...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 475...  Training loss: 5.4653...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 476...  Training loss: 5.2433...  0.3360 sec/batch\n",
      "Epoch: 3/100...  Training Step: 477...  Training loss: 5.2518...  0.3375 sec/batch\n",
      "Epoch: 3/100...  Training Step: 478...  Training loss: 5.2699...  0.3359 sec/batch\n",
      "Epoch: 3/100...  Training Step: 479...  Training loss: 5.2317...  0.3386 sec/batch\n",
      "Epoch: 3/100...  Training Step: 480...  Training loss: 5.3661...  0.3382 sec/batch\n",
      "Epoch: 3/100...  Training Step: 481...  Training loss: 5.3399...  0.3365 sec/batch\n",
      "Epoch: 3/100...  Training Step: 482...  Training loss: 5.3254...  0.3394 sec/batch\n",
      "Epoch: 3/100...  Training Step: 483...  Training loss: 5.1850...  0.3375 sec/batch\n",
      "Epoch: 3/100...  Training Step: 484...  Training loss: 5.1961...  0.3391 sec/batch\n",
      "Epoch: 3/100...  Training Step: 485...  Training loss: 5.4061...  0.3373 sec/batch\n",
      "Epoch: 3/100...  Training Step: 486...  Training loss: 5.2412...  0.3407 sec/batch\n",
      "Epoch: 3/100...  Training Step: 487...  Training loss: 5.2070...  0.3373 sec/batch\n",
      "Epoch: 3/100...  Training Step: 488...  Training loss: 5.3171...  0.3358 sec/batch\n",
      "Epoch: 3/100...  Training Step: 489...  Training loss: 5.5192...  0.3379 sec/batch\n",
      "Epoch: 3/100...  Training Step: 490...  Training loss: 5.3987...  0.3366 sec/batch\n",
      "Epoch: 3/100...  Training Step: 491...  Training loss: 5.2909...  0.3357 sec/batch\n",
      "Epoch: 3/100...  Training Step: 492...  Training loss: 5.2655...  0.3367 sec/batch\n",
      "Epoch: 3/100...  Training Step: 493...  Training loss: 5.1959...  0.3371 sec/batch\n",
      "Epoch: 3/100...  Training Step: 494...  Training loss: 5.2193...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 495...  Training loss: 5.3096...  0.3361 sec/batch\n",
      "Epoch: 3/100...  Training Step: 496...  Training loss: 5.3355...  0.3421 sec/batch\n",
      "Epoch: 3/100...  Training Step: 497...  Training loss: 5.4692...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100...  Training Step: 498...  Training loss: 5.2789...  0.3367 sec/batch\n",
      "Epoch: 3/100...  Training Step: 499...  Training loss: 5.2845...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 500...  Training loss: 5.3228...  0.3401 sec/batch\n",
      "Epoch: 3/100...  Training Step: 501...  Training loss: 5.3731...  0.3404 sec/batch\n",
      "Epoch: 3/100...  Training Step: 502...  Training loss: 5.1509...  0.3359 sec/batch\n",
      "Epoch: 3/100...  Training Step: 503...  Training loss: 5.2230...  0.3411 sec/batch\n",
      "Epoch: 3/100...  Training Step: 504...  Training loss: 5.1043...  0.3395 sec/batch\n",
      "Epoch: 3/100...  Training Step: 505...  Training loss: 5.4191...  0.3416 sec/batch\n",
      "Epoch: 3/100...  Training Step: 506...  Training loss: 5.2299...  0.3400 sec/batch\n",
      "Epoch: 3/100...  Training Step: 507...  Training loss: 5.3472...  0.3405 sec/batch\n",
      "Epoch: 3/100...  Training Step: 508...  Training loss: 5.8009...  0.3425 sec/batch\n",
      "Epoch: 3/100...  Training Step: 509...  Training loss: 5.7385...  0.3415 sec/batch\n",
      "Epoch: 3/100...  Training Step: 510...  Training loss: 5.5521...  0.3372 sec/batch\n",
      "Epoch: 3/100...  Training Step: 511...  Training loss: 5.3131...  0.3412 sec/batch\n",
      "Epoch: 3/100...  Training Step: 512...  Training loss: 5.2816...  0.3395 sec/batch\n",
      "Epoch: 3/100...  Training Step: 513...  Training loss: 5.2583...  0.3369 sec/batch\n",
      "Epoch: 3/100...  Training Step: 514...  Training loss: 5.3092...  0.3371 sec/batch\n",
      "Epoch: 3/100...  Training Step: 515...  Training loss: 5.4328...  0.3385 sec/batch\n",
      "Epoch: 3/100...  Training Step: 516...  Training loss: 5.3431...  0.3391 sec/batch\n",
      "Epoch: 3/100...  Training Step: 517...  Training loss: 5.3494...  0.3381 sec/batch\n",
      "Epoch: 3/100...  Training Step: 518...  Training loss: 5.2905...  0.3378 sec/batch\n",
      "Epoch: 3/100...  Training Step: 519...  Training loss: 5.3246...  0.3378 sec/batch\n",
      "Epoch: 3/100...  Training Step: 520...  Training loss: 5.0942...  0.3421 sec/batch\n",
      "Epoch: 3/100...  Training Step: 521...  Training loss: 5.3600...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 522...  Training loss: 5.2421...  0.3419 sec/batch\n",
      "Epoch: 3/100...  Training Step: 523...  Training loss: 5.3663...  0.3387 sec/batch\n",
      "Epoch: 3/100...  Training Step: 524...  Training loss: 5.2861...  0.3414 sec/batch\n",
      "Epoch: 3/100...  Training Step: 525...  Training loss: 5.3838...  0.3418 sec/batch\n",
      "Epoch: 3/100...  Training Step: 526...  Training loss: 5.3938...  0.3411 sec/batch\n",
      "Epoch: 3/100...  Training Step: 527...  Training loss: 5.2887...  0.3403 sec/batch\n",
      "Epoch: 3/100...  Training Step: 528...  Training loss: 5.2828...  0.3422 sec/batch\n",
      "Epoch: 3/100...  Training Step: 529...  Training loss: 5.2945...  0.3382 sec/batch\n",
      "Epoch: 3/100...  Training Step: 530...  Training loss: 5.3908...  0.3417 sec/batch\n",
      "Epoch: 3/100...  Training Step: 531...  Training loss: 5.3772...  0.3373 sec/batch\n",
      "Epoch: 3/100...  Training Step: 532...  Training loss: 5.3931...  0.3384 sec/batch\n",
      "Epoch: 3/100...  Training Step: 533...  Training loss: 5.6058...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 534...  Training loss: 5.5176...  0.3389 sec/batch\n",
      "Epoch: 3/100...  Training Step: 535...  Training loss: 5.4254...  0.3415 sec/batch\n",
      "Epoch: 3/100...  Training Step: 536...  Training loss: 5.3146...  0.3400 sec/batch\n",
      "Epoch: 3/100...  Training Step: 537...  Training loss: 5.2848...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 538...  Training loss: 5.2539...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 539...  Training loss: 5.2549...  0.3411 sec/batch\n",
      "Epoch: 3/100...  Training Step: 540...  Training loss: 5.2730...  0.3409 sec/batch\n",
      "Epoch: 3/100...  Training Step: 541...  Training loss: 5.3809...  0.3377 sec/batch\n",
      "Epoch: 3/100...  Training Step: 542...  Training loss: 5.3441...  0.3398 sec/batch\n",
      "Epoch: 3/100...  Training Step: 543...  Training loss: 5.3464...  0.3410 sec/batch\n",
      "Epoch: 3/100...  Training Step: 544...  Training loss: 5.2485...  0.3411 sec/batch\n",
      "Epoch: 3/100...  Training Step: 545...  Training loss: 5.3035...  0.3423 sec/batch\n",
      "Epoch: 3/100...  Training Step: 546...  Training loss: 5.2583...  0.3385 sec/batch\n",
      "Epoch: 3/100...  Training Step: 547...  Training loss: 5.2471...  0.3388 sec/batch\n",
      "Epoch: 3/100...  Training Step: 548...  Training loss: 5.2593...  0.3383 sec/batch\n",
      "Epoch: 3/100...  Training Step: 549...  Training loss: 5.6758...  0.3375 sec/batch\n",
      "Epoch: 3/100...  Training Step: 550...  Training loss: 5.5492...  0.3406 sec/batch\n",
      "Epoch: 3/100...  Training Step: 551...  Training loss: 5.6235...  0.3420 sec/batch\n",
      "Epoch: 3/100...  Training Step: 552...  Training loss: 5.5527...  0.3405 sec/batch\n",
      "Epoch: 4/100...  Training Step: 553...  Training loss: 6.0317...  0.3410 sec/batch\n",
      "Epoch: 4/100...  Training Step: 554...  Training loss: 5.8237...  0.3382 sec/batch\n",
      "Epoch: 4/100...  Training Step: 555...  Training loss: 5.5820...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 556...  Training loss: 5.5078...  0.3370 sec/batch\n",
      "Epoch: 4/100...  Training Step: 557...  Training loss: 5.5229...  0.3368 sec/batch\n",
      "Epoch: 4/100...  Training Step: 558...  Training loss: 5.4339...  0.3406 sec/batch\n",
      "Epoch: 4/100...  Training Step: 559...  Training loss: 5.6285...  0.3418 sec/batch\n",
      "Epoch: 4/100...  Training Step: 560...  Training loss: 5.4150...  0.3419 sec/batch\n",
      "Epoch: 4/100...  Training Step: 561...  Training loss: 5.4839...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 562...  Training loss: 5.3428...  0.3404 sec/batch\n",
      "Epoch: 4/100...  Training Step: 563...  Training loss: 5.3806...  0.3412 sec/batch\n",
      "Epoch: 4/100...  Training Step: 564...  Training loss: 5.3522...  0.3366 sec/batch\n",
      "Epoch: 4/100...  Training Step: 565...  Training loss: 5.3257...  0.3392 sec/batch\n",
      "Epoch: 4/100...  Training Step: 566...  Training loss: 5.5072...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 567...  Training loss: 5.4669...  0.3387 sec/batch\n",
      "Epoch: 4/100...  Training Step: 568...  Training loss: 5.4453...  0.3434 sec/batch\n",
      "Epoch: 4/100...  Training Step: 569...  Training loss: 5.4698...  0.3381 sec/batch\n",
      "Epoch: 4/100...  Training Step: 570...  Training loss: 5.3096...  0.3402 sec/batch\n",
      "Epoch: 4/100...  Training Step: 571...  Training loss: 5.4541...  0.3411 sec/batch\n",
      "Epoch: 4/100...  Training Step: 572...  Training loss: 5.3614...  0.3403 sec/batch\n",
      "Epoch: 4/100...  Training Step: 573...  Training loss: 5.4082...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 574...  Training loss: 5.3774...  0.3378 sec/batch\n",
      "Epoch: 4/100...  Training Step: 575...  Training loss: 5.3743...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 576...  Training loss: 5.4231...  0.3419 sec/batch\n",
      "Epoch: 4/100...  Training Step: 577...  Training loss: 5.3903...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 578...  Training loss: 5.3203...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 579...  Training loss: 5.3505...  0.3407 sec/batch\n",
      "Epoch: 4/100...  Training Step: 580...  Training loss: 5.3229...  0.3420 sec/batch\n",
      "Epoch: 4/100...  Training Step: 581...  Training loss: 5.3525...  0.3404 sec/batch\n",
      "Epoch: 4/100...  Training Step: 582...  Training loss: 5.3488...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 583...  Training loss: 5.2795...  0.3374 sec/batch\n",
      "Epoch: 4/100...  Training Step: 584...  Training loss: 5.2814...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 585...  Training loss: 5.0221...  0.3382 sec/batch\n",
      "Epoch: 4/100...  Training Step: 586...  Training loss: 5.1096...  0.3402 sec/batch\n",
      "Epoch: 4/100...  Training Step: 587...  Training loss: 5.2720...  0.3424 sec/batch\n",
      "Epoch: 4/100...  Training Step: 588...  Training loss: 5.4614...  0.3415 sec/batch\n",
      "Epoch: 4/100...  Training Step: 589...  Training loss: 5.3676...  0.3372 sec/batch\n",
      "Epoch: 4/100...  Training Step: 590...  Training loss: 5.4117...  0.3392 sec/batch\n",
      "Epoch: 4/100...  Training Step: 591...  Training loss: 5.2715...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 592...  Training loss: 5.2573...  0.3374 sec/batch\n",
      "Epoch: 4/100...  Training Step: 593...  Training loss: 5.3936...  0.3421 sec/batch\n",
      "Epoch: 4/100...  Training Step: 594...  Training loss: 5.3323...  0.3420 sec/batch\n",
      "Epoch: 4/100...  Training Step: 595...  Training loss: 5.4031...  0.3425 sec/batch\n",
      "Epoch: 4/100...  Training Step: 596...  Training loss: 5.4268...  0.3385 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100...  Training Step: 597...  Training loss: 5.3087...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 598...  Training loss: 5.3131...  0.3404 sec/batch\n",
      "Epoch: 4/100...  Training Step: 599...  Training loss: 5.3548...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 600...  Training loss: 5.2365...  0.3401 sec/batch\n",
      "Epoch: 4/100...  Training Step: 601...  Training loss: 5.2936...  0.3429 sec/batch\n",
      "Epoch: 4/100...  Training Step: 602...  Training loss: 5.5175...  0.3415 sec/batch\n",
      "Epoch: 4/100...  Training Step: 603...  Training loss: 5.4185...  0.3409 sec/batch\n",
      "Epoch: 4/100...  Training Step: 604...  Training loss: 5.3022...  0.3389 sec/batch\n",
      "Epoch: 4/100...  Training Step: 605...  Training loss: 5.4163...  0.3413 sec/batch\n",
      "Epoch: 4/100...  Training Step: 606...  Training loss: 5.4383...  0.3414 sec/batch\n",
      "Epoch: 4/100...  Training Step: 607...  Training loss: 5.4318...  0.3369 sec/batch\n",
      "Epoch: 4/100...  Training Step: 608...  Training loss: 5.3137...  0.3397 sec/batch\n",
      "Epoch: 4/100...  Training Step: 609...  Training loss: 5.2304...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 610...  Training loss: 5.3760...  0.3416 sec/batch\n",
      "Epoch: 4/100...  Training Step: 611...  Training loss: 5.1950...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 612...  Training loss: 5.2311...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 613...  Training loss: 5.2097...  0.3374 sec/batch\n",
      "Epoch: 4/100...  Training Step: 614...  Training loss: 5.2732...  0.3380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 615...  Training loss: 5.3687...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 616...  Training loss: 5.3723...  0.3392 sec/batch\n",
      "Epoch: 4/100...  Training Step: 617...  Training loss: 5.2292...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 618...  Training loss: 5.2501...  0.3409 sec/batch\n",
      "Epoch: 4/100...  Training Step: 619...  Training loss: 5.2760...  0.3403 sec/batch\n",
      "Epoch: 4/100...  Training Step: 620...  Training loss: 5.1456...  0.3380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 621...  Training loss: 5.3883...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 622...  Training loss: 5.2918...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 623...  Training loss: 5.3517...  0.3378 sec/batch\n",
      "Epoch: 4/100...  Training Step: 624...  Training loss: 5.1415...  0.3420 sec/batch\n",
      "Epoch: 4/100...  Training Step: 625...  Training loss: 5.1690...  0.3410 sec/batch\n",
      "Epoch: 4/100...  Training Step: 626...  Training loss: 5.2435...  0.3377 sec/batch\n",
      "Epoch: 4/100...  Training Step: 627...  Training loss: 5.1865...  0.3422 sec/batch\n",
      "Epoch: 4/100...  Training Step: 628...  Training loss: 5.1922...  0.3393 sec/batch\n",
      "Epoch: 4/100...  Training Step: 629...  Training loss: 5.2889...  0.3406 sec/batch\n",
      "Epoch: 4/100...  Training Step: 630...  Training loss: 5.3354...  0.3412 sec/batch\n",
      "Epoch: 4/100...  Training Step: 631...  Training loss: 5.3916...  0.3404 sec/batch\n",
      "Epoch: 4/100...  Training Step: 632...  Training loss: 5.5810...  0.3370 sec/batch\n",
      "Epoch: 4/100...  Training Step: 633...  Training loss: 5.4949...  0.3372 sec/batch\n",
      "Epoch: 4/100...  Training Step: 634...  Training loss: 5.4730...  0.3420 sec/batch\n",
      "Epoch: 4/100...  Training Step: 635...  Training loss: 5.4480...  0.3413 sec/batch\n",
      "Epoch: 4/100...  Training Step: 636...  Training loss: 5.4271...  0.3403 sec/batch\n",
      "Epoch: 4/100...  Training Step: 637...  Training loss: 5.5358...  0.3407 sec/batch\n",
      "Epoch: 4/100...  Training Step: 638...  Training loss: 5.4181...  0.3364 sec/batch\n",
      "Epoch: 4/100...  Training Step: 639...  Training loss: 5.3488...  0.3380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 640...  Training loss: 5.4835...  0.3396 sec/batch\n",
      "Epoch: 4/100...  Training Step: 641...  Training loss: 5.5114...  0.3392 sec/batch\n",
      "Epoch: 4/100...  Training Step: 642...  Training loss: 5.4672...  0.3421 sec/batch\n",
      "Epoch: 4/100...  Training Step: 643...  Training loss: 5.4579...  0.3407 sec/batch\n",
      "Epoch: 4/100...  Training Step: 644...  Training loss: 5.2146...  0.3388 sec/batch\n",
      "Epoch: 4/100...  Training Step: 645...  Training loss: 5.4989...  0.3409 sec/batch\n",
      "Epoch: 4/100...  Training Step: 646...  Training loss: 5.5262...  0.3412 sec/batch\n",
      "Epoch: 4/100...  Training Step: 647...  Training loss: 5.4221...  0.3380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 648...  Training loss: 5.3103...  0.3393 sec/batch\n",
      "Epoch: 4/100...  Training Step: 649...  Training loss: 5.3388...  0.3398 sec/batch\n",
      "Epoch: 4/100...  Training Step: 650...  Training loss: 5.3336...  0.3377 sec/batch\n",
      "Epoch: 4/100...  Training Step: 651...  Training loss: 5.4049...  0.3388 sec/batch\n",
      "Epoch: 4/100...  Training Step: 652...  Training loss: 5.3442...  0.3400 sec/batch\n",
      "Epoch: 4/100...  Training Step: 653...  Training loss: 5.3836...  0.3411 sec/batch\n",
      "Epoch: 4/100...  Training Step: 654...  Training loss: 5.3849...  0.3418 sec/batch\n",
      "Epoch: 4/100...  Training Step: 655...  Training loss: 5.5604...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 656...  Training loss: 5.4395...  0.3387 sec/batch\n",
      "Epoch: 4/100...  Training Step: 657...  Training loss: 5.4840...  0.3413 sec/batch\n",
      "Epoch: 4/100...  Training Step: 658...  Training loss: 5.5149...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 659...  Training loss: 5.4734...  0.3379 sec/batch\n",
      "Epoch: 4/100...  Training Step: 660...  Training loss: 5.2606...  0.3400 sec/batch\n",
      "Epoch: 4/100...  Training Step: 661...  Training loss: 5.2489...  0.3422 sec/batch\n",
      "Epoch: 4/100...  Training Step: 662...  Training loss: 5.2584...  0.3382 sec/batch\n",
      "Epoch: 4/100...  Training Step: 663...  Training loss: 5.2318...  0.3410 sec/batch\n",
      "Epoch: 4/100...  Training Step: 664...  Training loss: 5.3432...  0.3412 sec/batch\n",
      "Epoch: 4/100...  Training Step: 665...  Training loss: 5.3191...  0.3405 sec/batch\n",
      "Epoch: 4/100...  Training Step: 666...  Training loss: 5.3031...  0.3405 sec/batch\n",
      "Epoch: 4/100...  Training Step: 667...  Training loss: 5.1593...  0.3385 sec/batch\n",
      "Epoch: 4/100...  Training Step: 668...  Training loss: 5.1924...  0.3389 sec/batch\n",
      "Epoch: 4/100...  Training Step: 669...  Training loss: 5.4043...  0.3436 sec/batch\n",
      "Epoch: 4/100...  Training Step: 670...  Training loss: 5.2150...  0.3416 sec/batch\n",
      "Epoch: 4/100...  Training Step: 671...  Training loss: 5.1760...  0.3400 sec/batch\n",
      "Epoch: 4/100...  Training Step: 672...  Training loss: 5.3007...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 673...  Training loss: 5.4975...  0.3417 sec/batch\n",
      "Epoch: 4/100...  Training Step: 674...  Training loss: 5.3894...  0.3381 sec/batch\n",
      "Epoch: 4/100...  Training Step: 675...  Training loss: 5.2843...  0.3409 sec/batch\n",
      "Epoch: 4/100...  Training Step: 676...  Training loss: 5.2638...  0.3374 sec/batch\n",
      "Epoch: 4/100...  Training Step: 677...  Training loss: 5.1940...  0.3393 sec/batch\n",
      "Epoch: 4/100...  Training Step: 678...  Training loss: 5.2158...  0.3376 sec/batch\n",
      "Epoch: 4/100...  Training Step: 679...  Training loss: 5.3150...  0.3399 sec/batch\n",
      "Epoch: 4/100...  Training Step: 680...  Training loss: 5.3359...  0.3432 sec/batch\n",
      "Epoch: 4/100...  Training Step: 681...  Training loss: 5.4635...  0.3423 sec/batch\n",
      "Epoch: 4/100...  Training Step: 682...  Training loss: 5.2772...  0.3378 sec/batch\n",
      "Epoch: 4/100...  Training Step: 683...  Training loss: 5.2851...  0.3380 sec/batch\n",
      "Epoch: 4/100...  Training Step: 684...  Training loss: 5.3102...  0.3370 sec/batch\n",
      "Epoch: 4/100...  Training Step: 685...  Training loss: 5.3698...  0.3417 sec/batch\n",
      "Epoch: 4/100...  Training Step: 686...  Training loss: 5.1208...  0.3407 sec/batch\n",
      "Epoch: 4/100...  Training Step: 687...  Training loss: 5.1962...  0.3424 sec/batch\n",
      "Epoch: 4/100...  Training Step: 688...  Training loss: 5.0951...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 689...  Training loss: 5.4057...  0.3421 sec/batch\n",
      "Epoch: 4/100...  Training Step: 690...  Training loss: 5.2115...  0.3420 sec/batch\n",
      "Epoch: 4/100...  Training Step: 691...  Training loss: 5.3096...  0.3373 sec/batch\n",
      "Epoch: 4/100...  Training Step: 692...  Training loss: 5.7925...  0.3390 sec/batch\n",
      "Epoch: 4/100...  Training Step: 693...  Training loss: 5.7318...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 694...  Training loss: 5.5298...  0.3406 sec/batch\n",
      "Epoch: 4/100...  Training Step: 695...  Training loss: 5.3175...  0.3375 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100...  Training Step: 696...  Training loss: 5.2666...  0.3403 sec/batch\n",
      "Epoch: 4/100...  Training Step: 697...  Training loss: 5.2380...  0.3372 sec/batch\n",
      "Epoch: 4/100...  Training Step: 698...  Training loss: 5.2753...  0.3388 sec/batch\n",
      "Epoch: 4/100...  Training Step: 699...  Training loss: 5.4055...  0.3398 sec/batch\n",
      "Epoch: 4/100...  Training Step: 700...  Training loss: 5.3397...  0.3382 sec/batch\n",
      "Epoch: 4/100...  Training Step: 701...  Training loss: 5.3405...  0.3396 sec/batch\n",
      "Epoch: 4/100...  Training Step: 702...  Training loss: 5.2792...  0.3417 sec/batch\n",
      "Epoch: 4/100...  Training Step: 703...  Training loss: 5.3065...  0.3383 sec/batch\n",
      "Epoch: 4/100...  Training Step: 704...  Training loss: 5.0585...  0.3382 sec/batch\n",
      "Epoch: 4/100...  Training Step: 705...  Training loss: 5.3315...  0.3431 sec/batch\n",
      "Epoch: 4/100...  Training Step: 706...  Training loss: 5.2424...  0.3425 sec/batch\n",
      "Epoch: 4/100...  Training Step: 707...  Training loss: 5.3407...  0.3412 sec/batch\n",
      "Epoch: 4/100...  Training Step: 708...  Training loss: 5.2906...  0.3410 sec/batch\n",
      "Epoch: 4/100...  Training Step: 709...  Training loss: 5.3848...  0.3405 sec/batch\n",
      "Epoch: 4/100...  Training Step: 710...  Training loss: 5.3845...  0.3406 sec/batch\n",
      "Epoch: 4/100...  Training Step: 711...  Training loss: 5.2709...  0.3413 sec/batch\n",
      "Epoch: 4/100...  Training Step: 712...  Training loss: 5.2643...  0.3364 sec/batch\n",
      "Epoch: 4/100...  Training Step: 713...  Training loss: 5.2868...  0.3386 sec/batch\n",
      "Epoch: 4/100...  Training Step: 714...  Training loss: 5.3557...  0.3369 sec/batch\n",
      "Epoch: 4/100...  Training Step: 715...  Training loss: 5.3612...  0.3384 sec/batch\n",
      "Epoch: 4/100...  Training Step: 716...  Training loss: 5.3870...  0.3391 sec/batch\n",
      "Epoch: 4/100...  Training Step: 717...  Training loss: 5.5830...  0.3424 sec/batch\n",
      "Epoch: 4/100...  Training Step: 718...  Training loss: 5.5100...  0.3396 sec/batch\n",
      "Epoch: 4/100...  Training Step: 719...  Training loss: 5.4058...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 720...  Training loss: 5.2996...  0.3406 sec/batch\n",
      "Epoch: 4/100...  Training Step: 721...  Training loss: 5.2759...  0.3399 sec/batch\n",
      "Epoch: 4/100...  Training Step: 722...  Training loss: 5.2454...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 723...  Training loss: 5.2371...  0.3402 sec/batch\n",
      "Epoch: 4/100...  Training Step: 724...  Training loss: 5.2646...  0.3422 sec/batch\n",
      "Epoch: 4/100...  Training Step: 725...  Training loss: 5.3913...  0.3375 sec/batch\n",
      "Epoch: 4/100...  Training Step: 726...  Training loss: 5.3104...  0.3379 sec/batch\n",
      "Epoch: 4/100...  Training Step: 727...  Training loss: 5.3226...  0.3413 sec/batch\n",
      "Epoch: 4/100...  Training Step: 728...  Training loss: 5.2258...  0.3419 sec/batch\n",
      "Epoch: 4/100...  Training Step: 729...  Training loss: 5.2930...  0.3418 sec/batch\n",
      "Epoch: 4/100...  Training Step: 730...  Training loss: 5.2434...  0.3414 sec/batch\n",
      "Epoch: 4/100...  Training Step: 731...  Training loss: 5.2376...  0.3401 sec/batch\n",
      "Epoch: 4/100...  Training Step: 732...  Training loss: 5.2471...  0.3367 sec/batch\n",
      "Epoch: 4/100...  Training Step: 733...  Training loss: 5.6778...  0.3395 sec/batch\n",
      "Epoch: 4/100...  Training Step: 734...  Training loss: 5.5360...  0.3397 sec/batch\n",
      "Epoch: 4/100...  Training Step: 735...  Training loss: 5.5969...  0.3426 sec/batch\n",
      "Epoch: 4/100...  Training Step: 736...  Training loss: 5.5319...  0.3410 sec/batch\n",
      "Epoch: 5/100...  Training Step: 737...  Training loss: 5.8468...  0.3392 sec/batch\n",
      "Epoch: 5/100...  Training Step: 738...  Training loss: 5.7663...  0.3393 sec/batch\n",
      "Epoch: 5/100...  Training Step: 739...  Training loss: 5.5575...  0.3382 sec/batch\n",
      "Epoch: 5/100...  Training Step: 740...  Training loss: 5.4740...  0.3377 sec/batch\n",
      "Epoch: 5/100...  Training Step: 741...  Training loss: 5.4690...  0.3362 sec/batch\n",
      "Epoch: 5/100...  Training Step: 742...  Training loss: 5.3583...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 743...  Training loss: 5.5955...  0.3383 sec/batch\n",
      "Epoch: 5/100...  Training Step: 744...  Training loss: 5.3821...  0.3380 sec/batch\n",
      "Epoch: 5/100...  Training Step: 745...  Training loss: 5.4734...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 746...  Training loss: 5.3139...  0.3369 sec/batch\n",
      "Epoch: 5/100...  Training Step: 747...  Training loss: 5.3497...  0.3394 sec/batch\n",
      "Epoch: 5/100...  Training Step: 748...  Training loss: 5.3376...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 749...  Training loss: 5.2933...  0.3433 sec/batch\n",
      "Epoch: 5/100...  Training Step: 750...  Training loss: 5.4809...  0.3406 sec/batch\n",
      "Epoch: 5/100...  Training Step: 751...  Training loss: 5.4504...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 752...  Training loss: 5.4195...  0.3404 sec/batch\n",
      "Epoch: 5/100...  Training Step: 753...  Training loss: 5.4537...  0.3397 sec/batch\n",
      "Epoch: 5/100...  Training Step: 754...  Training loss: 5.2933...  0.3406 sec/batch\n",
      "Epoch: 5/100...  Training Step: 755...  Training loss: 5.4138...  0.3384 sec/batch\n",
      "Epoch: 5/100...  Training Step: 756...  Training loss: 5.3231...  0.3378 sec/batch\n",
      "Epoch: 5/100...  Training Step: 757...  Training loss: 5.3713...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 758...  Training loss: 5.3455...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 759...  Training loss: 5.3420...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 760...  Training loss: 5.3992...  0.3385 sec/batch\n",
      "Epoch: 5/100...  Training Step: 761...  Training loss: 5.3635...  0.3415 sec/batch\n",
      "Epoch: 5/100...  Training Step: 762...  Training loss: 5.2830...  0.3413 sec/batch\n",
      "Epoch: 5/100...  Training Step: 763...  Training loss: 5.3221...  0.3406 sec/batch\n",
      "Epoch: 5/100...  Training Step: 764...  Training loss: 5.3165...  0.3396 sec/batch\n",
      "Epoch: 5/100...  Training Step: 765...  Training loss: 5.3444...  0.3366 sec/batch\n",
      "Epoch: 5/100...  Training Step: 766...  Training loss: 5.3658...  0.3374 sec/batch\n",
      "Epoch: 5/100...  Training Step: 767...  Training loss: 5.3069...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 768...  Training loss: 5.2985...  0.3385 sec/batch\n",
      "Epoch: 5/100...  Training Step: 769...  Training loss: 5.0268...  0.3413 sec/batch\n",
      "Epoch: 5/100...  Training Step: 770...  Training loss: 5.1070...  0.3413 sec/batch\n",
      "Epoch: 5/100...  Training Step: 771...  Training loss: 5.2543...  0.3389 sec/batch\n",
      "Epoch: 5/100...  Training Step: 772...  Training loss: 5.4392...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 773...  Training loss: 5.3476...  0.3418 sec/batch\n",
      "Epoch: 5/100...  Training Step: 774...  Training loss: 5.3777...  0.3399 sec/batch\n",
      "Epoch: 5/100...  Training Step: 775...  Training loss: 5.2455...  0.3405 sec/batch\n",
      "Epoch: 5/100...  Training Step: 776...  Training loss: 5.2457...  0.3391 sec/batch\n",
      "Epoch: 5/100...  Training Step: 777...  Training loss: 5.3753...  0.3429 sec/batch\n",
      "Epoch: 5/100...  Training Step: 778...  Training loss: 5.3311...  0.3391 sec/batch\n",
      "Epoch: 5/100...  Training Step: 779...  Training loss: 5.3986...  0.3406 sec/batch\n",
      "Epoch: 5/100...  Training Step: 780...  Training loss: 5.3980...  0.3416 sec/batch\n",
      "Epoch: 5/100...  Training Step: 781...  Training loss: 5.2916...  0.3416 sec/batch\n",
      "Epoch: 5/100...  Training Step: 782...  Training loss: 5.2943...  0.3416 sec/batch\n",
      "Epoch: 5/100...  Training Step: 783...  Training loss: 5.3391...  0.3383 sec/batch\n",
      "Epoch: 5/100...  Training Step: 784...  Training loss: 5.2142...  0.3420 sec/batch\n",
      "Epoch: 5/100...  Training Step: 785...  Training loss: 5.2700...  0.3388 sec/batch\n",
      "Epoch: 5/100...  Training Step: 786...  Training loss: 5.4855...  0.3399 sec/batch\n",
      "Epoch: 5/100...  Training Step: 787...  Training loss: 5.3966...  0.3406 sec/batch\n",
      "Epoch: 5/100...  Training Step: 788...  Training loss: 5.2689...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 789...  Training loss: 5.3785...  0.3376 sec/batch\n",
      "Epoch: 5/100...  Training Step: 790...  Training loss: 5.4019...  0.3371 sec/batch\n",
      "Epoch: 5/100...  Training Step: 791...  Training loss: 5.4033...  0.3355 sec/batch\n",
      "Epoch: 5/100...  Training Step: 792...  Training loss: 5.2932...  0.3386 sec/batch\n",
      "Epoch: 5/100...  Training Step: 793...  Training loss: 5.2091...  0.3377 sec/batch\n",
      "Epoch: 5/100...  Training Step: 794...  Training loss: 5.3655...  0.3369 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100...  Training Step: 795...  Training loss: 5.1678...  0.3375 sec/batch\n",
      "Epoch: 5/100...  Training Step: 796...  Training loss: 5.2121...  0.3390 sec/batch\n",
      "Epoch: 5/100...  Training Step: 797...  Training loss: 5.2067...  0.3382 sec/batch\n",
      "Epoch: 5/100...  Training Step: 798...  Training loss: 5.2478...  0.3419 sec/batch\n",
      "Epoch: 5/100...  Training Step: 799...  Training loss: 5.3409...  0.3398 sec/batch\n",
      "Epoch: 5/100...  Training Step: 800...  Training loss: 5.3647...  0.3422 sec/batch\n",
      "Epoch: 5/100...  Training Step: 801...  Training loss: 5.2018...  0.3418 sec/batch\n",
      "Epoch: 5/100...  Training Step: 802...  Training loss: 5.2408...  0.3387 sec/batch\n",
      "Epoch: 5/100...  Training Step: 803...  Training loss: 5.2555...  0.3389 sec/batch\n",
      "Epoch: 5/100...  Training Step: 804...  Training loss: 5.1270...  0.3409 sec/batch\n",
      "Epoch: 5/100...  Training Step: 805...  Training loss: 5.3713...  0.3415 sec/batch\n",
      "Epoch: 5/100...  Training Step: 806...  Training loss: 5.2656...  0.3429 sec/batch\n",
      "Epoch: 5/100...  Training Step: 807...  Training loss: 5.3248...  0.3396 sec/batch\n",
      "Epoch: 5/100...  Training Step: 808...  Training loss: 5.1043...  0.3418 sec/batch\n",
      "Epoch: 5/100...  Training Step: 809...  Training loss: 5.1444...  0.3428 sec/batch\n",
      "Epoch: 5/100...  Training Step: 810...  Training loss: 5.2166...  0.3447 sec/batch\n",
      "Epoch: 5/100...  Training Step: 811...  Training loss: 5.1690...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 812...  Training loss: 5.1640...  0.3362 sec/batch\n",
      "Epoch: 5/100...  Training Step: 813...  Training loss: 5.2735...  0.3390 sec/batch\n",
      "Epoch: 5/100...  Training Step: 814...  Training loss: 5.3095...  0.3398 sec/batch\n",
      "Epoch: 5/100...  Training Step: 815...  Training loss: 5.3612...  0.3399 sec/batch\n",
      "Epoch: 5/100...  Training Step: 816...  Training loss: 5.5620...  0.3427 sec/batch\n",
      "Epoch: 5/100...  Training Step: 817...  Training loss: 5.4748...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 818...  Training loss: 5.4695...  0.3389 sec/batch\n",
      "Epoch: 5/100...  Training Step: 819...  Training loss: 5.4290...  0.3394 sec/batch\n",
      "Epoch: 5/100...  Training Step: 820...  Training loss: 5.3912...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 821...  Training loss: 5.5032...  0.3412 sec/batch\n",
      "Epoch: 5/100...  Training Step: 822...  Training loss: 5.3926...  0.3431 sec/batch\n",
      "Epoch: 5/100...  Training Step: 823...  Training loss: 5.3387...  0.3409 sec/batch\n",
      "Epoch: 5/100...  Training Step: 824...  Training loss: 5.4445...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 825...  Training loss: 5.4786...  0.3420 sec/batch\n",
      "Epoch: 5/100...  Training Step: 826...  Training loss: 5.4487...  0.3415 sec/batch\n",
      "Epoch: 5/100...  Training Step: 827...  Training loss: 5.4479...  0.3393 sec/batch\n",
      "Epoch: 5/100...  Training Step: 828...  Training loss: 5.1781...  0.3389 sec/batch\n",
      "Epoch: 5/100...  Training Step: 829...  Training loss: 5.4784...  0.3396 sec/batch\n",
      "Epoch: 5/100...  Training Step: 830...  Training loss: 5.4988...  0.3385 sec/batch\n",
      "Epoch: 5/100...  Training Step: 831...  Training loss: 5.4024...  0.3410 sec/batch\n",
      "Epoch: 5/100...  Training Step: 832...  Training loss: 5.3031...  0.3384 sec/batch\n",
      "Epoch: 5/100...  Training Step: 833...  Training loss: 5.3091...  0.3423 sec/batch\n",
      "Epoch: 5/100...  Training Step: 834...  Training loss: 5.3070...  0.3369 sec/batch\n",
      "Epoch: 5/100...  Training Step: 835...  Training loss: 5.3888...  0.3375 sec/batch\n",
      "Epoch: 5/100...  Training Step: 836...  Training loss: 5.3092...  0.3435 sec/batch\n",
      "Epoch: 5/100...  Training Step: 837...  Training loss: 5.3583...  0.3388 sec/batch\n",
      "Epoch: 5/100...  Training Step: 838...  Training loss: 5.3545...  0.3425 sec/batch\n",
      "Epoch: 5/100...  Training Step: 839...  Training loss: 5.5268...  0.3400 sec/batch\n",
      "Epoch: 5/100...  Training Step: 840...  Training loss: 5.3966...  0.3389 sec/batch\n",
      "Epoch: 5/100...  Training Step: 841...  Training loss: 5.4584...  0.3381 sec/batch\n",
      "Epoch: 5/100...  Training Step: 842...  Training loss: 5.4839...  0.3425 sec/batch\n",
      "Epoch: 5/100...  Training Step: 843...  Training loss: 5.4328...  0.3424 sec/batch\n",
      "Epoch: 5/100...  Training Step: 844...  Training loss: 5.2086...  0.3397 sec/batch\n",
      "Epoch: 5/100...  Training Step: 845...  Training loss: 5.2066...  0.3380 sec/batch\n",
      "Epoch: 5/100...  Training Step: 846...  Training loss: 5.2255...  0.3385 sec/batch\n",
      "Epoch: 5/100...  Training Step: 847...  Training loss: 5.2127...  0.3439 sec/batch\n",
      "Epoch: 5/100...  Training Step: 848...  Training loss: 5.3145...  0.3380 sec/batch\n",
      "Epoch: 5/100...  Training Step: 849...  Training loss: 5.2979...  0.3417 sec/batch\n",
      "Epoch: 5/100...  Training Step: 850...  Training loss: 5.2801...  0.3412 sec/batch\n",
      "Epoch: 5/100...  Training Step: 851...  Training loss: 5.1498...  0.3405 sec/batch\n",
      "Epoch: 5/100...  Training Step: 852...  Training loss: 5.1795...  0.3420 sec/batch\n",
      "Epoch: 5/100...  Training Step: 853...  Training loss: 5.3623...  0.3417 sec/batch\n",
      "Epoch: 5/100...  Training Step: 854...  Training loss: 5.2006...  0.3405 sec/batch\n",
      "Epoch: 5/100...  Training Step: 855...  Training loss: 5.1553...  0.3391 sec/batch\n",
      "Epoch: 5/100...  Training Step: 856...  Training loss: 5.2695...  0.3402 sec/batch\n",
      "Epoch: 5/100...  Training Step: 857...  Training loss: 5.4773...  0.3398 sec/batch\n",
      "Epoch: 5/100...  Training Step: 858...  Training loss: 5.3754...  0.3415 sec/batch\n",
      "Epoch: 5/100...  Training Step: 859...  Training loss: 5.2585...  0.3385 sec/batch\n",
      "Epoch: 5/100...  Training Step: 860...  Training loss: 5.2290...  0.3379 sec/batch\n",
      "Epoch: 5/100...  Training Step: 861...  Training loss: 5.1507...  0.3384 sec/batch\n",
      "Epoch: 5/100...  Training Step: 862...  Training loss: 5.1945...  0.3393 sec/batch\n",
      "Epoch: 5/100...  Training Step: 863...  Training loss: 5.2636...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 864...  Training loss: 5.3055...  0.3394 sec/batch\n",
      "Epoch: 5/100...  Training Step: 865...  Training loss: 5.4320...  0.3409 sec/batch\n",
      "Epoch: 5/100...  Training Step: 866...  Training loss: 5.2475...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 867...  Training loss: 5.2603...  0.3409 sec/batch\n",
      "Epoch: 5/100...  Training Step: 868...  Training loss: 5.2828...  0.3420 sec/batch\n",
      "Epoch: 5/100...  Training Step: 869...  Training loss: 5.3468...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 870...  Training loss: 5.1078...  0.3419 sec/batch\n",
      "Epoch: 5/100...  Training Step: 871...  Training loss: 5.1808...  0.3383 sec/batch\n",
      "Epoch: 5/100...  Training Step: 872...  Training loss: 5.0886...  0.3391 sec/batch\n",
      "Epoch: 5/100...  Training Step: 873...  Training loss: 5.3943...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 874...  Training loss: 5.1883...  0.3378 sec/batch\n",
      "Epoch: 5/100...  Training Step: 875...  Training loss: 5.2825...  0.3423 sec/batch\n",
      "Epoch: 5/100...  Training Step: 876...  Training loss: 5.7545...  0.3408 sec/batch\n",
      "Epoch: 5/100...  Training Step: 877...  Training loss: 5.7257...  0.3375 sec/batch\n",
      "Epoch: 5/100...  Training Step: 878...  Training loss: 5.4932...  0.3394 sec/batch\n",
      "Epoch: 5/100...  Training Step: 879...  Training loss: 5.2404...  0.3391 sec/batch\n",
      "Epoch: 5/100...  Training Step: 880...  Training loss: 5.2056...  0.3414 sec/batch\n",
      "Epoch: 5/100...  Training Step: 881...  Training loss: 5.1934...  0.3415 sec/batch\n",
      "Epoch: 5/100...  Training Step: 882...  Training loss: 5.2632...  0.3416 sec/batch\n",
      "Epoch: 5/100...  Training Step: 883...  Training loss: 5.3860...  0.3403 sec/batch\n",
      "Epoch: 5/100...  Training Step: 884...  Training loss: 5.3077...  0.3400 sec/batch\n",
      "Epoch: 5/100...  Training Step: 885...  Training loss: 5.3081...  0.3414 sec/batch\n",
      "Epoch: 5/100...  Training Step: 886...  Training loss: 5.2650...  0.3431 sec/batch\n",
      "Epoch: 5/100...  Training Step: 887...  Training loss: 5.2806...  0.3462 sec/batch\n",
      "Epoch: 5/100...  Training Step: 888...  Training loss: 5.0481...  0.3455 sec/batch\n",
      "Epoch: 5/100...  Training Step: 889...  Training loss: 5.2969...  0.3442 sec/batch\n",
      "Epoch: 5/100...  Training Step: 890...  Training loss: 5.1949...  0.3416 sec/batch\n",
      "Epoch: 5/100...  Training Step: 891...  Training loss: 5.2976...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 892...  Training loss: 5.2577...  0.3384 sec/batch\n",
      "Epoch: 5/100...  Training Step: 893...  Training loss: 5.3538...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100...  Training Step: 894...  Training loss: 5.3552...  0.3395 sec/batch\n",
      "Epoch: 5/100...  Training Step: 895...  Training loss: 5.2534...  0.3395 sec/batch\n",
      "Epoch: 5/100...  Training Step: 896...  Training loss: 5.2442...  0.3386 sec/batch\n",
      "Epoch: 5/100...  Training Step: 897...  Training loss: 5.2547...  0.3417 sec/batch\n",
      "Epoch: 5/100...  Training Step: 898...  Training loss: 5.3257...  0.3390 sec/batch\n",
      "Epoch: 5/100...  Training Step: 899...  Training loss: 5.3381...  0.3423 sec/batch\n",
      "Epoch: 5/100...  Training Step: 900...  Training loss: 5.3679...  0.3407 sec/batch\n",
      "Epoch: 5/100...  Training Step: 901...  Training loss: 5.5567...  0.3408 sec/batch\n",
      "Epoch: 5/100...  Training Step: 902...  Training loss: 5.4762...  0.3426 sec/batch\n",
      "Epoch: 5/100...  Training Step: 903...  Training loss: 5.3739...  0.3411 sec/batch\n",
      "Epoch: 5/100...  Training Step: 904...  Training loss: 5.2779...  0.3393 sec/batch\n",
      "Epoch: 5/100...  Training Step: 905...  Training loss: 5.2444...  0.3393 sec/batch\n",
      "Epoch: 5/100...  Training Step: 906...  Training loss: 5.2146...  0.3392 sec/batch\n",
      "Epoch: 5/100...  Training Step: 907...  Training loss: 5.2074...  0.3387 sec/batch\n",
      "Epoch: 5/100...  Training Step: 908...  Training loss: 5.2678...  0.3386 sec/batch\n",
      "Epoch: 5/100...  Training Step: 909...  Training loss: 5.3512...  0.3392 sec/batch\n",
      "Epoch: 5/100...  Training Step: 910...  Training loss: 5.2841...  0.3401 sec/batch\n",
      "Epoch: 5/100...  Training Step: 911...  Training loss: 5.2907...  0.3397 sec/batch\n",
      "Epoch: 5/100...  Training Step: 912...  Training loss: 5.1975...  0.3433 sec/batch\n",
      "Epoch: 5/100...  Training Step: 913...  Training loss: 5.2715...  0.3390 sec/batch\n",
      "Epoch: 5/100...  Training Step: 914...  Training loss: 5.2223...  0.3422 sec/batch\n",
      "Epoch: 5/100...  Training Step: 915...  Training loss: 5.2071...  0.3429 sec/batch\n",
      "Epoch: 5/100...  Training Step: 916...  Training loss: 5.2329...  0.3442 sec/batch\n",
      "Epoch: 5/100...  Training Step: 917...  Training loss: 5.6624...  0.3383 sec/batch\n",
      "Epoch: 5/100...  Training Step: 918...  Training loss: 5.5283...  0.3386 sec/batch\n",
      "Epoch: 5/100...  Training Step: 919...  Training loss: 5.5852...  0.3392 sec/batch\n",
      "Epoch: 5/100...  Training Step: 920...  Training loss: 5.5041...  0.3407 sec/batch\n",
      "Epoch: 6/100...  Training Step: 921...  Training loss: 5.7034...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 922...  Training loss: 5.7286...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 923...  Training loss: 5.4958...  0.3383 sec/batch\n",
      "Epoch: 6/100...  Training Step: 924...  Training loss: 5.4344...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 925...  Training loss: 5.4313...  0.3401 sec/batch\n",
      "Epoch: 6/100...  Training Step: 926...  Training loss: 5.3337...  0.3430 sec/batch\n",
      "Epoch: 6/100...  Training Step: 927...  Training loss: 5.5640...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 928...  Training loss: 5.3703...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 929...  Training loss: 5.4422...  0.3378 sec/batch\n",
      "Epoch: 6/100...  Training Step: 930...  Training loss: 5.2990...  0.3409 sec/batch\n",
      "Epoch: 6/100...  Training Step: 931...  Training loss: 5.3275...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 932...  Training loss: 5.3050...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 933...  Training loss: 5.2851...  0.3392 sec/batch\n",
      "Epoch: 6/100...  Training Step: 934...  Training loss: 5.4607...  0.3411 sec/batch\n",
      "Epoch: 6/100...  Training Step: 935...  Training loss: 5.4264...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 936...  Training loss: 5.4204...  0.3395 sec/batch\n",
      "Epoch: 6/100...  Training Step: 937...  Training loss: 5.4134...  0.3412 sec/batch\n",
      "Epoch: 6/100...  Training Step: 938...  Training loss: 5.2781...  0.3388 sec/batch\n",
      "Epoch: 6/100...  Training Step: 939...  Training loss: 5.4126...  0.3425 sec/batch\n",
      "Epoch: 6/100...  Training Step: 940...  Training loss: 5.3031...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 941...  Training loss: 5.3484...  0.3362 sec/batch\n",
      "Epoch: 6/100...  Training Step: 942...  Training loss: 5.3264...  0.3385 sec/batch\n",
      "Epoch: 6/100...  Training Step: 943...  Training loss: 5.3362...  0.3394 sec/batch\n",
      "Epoch: 6/100...  Training Step: 944...  Training loss: 5.3764...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 945...  Training loss: 5.3304...  0.3406 sec/batch\n",
      "Epoch: 6/100...  Training Step: 946...  Training loss: 5.2559...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 947...  Training loss: 5.3053...  0.3393 sec/batch\n",
      "Epoch: 6/100...  Training Step: 948...  Training loss: 5.2694...  0.3393 sec/batch\n",
      "Epoch: 6/100...  Training Step: 949...  Training loss: 5.3009...  0.3421 sec/batch\n",
      "Epoch: 6/100...  Training Step: 950...  Training loss: 5.3104...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 951...  Training loss: 5.2560...  0.3395 sec/batch\n",
      "Epoch: 6/100...  Training Step: 952...  Training loss: 5.2411...  0.3409 sec/batch\n",
      "Epoch: 6/100...  Training Step: 953...  Training loss: 4.9616...  0.3427 sec/batch\n",
      "Epoch: 6/100...  Training Step: 954...  Training loss: 5.0657...  0.3389 sec/batch\n",
      "Epoch: 6/100...  Training Step: 955...  Training loss: 5.2082...  0.3412 sec/batch\n",
      "Epoch: 6/100...  Training Step: 956...  Training loss: 5.4028...  0.3388 sec/batch\n",
      "Epoch: 6/100...  Training Step: 957...  Training loss: 5.2784...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 958...  Training loss: 5.3570...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 959...  Training loss: 5.2222...  0.3395 sec/batch\n",
      "Epoch: 6/100...  Training Step: 960...  Training loss: 5.2361...  0.3396 sec/batch\n",
      "Epoch: 6/100...  Training Step: 961...  Training loss: 5.3524...  0.3381 sec/batch\n",
      "Epoch: 6/100...  Training Step: 962...  Training loss: 5.2999...  0.3382 sec/batch\n",
      "Epoch: 6/100...  Training Step: 963...  Training loss: 5.3578...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 964...  Training loss: 5.3722...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 965...  Training loss: 5.2740...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 966...  Training loss: 5.2675...  0.3376 sec/batch\n",
      "Epoch: 6/100...  Training Step: 967...  Training loss: 5.3140...  0.3400 sec/batch\n",
      "Epoch: 6/100...  Training Step: 968...  Training loss: 5.1818...  0.3406 sec/batch\n",
      "Epoch: 6/100...  Training Step: 969...  Training loss: 5.2368...  0.3426 sec/batch\n",
      "Epoch: 6/100...  Training Step: 970...  Training loss: 5.4616...  0.3434 sec/batch\n",
      "Epoch: 6/100...  Training Step: 971...  Training loss: 5.3558...  0.3420 sec/batch\n",
      "Epoch: 6/100...  Training Step: 972...  Training loss: 5.2363...  0.3401 sec/batch\n",
      "Epoch: 6/100...  Training Step: 973...  Training loss: 5.3416...  0.3419 sec/batch\n",
      "Epoch: 6/100...  Training Step: 974...  Training loss: 5.3791...  0.3424 sec/batch\n",
      "Epoch: 6/100...  Training Step: 975...  Training loss: 5.3840...  0.3417 sec/batch\n",
      "Epoch: 6/100...  Training Step: 976...  Training loss: 5.2756...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 977...  Training loss: 5.1675...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 978...  Training loss: 5.3205...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 979...  Training loss: 5.1333...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 980...  Training loss: 5.1767...  0.3380 sec/batch\n",
      "Epoch: 6/100...  Training Step: 981...  Training loss: 5.1799...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 982...  Training loss: 5.2295...  0.3377 sec/batch\n",
      "Epoch: 6/100...  Training Step: 983...  Training loss: 5.3101...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 984...  Training loss: 5.3377...  0.3388 sec/batch\n",
      "Epoch: 6/100...  Training Step: 985...  Training loss: 5.1697...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 986...  Training loss: 5.2043...  0.3381 sec/batch\n",
      "Epoch: 6/100...  Training Step: 987...  Training loss: 5.2261...  0.3428 sec/batch\n",
      "Epoch: 6/100...  Training Step: 988...  Training loss: 5.1029...  0.3392 sec/batch\n",
      "Epoch: 6/100...  Training Step: 989...  Training loss: 5.3483...  0.3407 sec/batch\n",
      "Epoch: 6/100...  Training Step: 990...  Training loss: 5.2474...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 991...  Training loss: 5.2929...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 992...  Training loss: 5.0877...  0.3399 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100...  Training Step: 993...  Training loss: 5.1132...  0.3430 sec/batch\n",
      "Epoch: 6/100...  Training Step: 994...  Training loss: 5.1889...  0.3426 sec/batch\n",
      "Epoch: 6/100...  Training Step: 995...  Training loss: 5.1439...  0.3396 sec/batch\n",
      "Epoch: 6/100...  Training Step: 996...  Training loss: 5.1468...  0.3376 sec/batch\n",
      "Epoch: 6/100...  Training Step: 997...  Training loss: 5.2440...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 998...  Training loss: 5.2769...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 999...  Training loss: 5.3192...  0.3417 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1000...  Training loss: 5.5363...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1001...  Training loss: 5.4459...  0.3961 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1002...  Training loss: 5.4285...  0.3501 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1003...  Training loss: 5.4066...  0.3388 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1004...  Training loss: 5.3607...  0.3380 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1005...  Training loss: 5.4766...  0.3370 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1006...  Training loss: 5.3694...  0.3396 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1007...  Training loss: 5.3029...  0.3393 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1008...  Training loss: 5.4231...  0.3367 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1009...  Training loss: 5.4766...  0.3426 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1010...  Training loss: 5.4373...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1011...  Training loss: 5.4294...  0.3409 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1012...  Training loss: 5.1609...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1013...  Training loss: 5.4468...  0.3406 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1014...  Training loss: 5.4722...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1015...  Training loss: 5.3866...  0.3414 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1016...  Training loss: 5.2881...  0.3393 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1017...  Training loss: 5.2860...  0.3396 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1018...  Training loss: 5.2773...  0.3404 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1019...  Training loss: 5.3716...  0.3383 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1020...  Training loss: 5.2869...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1021...  Training loss: 5.3267...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1022...  Training loss: 5.3381...  0.3407 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1023...  Training loss: 5.4756...  0.3421 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1024...  Training loss: 5.3417...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1025...  Training loss: 5.4232...  0.3371 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1026...  Training loss: 5.4480...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1027...  Training loss: 5.3875...  0.3410 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1028...  Training loss: 5.1576...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1029...  Training loss: 5.1836...  0.3380 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1030...  Training loss: 5.1941...  0.3417 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1031...  Training loss: 5.1737...  0.3418 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1032...  Training loss: 5.2772...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1033...  Training loss: 5.2629...  0.3392 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1034...  Training loss: 5.2447...  0.3427 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1035...  Training loss: 5.1172...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1036...  Training loss: 5.1444...  0.3419 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1037...  Training loss: 5.9311...  0.3401 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1038...  Training loss: 5.3361...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1039...  Training loss: 5.3840...  0.3413 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1040...  Training loss: 5.5330...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1041...  Training loss: 5.6924...  0.3418 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1042...  Training loss: 5.5310...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1043...  Training loss: 5.4428...  0.3400 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1044...  Training loss: 5.3941...  0.3400 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1045...  Training loss: 5.3161...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1046...  Training loss: 5.2841...  0.3424 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1047...  Training loss: 5.3765...  0.3429 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1048...  Training loss: 5.4169...  0.3384 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1049...  Training loss: 5.5036...  0.3393 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1050...  Training loss: 5.3069...  0.3400 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1051...  Training loss: 5.3413...  0.3389 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1052...  Training loss: 5.3687...  0.3417 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1053...  Training loss: 5.4201...  0.3381 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1054...  Training loss: 5.1778...  0.3412 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1055...  Training loss: 5.2534...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1056...  Training loss: 5.1449...  0.3383 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1057...  Training loss: 5.4631...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1058...  Training loss: 5.2693...  0.3383 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1059...  Training loss: 5.3548...  0.3412 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1060...  Training loss: 5.8115...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1061...  Training loss: 5.7452...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1062...  Training loss: 5.5373...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1063...  Training loss: 5.3393...  0.3398 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1064...  Training loss: 5.3304...  0.3434 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1065...  Training loss: 5.3000...  0.3429 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1066...  Training loss: 5.3346...  0.3417 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1067...  Training loss: 5.4413...  0.3394 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1068...  Training loss: 5.3538...  0.3411 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1069...  Training loss: 5.3751...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1070...  Training loss: 5.3216...  0.3412 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1071...  Training loss: 5.3526...  0.3396 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1072...  Training loss: 5.1005...  0.3401 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1073...  Training loss: 5.3375...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1074...  Training loss: 5.2505...  0.3404 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1075...  Training loss: 5.3485...  0.3402 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1076...  Training loss: 5.2864...  0.3401 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1077...  Training loss: 5.3844...  0.3390 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1078...  Training loss: 5.4023...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1079...  Training loss: 5.2824...  0.3410 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1080...  Training loss: 5.2896...  0.3397 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1081...  Training loss: 5.2823...  0.3403 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1082...  Training loss: 5.3837...  0.3409 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1083...  Training loss: 5.3845...  0.3428 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1084...  Training loss: 5.4048...  0.3415 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1085...  Training loss: 5.5990...  0.3407 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1086...  Training loss: 5.5155...  0.3387 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1087...  Training loss: 5.4072...  0.3385 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1088...  Training loss: 5.3139...  0.3388 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1089...  Training loss: 5.2923...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1090...  Training loss: 5.2459...  0.3376 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100...  Training Step: 1091...  Training loss: 5.2488...  0.3405 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1092...  Training loss: 5.2734...  0.3416 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1093...  Training loss: 5.3842...  0.3386 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1094...  Training loss: 5.3382...  0.3377 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1095...  Training loss: 5.3496...  0.3414 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1096...  Training loss: 5.2614...  0.3373 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1097...  Training loss: 5.3179...  0.3398 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1098...  Training loss: 5.2602...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1099...  Training loss: 5.2460...  0.3391 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1100...  Training loss: 5.2684...  0.3413 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1101...  Training loss: 5.6620...  0.3399 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1102...  Training loss: 5.5289...  0.3408 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1103...  Training loss: 5.6047...  0.3372 sec/batch\n",
      "Epoch: 6/100...  Training Step: 1104...  Training loss: 5.5299...  0.3400 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1105...  Training loss: 6.0056...  0.3395 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1106...  Training loss: 5.7904...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1107...  Training loss: 5.5591...  0.3369 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1108...  Training loss: 5.4704...  0.3376 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1109...  Training loss: 5.4670...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1110...  Training loss: 5.3617...  0.3388 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1111...  Training loss: 5.5969...  0.3432 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1112...  Training loss: 5.3890...  0.3404 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1113...  Training loss: 5.4584...  0.3391 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1114...  Training loss: 5.3278...  0.3401 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1115...  Training loss: 5.3652...  0.3402 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1116...  Training loss: 5.3258...  0.3411 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1117...  Training loss: 5.2945...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1118...  Training loss: 5.4858...  0.3416 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1119...  Training loss: 5.4405...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1120...  Training loss: 5.4144...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1121...  Training loss: 5.4361...  0.3380 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1122...  Training loss: 5.2897...  0.3393 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1123...  Training loss: 5.4377...  0.3402 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1124...  Training loss: 5.3221...  0.3379 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1125...  Training loss: 5.3732...  0.3388 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1126...  Training loss: 5.3496...  0.3411 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1127...  Training loss: 5.3410...  0.3374 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1128...  Training loss: 5.3965...  0.3388 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1129...  Training loss: 5.3587...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1130...  Training loss: 5.2886...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1131...  Training loss: 5.3338...  0.3398 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1132...  Training loss: 5.3094...  0.3399 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1133...  Training loss: 5.3179...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1134...  Training loss: 5.3335...  0.3375 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1135...  Training loss: 5.2660...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1136...  Training loss: 5.2473...  0.3383 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1137...  Training loss: 4.9766...  0.3425 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1138...  Training loss: 5.0798...  0.3376 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1139...  Training loss: 5.2356...  0.3407 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1140...  Training loss: 5.4340...  0.3379 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1141...  Training loss: 5.3407...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1142...  Training loss: 5.3857...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1143...  Training loss: 5.2360...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1144...  Training loss: 5.2554...  0.3386 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1145...  Training loss: 5.3698...  0.3404 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1146...  Training loss: 5.3148...  0.3377 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1147...  Training loss: 5.3812...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1148...  Training loss: 5.3979...  0.3364 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1149...  Training loss: 5.2993...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1150...  Training loss: 5.2923...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1151...  Training loss: 5.3365...  0.3386 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1152...  Training loss: 5.2218...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1153...  Training loss: 5.2697...  0.3442 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1154...  Training loss: 5.4822...  0.3448 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1155...  Training loss: 5.3799...  0.3454 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1156...  Training loss: 5.2588...  0.3424 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1157...  Training loss: 5.3739...  0.3424 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1158...  Training loss: 5.4089...  0.3429 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1159...  Training loss: 5.4130...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1160...  Training loss: 5.2814...  0.3430 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1161...  Training loss: 5.1970...  0.3438 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1162...  Training loss: 5.3489...  0.3400 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1163...  Training loss: 5.1531...  0.3473 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1164...  Training loss: 5.1990...  0.3421 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1165...  Training loss: 5.1886...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1166...  Training loss: 5.2291...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1167...  Training loss: 5.3312...  0.3428 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1168...  Training loss: 5.3477...  0.3432 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1169...  Training loss: 5.1917...  0.3441 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1170...  Training loss: 5.2108...  0.3446 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1171...  Training loss: 5.2447...  0.3455 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1172...  Training loss: 5.1198...  0.3436 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1173...  Training loss: 5.3508...  0.3409 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1174...  Training loss: 5.2550...  0.3425 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1175...  Training loss: 5.3042...  0.3405 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1176...  Training loss: 5.1088...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1177...  Training loss: 5.1285...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1178...  Training loss: 5.2033...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1179...  Training loss: 5.1559...  0.3417 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1180...  Training loss: 5.1735...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1181...  Training loss: 5.2624...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1182...  Training loss: 5.3020...  0.3430 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1183...  Training loss: 5.3411...  0.3372 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1184...  Training loss: 5.5367...  0.3429 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1185...  Training loss: 5.4584...  0.3418 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1186...  Training loss: 5.4520...  0.3386 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1187...  Training loss: 5.4034...  0.3401 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1188...  Training loss: 5.3813...  0.3392 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100...  Training Step: 1189...  Training loss: 5.4981...  0.3370 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1190...  Training loss: 5.3785...  0.3378 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1191...  Training loss: 5.3236...  0.3420 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1192...  Training loss: 5.4333...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1193...  Training loss: 5.4838...  0.3391 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1194...  Training loss: 5.4450...  0.3384 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1195...  Training loss: 5.4374...  0.3388 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1196...  Training loss: 5.1745...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1197...  Training loss: 5.4589...  0.3406 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1198...  Training loss: 5.4785...  0.3518 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1199...  Training loss: 5.3877...  0.3436 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1200...  Training loss: 5.2799...  0.3399 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1201...  Training loss: 5.2884...  0.3379 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1202...  Training loss: 5.2699...  0.3377 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1203...  Training loss: 5.3664...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1204...  Training loss: 5.2836...  0.3382 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1205...  Training loss: 5.3154...  0.3430 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1206...  Training loss: 5.3298...  0.3393 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1207...  Training loss: 5.4992...  0.3413 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1208...  Training loss: 5.3503...  0.3404 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1209...  Training loss: 5.4299...  0.3401 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1210...  Training loss: 5.4486...  0.3387 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1211...  Training loss: 5.3786...  0.3413 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1212...  Training loss: 5.1508...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1213...  Training loss: 5.1609...  0.3414 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1214...  Training loss: 5.1831...  0.3404 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1215...  Training loss: 5.1532...  0.3390 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1216...  Training loss: 5.2675...  0.3392 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1217...  Training loss: 5.2402...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1218...  Training loss: 5.2296...  0.3390 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1219...  Training loss: 5.1163...  0.3379 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1220...  Training loss: 5.1343...  0.3384 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1221...  Training loss: 5.3744...  0.3382 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1222...  Training loss: 5.1986...  0.3428 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1223...  Training loss: 5.1678...  0.3406 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1224...  Training loss: 5.2590...  0.3415 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1225...  Training loss: 5.4582...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1226...  Training loss: 5.3624...  0.3429 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1227...  Training loss: 5.2517...  0.3378 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1228...  Training loss: 5.2160...  0.3392 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1229...  Training loss: 5.1498...  0.3421 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1230...  Training loss: 5.1570...  0.3436 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1231...  Training loss: 5.2444...  0.3406 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1232...  Training loss: 5.2819...  0.3387 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1233...  Training loss: 5.4068...  0.3384 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1234...  Training loss: 5.2250...  0.3391 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1235...  Training loss: 5.2397...  0.3386 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1236...  Training loss: 5.2652...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1237...  Training loss: 5.3250...  0.3376 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1238...  Training loss: 5.0877...  0.3407 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1239...  Training loss: 5.1587...  0.3391 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1240...  Training loss: 5.0711...  0.3412 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1241...  Training loss: 5.3813...  0.3417 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1242...  Training loss: 5.1571...  0.3421 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1243...  Training loss: 5.2630...  0.3392 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1244...  Training loss: 5.7339...  0.3430 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1245...  Training loss: 5.6839...  0.3393 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1246...  Training loss: 5.4674...  0.3403 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1247...  Training loss: 5.2161...  0.3387 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1248...  Training loss: 5.1804...  0.3409 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1249...  Training loss: 5.1571...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1250...  Training loss: 5.2116...  0.3409 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1251...  Training loss: 5.3550...  0.3417 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1252...  Training loss: 5.2838...  0.3381 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1253...  Training loss: 5.2880...  0.3390 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1254...  Training loss: 5.2417...  0.3399 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1255...  Training loss: 5.2441...  0.3383 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1256...  Training loss: 5.0220...  0.3407 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1257...  Training loss: 5.2612...  0.3406 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1258...  Training loss: 5.1687...  0.3411 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1259...  Training loss: 5.2742...  0.3397 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1260...  Training loss: 5.2332...  0.3392 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1261...  Training loss: 5.3303...  0.3374 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1262...  Training loss: 5.3345...  0.3410 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1263...  Training loss: 5.2328...  0.3384 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1264...  Training loss: 5.2162...  0.3377 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1265...  Training loss: 5.2301...  0.3402 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1266...  Training loss: 5.2931...  0.3391 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1267...  Training loss: 5.3210...  0.3408 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1268...  Training loss: 5.3480...  0.3414 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1269...  Training loss: 5.5163...  0.3380 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1270...  Training loss: 5.4299...  0.3426 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1271...  Training loss: 5.3322...  0.3405 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1272...  Training loss: 5.2498...  0.3415 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1273...  Training loss: 5.2051...  0.3389 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1274...  Training loss: 5.1841...  0.3378 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1275...  Training loss: 5.1892...  0.3404 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1276...  Training loss: 5.2321...  0.3387 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1277...  Training loss: 5.3291...  0.3398 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1278...  Training loss: 5.2552...  0.3402 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1279...  Training loss: 5.2598...  0.3393 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1280...  Training loss: 5.1743...  0.3393 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1281...  Training loss: 5.2422...  0.3406 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1282...  Training loss: 5.2008...  0.3392 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1283...  Training loss: 5.1842...  0.3413 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1284...  Training loss: 5.2194...  0.3376 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1285...  Training loss: 5.6618...  0.3379 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1286...  Training loss: 5.5059...  0.3384 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100...  Training Step: 1287...  Training loss: 5.5630...  0.3411 sec/batch\n",
      "Epoch: 7/100...  Training Step: 1288...  Training loss: 5.4921...  0.3371 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1289...  Training loss: 5.7489...  0.3435 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1290...  Training loss: 5.6917...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1291...  Training loss: 5.4880...  0.3390 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1292...  Training loss: 5.4088...  0.3422 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1293...  Training loss: 5.4074...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1294...  Training loss: 5.2980...  0.3376 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1295...  Training loss: 5.5568...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1296...  Training loss: 5.3473...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1297...  Training loss: 5.4300...  0.3392 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1298...  Training loss: 5.2756...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1299...  Training loss: 5.3081...  0.3419 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1300...  Training loss: 5.2660...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1301...  Training loss: 5.2637...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1302...  Training loss: 5.4266...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1303...  Training loss: 5.3942...  0.3390 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1304...  Training loss: 5.3872...  0.3391 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1305...  Training loss: 5.4023...  0.3405 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1306...  Training loss: 5.2572...  0.3404 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1307...  Training loss: 5.3903...  0.3405 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1308...  Training loss: 5.2668...  0.3409 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1309...  Training loss: 5.3037...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1310...  Training loss: 5.2928...  0.3416 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1311...  Training loss: 5.2979...  0.3394 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1312...  Training loss: 5.3476...  0.3385 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1313...  Training loss: 5.3131...  0.3397 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1314...  Training loss: 5.2398...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1315...  Training loss: 5.2839...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1316...  Training loss: 5.2589...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1317...  Training loss: 5.2810...  0.3425 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1318...  Training loss: 5.2940...  0.3377 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1319...  Training loss: 5.2279...  0.3387 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1320...  Training loss: 5.2051...  0.3385 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1321...  Training loss: 4.9480...  0.3412 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1322...  Training loss: 5.0308...  0.3414 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1323...  Training loss: 5.1818...  0.3407 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1324...  Training loss: 5.3777...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1325...  Training loss: 5.2609...  0.3394 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1326...  Training loss: 5.3269...  0.3377 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1327...  Training loss: 5.2025...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1328...  Training loss: 5.2320...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1329...  Training loss: 5.3375...  0.3394 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1330...  Training loss: 5.2829...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1331...  Training loss: 5.3532...  0.3407 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1332...  Training loss: 5.3532...  0.3394 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1333...  Training loss: 5.2468...  0.3392 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1334...  Training loss: 5.2354...  0.3388 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1335...  Training loss: 5.2824...  0.3419 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1336...  Training loss: 5.1585...  0.3409 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1337...  Training loss: 5.2150...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1338...  Training loss: 5.4382...  0.3404 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1339...  Training loss: 5.3311...  0.3409 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1340...  Training loss: 5.2026...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1341...  Training loss: 5.3062...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1342...  Training loss: 5.3489...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1343...  Training loss: 5.3548...  0.3390 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1344...  Training loss: 5.2323...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1345...  Training loss: 5.1523...  0.3396 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1346...  Training loss: 5.3049...  0.3406 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1347...  Training loss: 5.1004...  0.3391 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1348...  Training loss: 5.1551...  0.3370 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1349...  Training loss: 5.1486...  0.3398 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1350...  Training loss: 5.2065...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1351...  Training loss: 5.2921...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1352...  Training loss: 5.3106...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1353...  Training loss: 5.1486...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1354...  Training loss: 5.1657...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1355...  Training loss: 5.1988...  0.3412 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1356...  Training loss: 5.0888...  0.3388 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1357...  Training loss: 5.3250...  0.3410 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1358...  Training loss: 5.2164...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1359...  Training loss: 5.2651...  0.3408 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1360...  Training loss: 5.0590...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1361...  Training loss: 5.0921...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1362...  Training loss: 5.1540...  0.3399 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1363...  Training loss: 5.1162...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1364...  Training loss: 5.1359...  0.3399 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1365...  Training loss: 5.2317...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1366...  Training loss: 5.2693...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1367...  Training loss: 5.2870...  0.3424 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1368...  Training loss: 5.5057...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1369...  Training loss: 5.4145...  0.3398 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1370...  Training loss: 5.4012...  0.3407 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1371...  Training loss: 5.3655...  0.3425 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1372...  Training loss: 5.3258...  0.3426 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1373...  Training loss: 5.4470...  0.3385 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1374...  Training loss: 5.3301...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1375...  Training loss: 5.2825...  0.3411 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1376...  Training loss: 5.3840...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1377...  Training loss: 5.4515...  0.3416 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1378...  Training loss: 5.4296...  0.3397 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1379...  Training loss: 5.4098...  0.3380 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1380...  Training loss: 5.1590...  0.3371 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1381...  Training loss: 5.4331...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1382...  Training loss: 5.4583...  0.3397 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1383...  Training loss: 5.3791...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1384...  Training loss: 5.2725...  0.3414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100...  Training Step: 1385...  Training loss: 5.2681...  0.3416 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1386...  Training loss: 5.2458...  0.3400 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1387...  Training loss: 5.3397...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1388...  Training loss: 5.2620...  0.3399 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1389...  Training loss: 5.3002...  0.3408 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1390...  Training loss: 5.3104...  0.3392 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1391...  Training loss: 5.4602...  0.3380 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1392...  Training loss: 5.3194...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1393...  Training loss: 5.3996...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1394...  Training loss: 5.4208...  0.3397 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1395...  Training loss: 5.3583...  0.3391 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1396...  Training loss: 5.1232...  0.3387 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1397...  Training loss: 5.1435...  0.3408 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1398...  Training loss: 5.1486...  0.3405 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1399...  Training loss: 5.1505...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1400...  Training loss: 5.2621...  0.3429 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1401...  Training loss: 5.2236...  0.3425 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1402...  Training loss: 5.2084...  0.3399 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1403...  Training loss: 5.0966...  0.3424 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1404...  Training loss: 5.1207...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1405...  Training loss: 5.3203...  0.3380 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1406...  Training loss: 5.1688...  0.3391 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1407...  Training loss: 5.1254...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1408...  Training loss: 5.2185...  0.3378 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1409...  Training loss: 5.4325...  0.3408 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1410...  Training loss: 5.3388...  0.3396 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1411...  Training loss: 5.2171...  0.3392 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1412...  Training loss: 5.2063...  0.3383 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1413...  Training loss: 5.1156...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1414...  Training loss: 5.1438...  0.3402 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1415...  Training loss: 5.2334...  0.3396 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1416...  Training loss: 5.2553...  0.3376 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1417...  Training loss: 5.3927...  0.3435 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1418...  Training loss: 5.2115...  0.3410 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1419...  Training loss: 5.2077...  0.3425 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1420...  Training loss: 5.2514...  0.3423 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1421...  Training loss: 5.3030...  0.3405 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1422...  Training loss: 5.0562...  0.3429 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1423...  Training loss: 5.1333...  0.3390 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1424...  Training loss: 5.0449...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1425...  Training loss: 5.3632...  0.3390 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1426...  Training loss: 5.1455...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1427...  Training loss: 5.2347...  0.3398 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1428...  Training loss: 5.7015...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1429...  Training loss: 5.6526...  0.3395 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1430...  Training loss: 5.4336...  0.3375 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1431...  Training loss: 5.1985...  0.3384 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1432...  Training loss: 5.1541...  0.3382 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1433...  Training loss: 5.1409...  0.3412 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1434...  Training loss: 5.2041...  0.3380 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1435...  Training loss: 5.3378...  0.3369 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1436...  Training loss: 5.2520...  0.3379 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1437...  Training loss: 5.2718...  0.3392 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1438...  Training loss: 5.2034...  0.3373 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1439...  Training loss: 5.2237...  0.3410 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1440...  Training loss: 5.0079...  0.3381 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1441...  Training loss: 5.2357...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1442...  Training loss: 5.1435...  0.3414 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1443...  Training loss: 5.2349...  0.3394 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1444...  Training loss: 5.2124...  0.3382 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1445...  Training loss: 5.3021...  0.3403 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1446...  Training loss: 5.3230...  0.3375 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1447...  Training loss: 5.2174...  0.3412 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1448...  Training loss: 5.1952...  0.3371 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1449...  Training loss: 5.2115...  0.3393 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1450...  Training loss: 5.2738...  0.3383 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1451...  Training loss: 5.2997...  0.3389 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1452...  Training loss: 5.3265...  0.3380 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1453...  Training loss: 5.5010...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1454...  Training loss: 5.4195...  0.3382 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1455...  Training loss: 5.3324...  0.3386 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1456...  Training loss: 5.2449...  0.3419 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1457...  Training loss: 5.1979...  0.3416 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1458...  Training loss: 5.1671...  0.3388 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1459...  Training loss: 5.1750...  0.3424 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1460...  Training loss: 5.2175...  0.3422 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1461...  Training loss: 5.2955...  0.3436 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1462...  Training loss: 5.2425...  0.3428 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1463...  Training loss: 5.2485...  0.3439 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1464...  Training loss: 5.1549...  0.3439 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1465...  Training loss: 5.2308...  0.3388 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1466...  Training loss: 5.1837...  0.3420 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1467...  Training loss: 5.1717...  0.3391 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1468...  Training loss: 5.1989...  0.3434 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1469...  Training loss: 5.6329...  0.3443 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1470...  Training loss: 5.4841...  0.3427 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1471...  Training loss: 5.5508...  0.3434 sec/batch\n",
      "Epoch: 8/100...  Training Step: 1472...  Training loss: 5.4520...  0.3416 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1473...  Training loss: 5.6558...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1474...  Training loss: 5.6847...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1475...  Training loss: 5.4653...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1476...  Training loss: 5.3780...  0.3439 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1477...  Training loss: 5.3898...  0.3414 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1478...  Training loss: 5.2823...  0.3428 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1479...  Training loss: 5.5390...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1480...  Training loss: 5.3429...  0.3447 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1481...  Training loss: 5.4271...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1482...  Training loss: 5.2534...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100...  Training Step: 1483...  Training loss: 5.2846...  0.3409 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1484...  Training loss: 5.2643...  0.3405 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1485...  Training loss: 5.2538...  0.3397 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1486...  Training loss: 5.3948...  0.3416 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1487...  Training loss: 5.3703...  0.3440 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1488...  Training loss: 5.3627...  0.3427 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1489...  Training loss: 5.3703...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1490...  Training loss: 5.2426...  0.3438 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1491...  Training loss: 5.3783...  0.3394 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1492...  Training loss: 5.2511...  0.3420 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1493...  Training loss: 5.2965...  0.3427 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1494...  Training loss: 5.2707...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1495...  Training loss: 5.2801...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1496...  Training loss: 5.3253...  0.3409 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1497...  Training loss: 5.2930...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1498...  Training loss: 5.2079...  0.3423 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1499...  Training loss: 5.2646...  0.3440 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1500...  Training loss: 5.2307...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1501...  Training loss: 5.2490...  0.3421 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1502...  Training loss: 5.2574...  0.3395 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1503...  Training loss: 5.2144...  0.3426 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1504...  Training loss: 5.1858...  0.3405 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1505...  Training loss: 4.8922...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1506...  Training loss: 4.9937...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1507...  Training loss: 5.1462...  0.3412 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1508...  Training loss: 5.3626...  0.3416 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1509...  Training loss: 5.2447...  0.3422 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1510...  Training loss: 5.3019...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1511...  Training loss: 5.1693...  0.3398 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1512...  Training loss: 5.2089...  0.3448 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1513...  Training loss: 5.3278...  0.3441 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1514...  Training loss: 5.2727...  0.3449 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1515...  Training loss: 5.3409...  0.3442 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1516...  Training loss: 5.3387...  0.3421 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1517...  Training loss: 5.2239...  0.3431 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1518...  Training loss: 5.2148...  0.3412 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1519...  Training loss: 5.2647...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1520...  Training loss: 5.1228...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1521...  Training loss: 5.2039...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1522...  Training loss: 5.4071...  0.3414 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1523...  Training loss: 5.3115...  0.3431 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1524...  Training loss: 5.1988...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1525...  Training loss: 5.3006...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1526...  Training loss: 5.3290...  0.3447 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1527...  Training loss: 5.3364...  0.3389 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1528...  Training loss: 5.2277...  0.3400 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1529...  Training loss: 5.1277...  0.3402 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1530...  Training loss: 5.2910...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1531...  Training loss: 5.0844...  0.3424 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1532...  Training loss: 5.1499...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1533...  Training loss: 5.1364...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1534...  Training loss: 5.2061...  0.3400 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1535...  Training loss: 5.2778...  0.3397 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1536...  Training loss: 5.3029...  0.3422 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1537...  Training loss: 5.1265...  0.3431 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1538...  Training loss: 5.1583...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1539...  Training loss: 5.1936...  0.3413 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1540...  Training loss: 5.0744...  0.3428 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1541...  Training loss: 5.2958...  0.3420 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1542...  Training loss: 5.2100...  0.3410 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1543...  Training loss: 5.2362...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1544...  Training loss: 5.0403...  0.3388 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1545...  Training loss: 5.0728...  0.3443 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1546...  Training loss: 5.1453...  0.3439 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1547...  Training loss: 5.1060...  0.3419 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1548...  Training loss: 5.1299...  0.3386 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1549...  Training loss: 5.2196...  0.3414 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1550...  Training loss: 5.2417...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1551...  Training loss: 5.2881...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1552...  Training loss: 5.4936...  0.3438 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1553...  Training loss: 5.4055...  0.3455 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1554...  Training loss: 5.3852...  0.3436 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1555...  Training loss: 5.3591...  0.3414 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1556...  Training loss: 5.3197...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1557...  Training loss: 5.4328...  0.3400 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1558...  Training loss: 5.3151...  0.3440 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1559...  Training loss: 5.2629...  0.3432 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1560...  Training loss: 5.3721...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1561...  Training loss: 5.4338...  0.3423 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1562...  Training loss: 5.4055...  0.3440 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1563...  Training loss: 5.3808...  0.3423 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1564...  Training loss: 5.1547...  0.3427 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1565...  Training loss: 5.4265...  0.3393 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1566...  Training loss: 5.4342...  0.3398 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1567...  Training loss: 5.3632...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1568...  Training loss: 5.2664...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1569...  Training loss: 5.2477...  0.3452 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1570...  Training loss: 5.2281...  0.3442 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1571...  Training loss: 5.3158...  0.3431 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1572...  Training loss: 5.2506...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1573...  Training loss: 5.2806...  0.3448 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1574...  Training loss: 5.2985...  0.3447 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1575...  Training loss: 5.4304...  0.3421 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1576...  Training loss: 5.2901...  0.3440 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1577...  Training loss: 5.3899...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1578...  Training loss: 5.4095...  0.3397 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1579...  Training loss: 5.3420...  0.3423 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1580...  Training loss: 5.1081...  0.3450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100...  Training Step: 1581...  Training loss: 5.1270...  0.3408 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1582...  Training loss: 5.1372...  0.3390 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1583...  Training loss: 5.1275...  0.3378 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1584...  Training loss: 5.2300...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1585...  Training loss: 5.2134...  0.3441 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1586...  Training loss: 5.2061...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1587...  Training loss: 5.0828...  0.3430 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1588...  Training loss: 5.0980...  0.3421 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1589...  Training loss: 5.3074...  0.3410 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1590...  Training loss: 5.1379...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1591...  Training loss: 5.1137...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1592...  Training loss: 5.2071...  0.3432 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1593...  Training loss: 5.4129...  0.3394 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1594...  Training loss: 5.3163...  0.3401 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1595...  Training loss: 5.1941...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1596...  Training loss: 5.1840...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1597...  Training loss: 5.0965...  0.3395 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1598...  Training loss: 5.1141...  0.3410 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1599...  Training loss: 5.2067...  0.3438 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1600...  Training loss: 5.2397...  0.3403 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1601...  Training loss: 5.3706...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1602...  Training loss: 5.1953...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1603...  Training loss: 5.1973...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1604...  Training loss: 5.2523...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1605...  Training loss: 5.2848...  0.3434 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1606...  Training loss: 5.0513...  0.3426 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1607...  Training loss: 5.1264...  0.3428 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1608...  Training loss: 5.0512...  0.3418 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1609...  Training loss: 5.3485...  0.3428 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1610...  Training loss: 5.1349...  0.3425 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1611...  Training loss: 5.2267...  0.3395 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1612...  Training loss: 5.6844...  0.3406 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1613...  Training loss: 5.6326...  0.3391 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1614...  Training loss: 5.4268...  0.3424 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1615...  Training loss: 5.1812...  0.3444 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1616...  Training loss: 5.1307...  0.3390 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1617...  Training loss: 5.1150...  0.3417 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1618...  Training loss: 5.1807...  0.3432 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1619...  Training loss: 5.3177...  0.3427 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1620...  Training loss: 5.2450...  0.3403 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1621...  Training loss: 5.2476...  0.3421 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1622...  Training loss: 5.1999...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1623...  Training loss: 5.2054...  0.3409 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1624...  Training loss: 4.9966...  0.3397 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1625...  Training loss: 5.2322...  0.3419 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1626...  Training loss: 5.1488...  0.3436 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1627...  Training loss: 5.2251...  0.3431 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1628...  Training loss: 5.1983...  0.3426 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1629...  Training loss: 5.2802...  0.3427 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1630...  Training loss: 5.3065...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1631...  Training loss: 5.2013...  0.3397 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1632...  Training loss: 5.1786...  0.3424 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1633...  Training loss: 5.1904...  0.3449 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1634...  Training loss: 5.2597...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1635...  Training loss: 5.2795...  0.3437 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1636...  Training loss: 5.3088...  0.3433 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1637...  Training loss: 5.4775...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1638...  Training loss: 5.3878...  0.3424 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1639...  Training loss: 5.3032...  0.3451 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1640...  Training loss: 5.2191...  0.3438 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1641...  Training loss: 5.1859...  0.3408 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1642...  Training loss: 5.1604...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1643...  Training loss: 5.1570...  0.3429 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1644...  Training loss: 5.2042...  0.3387 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1645...  Training loss: 5.3014...  0.3392 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1646...  Training loss: 5.2359...  0.3435 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1647...  Training loss: 5.2428...  0.3430 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1648...  Training loss: 5.1543...  0.3408 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1649...  Training loss: 5.2218...  0.3413 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1650...  Training loss: 5.1740...  0.3415 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1651...  Training loss: 5.1505...  0.3438 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1652...  Training loss: 5.1788...  0.3412 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1653...  Training loss: 5.6290...  0.3387 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1654...  Training loss: 5.4756...  0.3404 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1655...  Training loss: 5.5230...  0.3411 sec/batch\n",
      "Epoch: 9/100...  Training Step: 1656...  Training loss: 5.4417...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1657...  Training loss: 5.6159...  0.3420 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1658...  Training loss: 5.6649...  0.3416 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1659...  Training loss: 5.4482...  0.3445 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1660...  Training loss: 5.3786...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1661...  Training loss: 5.3720...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1662...  Training loss: 5.2656...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1663...  Training loss: 5.5178...  0.3400 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1664...  Training loss: 5.3289...  0.3385 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1665...  Training loss: 5.3995...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1666...  Training loss: 5.2451...  0.3397 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1667...  Training loss: 5.2511...  0.3377 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1668...  Training loss: 5.2493...  0.3386 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1669...  Training loss: 5.2313...  0.3391 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1670...  Training loss: 5.3623...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1671...  Training loss: 5.3440...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1672...  Training loss: 5.3352...  0.3388 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1673...  Training loss: 5.3583...  0.3405 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1674...  Training loss: 5.2344...  0.3383 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1675...  Training loss: 5.3700...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1676...  Training loss: 5.2270...  0.3363 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1677...  Training loss: 5.2660...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1678...  Training loss: 5.2526...  0.3391 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100...  Training Step: 1679...  Training loss: 5.2527...  0.3389 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1680...  Training loss: 5.3010...  0.3378 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1681...  Training loss: 5.2686...  0.3394 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1682...  Training loss: 5.1867...  0.3405 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1683...  Training loss: 5.2378...  0.3388 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1684...  Training loss: 5.2200...  0.3387 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1685...  Training loss: 5.2415...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1686...  Training loss: 5.2444...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1687...  Training loss: 5.1946...  0.3376 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1688...  Training loss: 5.1734...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1689...  Training loss: 4.8858...  0.3388 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1690...  Training loss: 4.9908...  0.3374 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1691...  Training loss: 5.1282...  0.3391 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1692...  Training loss: 5.3369...  0.3389 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1693...  Training loss: 5.2257...  0.3397 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1694...  Training loss: 5.2843...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1695...  Training loss: 5.1797...  0.3381 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1696...  Training loss: 5.2004...  0.3413 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1697...  Training loss: 5.3112...  0.3398 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1698...  Training loss: 5.2550...  0.3382 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1699...  Training loss: 5.3298...  0.3398 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1700...  Training loss: 5.3255...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1701...  Training loss: 5.2148...  0.3383 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1702...  Training loss: 5.2025...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1703...  Training loss: 5.2575...  0.3421 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1704...  Training loss: 5.1203...  0.3425 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1705...  Training loss: 5.1930...  0.3398 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1706...  Training loss: 5.4006...  0.3416 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1707...  Training loss: 5.2930...  0.3381 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1708...  Training loss: 5.1892...  0.3385 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1709...  Training loss: 5.2791...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1710...  Training loss: 5.3097...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1711...  Training loss: 5.3195...  0.3387 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1712...  Training loss: 5.2016...  0.3381 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1713...  Training loss: 5.1057...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1714...  Training loss: 5.2741...  0.3381 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1715...  Training loss: 5.0805...  0.3397 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1716...  Training loss: 5.1229...  0.3410 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1717...  Training loss: 5.1186...  0.3398 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1718...  Training loss: 5.1861...  0.3429 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1719...  Training loss: 5.2550...  0.3420 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1720...  Training loss: 5.2734...  0.3413 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1721...  Training loss: 5.1206...  0.3443 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1722...  Training loss: 5.1386...  0.3423 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1723...  Training loss: 5.1661...  0.3433 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1724...  Training loss: 5.0586...  0.3410 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1725...  Training loss: 5.2891...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1726...  Training loss: 5.1976...  0.3408 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1727...  Training loss: 5.2307...  0.3398 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1728...  Training loss: 5.0170...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1729...  Training loss: 5.0613...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1730...  Training loss: 5.1246...  0.3383 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1731...  Training loss: 5.1045...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1732...  Training loss: 5.1097...  0.3388 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1733...  Training loss: 5.2164...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1734...  Training loss: 5.2388...  0.3439 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1735...  Training loss: 5.2698...  0.3434 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1736...  Training loss: 5.4699...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1737...  Training loss: 5.3920...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1738...  Training loss: 5.3678...  0.3411 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1739...  Training loss: 5.3337...  0.3401 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1740...  Training loss: 5.3028...  0.3433 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1741...  Training loss: 5.4169...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1742...  Training loss: 5.3013...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1743...  Training loss: 5.2514...  0.3429 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1744...  Training loss: 5.3556...  0.3415 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1745...  Training loss: 5.4255...  0.3403 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1746...  Training loss: 5.3879...  0.3376 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1747...  Training loss: 5.3718...  0.3395 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1748...  Training loss: 5.1111...  0.3415 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1749...  Training loss: 5.3839...  0.3405 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1750...  Training loss: 5.4138...  0.3415 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1751...  Training loss: 5.3309...  0.3386 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1752...  Training loss: 5.2390...  0.3386 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1753...  Training loss: 5.2318...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1754...  Training loss: 5.2158...  0.3431 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1755...  Training loss: 5.2977...  0.3376 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1756...  Training loss: 5.2192...  0.3390 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1757...  Training loss: 5.2563...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1758...  Training loss: 5.2726...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1759...  Training loss: 5.4238...  0.3381 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1760...  Training loss: 5.2781...  0.3389 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1761...  Training loss: 5.3635...  0.3385 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1762...  Training loss: 5.3968...  0.3386 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1763...  Training loss: 5.3271...  0.3384 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1764...  Training loss: 5.0996...  0.3387 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1765...  Training loss: 5.1213...  0.3395 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1766...  Training loss: 5.1295...  0.3400 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1767...  Training loss: 5.1108...  0.3385 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1768...  Training loss: 5.2095...  0.3397 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1769...  Training loss: 5.2033...  0.3390 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1770...  Training loss: 5.1959...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1771...  Training loss: 5.0819...  0.3403 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1772...  Training loss: 5.0958...  0.3393 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1773...  Training loss: 5.2722...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1774...  Training loss: 5.1282...  0.3420 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1775...  Training loss: 5.0995...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100...  Training Step: 1776...  Training loss: 5.1807...  0.3423 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1777...  Training loss: 5.3811...  0.3405 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1778...  Training loss: 5.2899...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1779...  Training loss: 5.1770...  0.3407 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1780...  Training loss: 5.1686...  0.3415 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1781...  Training loss: 5.0873...  0.3390 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1782...  Training loss: 5.1074...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1783...  Training loss: 5.1908...  0.3400 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1784...  Training loss: 5.2185...  0.3411 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1785...  Training loss: 5.3589...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1786...  Training loss: 5.1855...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1787...  Training loss: 5.1904...  0.3416 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1788...  Training loss: 5.2423...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1789...  Training loss: 5.2798...  0.3428 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1790...  Training loss: 5.0410...  0.3396 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1791...  Training loss: 5.1015...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1792...  Training loss: 5.0355...  0.3404 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1793...  Training loss: 5.3171...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1794...  Training loss: 5.1190...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1795...  Training loss: 5.2075...  0.3406 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1796...  Training loss: 5.6478...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1797...  Training loss: 5.6043...  0.3412 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1798...  Training loss: 5.4065...  0.3416 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1799...  Training loss: 5.1639...  0.3394 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1800...  Training loss: 5.1349...  0.3415 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1801...  Training loss: 5.1039...  0.3409 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1802...  Training loss: 5.1635...  0.3400 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1803...  Training loss: 5.3029...  0.3409 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1804...  Training loss: 5.2274...  0.3420 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1805...  Training loss: 5.2342...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1806...  Training loss: 5.1818...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1807...  Training loss: 5.1984...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1808...  Training loss: 4.9842...  0.3391 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1809...  Training loss: 5.2189...  0.3396 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1810...  Training loss: 5.1386...  0.3436 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1811...  Training loss: 5.2236...  0.3402 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1812...  Training loss: 5.1957...  0.3447 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1813...  Training loss: 5.2814...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1814...  Training loss: 5.3073...  0.3392 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1815...  Training loss: 5.1954...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1816...  Training loss: 5.1742...  0.3400 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1817...  Training loss: 5.1826...  0.3426 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1818...  Training loss: 5.2522...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1819...  Training loss: 5.2696...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1820...  Training loss: 5.2973...  0.3428 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1821...  Training loss: 5.4608...  0.3406 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1822...  Training loss: 5.3788...  0.3403 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1823...  Training loss: 5.2938...  0.3446 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1824...  Training loss: 5.2166...  0.3414 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1825...  Training loss: 5.1793...  0.3404 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1826...  Training loss: 5.1503...  0.3432 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1827...  Training loss: 5.1390...  0.3405 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1828...  Training loss: 5.1953...  0.3399 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1829...  Training loss: 5.2871...  0.3411 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1830...  Training loss: 5.2330...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1831...  Training loss: 5.2233...  0.3413 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1832...  Training loss: 5.1440...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1833...  Training loss: 5.2189...  0.3419 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1834...  Training loss: 5.1651...  0.3439 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1835...  Training loss: 5.1491...  0.3404 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1836...  Training loss: 5.1700...  0.3408 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1837...  Training loss: 5.5928...  0.3411 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1838...  Training loss: 5.4667...  0.3417 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1839...  Training loss: 5.5245...  0.3409 sec/batch\n",
      "Epoch: 10/100...  Training Step: 1840...  Training loss: 5.4255...  0.3400 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1841...  Training loss: 5.5827...  0.3440 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1842...  Training loss: 5.6525...  0.3435 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1843...  Training loss: 5.4356...  0.3408 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1844...  Training loss: 5.3527...  0.3406 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1845...  Training loss: 5.3609...  0.3411 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1846...  Training loss: 5.2556...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1847...  Training loss: 5.5086...  0.3460 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1848...  Training loss: 5.3096...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1849...  Training loss: 5.3857...  0.3418 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1850...  Training loss: 5.2340...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1851...  Training loss: 5.2591...  0.3428 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1852...  Training loss: 5.2272...  0.3406 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1853...  Training loss: 5.2195...  0.3411 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1854...  Training loss: 5.3558...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1855...  Training loss: 5.3333...  0.3407 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1856...  Training loss: 5.3246...  0.3399 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1857...  Training loss: 5.3433...  0.3421 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1858...  Training loss: 5.2159...  0.3402 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1859...  Training loss: 5.3410...  0.3401 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1860...  Training loss: 5.2106...  0.3400 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1861...  Training loss: 5.2640...  0.3410 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1862...  Training loss: 5.2449...  0.3404 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1863...  Training loss: 5.2361...  0.3416 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1864...  Training loss: 5.2881...  0.3432 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1865...  Training loss: 5.2487...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1866...  Training loss: 5.1776...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1867...  Training loss: 5.2150...  0.3411 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1868...  Training loss: 5.2051...  0.3384 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1869...  Training loss: 5.2218...  0.3422 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1870...  Training loss: 5.2258...  0.3418 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1871...  Training loss: 5.1707...  0.3420 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1872...  Training loss: 5.1447...  0.3395 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100...  Training Step: 1873...  Training loss: 4.8740...  0.3419 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1874...  Training loss: 4.9733...  0.3411 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1875...  Training loss: 5.1161...  0.3405 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1876...  Training loss: 5.3360...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1877...  Training loss: 5.2047...  0.3413 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1878...  Training loss: 5.2749...  0.3401 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1879...  Training loss: 5.1615...  0.3409 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1880...  Training loss: 5.1928...  0.3401 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1881...  Training loss: 5.2912...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1882...  Training loss: 5.2507...  0.3440 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1883...  Training loss: 5.3219...  0.3399 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1884...  Training loss: 5.3030...  0.3425 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1885...  Training loss: 5.2047...  0.3450 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1886...  Training loss: 5.1843...  0.3442 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1887...  Training loss: 5.2489...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1888...  Training loss: 5.1149...  0.3408 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1889...  Training loss: 5.2029...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1890...  Training loss: 5.3990...  0.3442 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1891...  Training loss: 5.2680...  0.3467 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1892...  Training loss: 5.1738...  0.3469 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1893...  Training loss: 5.2761...  0.3432 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1894...  Training loss: 5.3000...  0.3454 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1895...  Training loss: 5.3112...  0.3439 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1896...  Training loss: 5.1896...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1897...  Training loss: 5.0933...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1898...  Training loss: 5.2618...  0.3436 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1899...  Training loss: 5.0686...  0.3433 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1900...  Training loss: 5.1263...  0.3455 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1901...  Training loss: 5.1171...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1902...  Training loss: 5.1873...  0.3434 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1903...  Training loss: 5.2457...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1904...  Training loss: 5.2638...  0.3450 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1905...  Training loss: 5.0985...  0.3444 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1906...  Training loss: 5.1243...  0.3430 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1907...  Training loss: 5.1654...  0.3471 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1908...  Training loss: 5.0431...  0.3461 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1909...  Training loss: 5.2857...  0.3433 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1910...  Training loss: 5.1707...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1911...  Training loss: 5.2178...  0.3420 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1912...  Training loss: 5.0224...  0.3445 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1913...  Training loss: 5.0548...  0.3429 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1914...  Training loss: 5.1225...  0.3428 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1915...  Training loss: 5.0884...  0.3437 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1916...  Training loss: 5.0835...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1917...  Training loss: 5.1951...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1918...  Training loss: 5.2202...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1919...  Training loss: 5.2675...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1920...  Training loss: 5.4562...  0.3410 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1921...  Training loss: 5.3863...  0.3459 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1922...  Training loss: 5.3521...  0.3454 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1923...  Training loss: 5.3345...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1924...  Training loss: 5.2959...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1925...  Training loss: 5.4043...  0.3446 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1926...  Training loss: 5.2944...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1927...  Training loss: 5.2415...  0.3433 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1928...  Training loss: 5.3468...  0.3435 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1929...  Training loss: 5.4131...  0.3409 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1930...  Training loss: 5.3834...  0.3430 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1931...  Training loss: 5.3539...  0.3433 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1932...  Training loss: 5.0998...  0.3466 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1933...  Training loss: 5.3694...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1934...  Training loss: 5.3985...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1935...  Training loss: 5.3168...  0.3411 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1936...  Training loss: 5.2279...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1937...  Training loss: 5.2204...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1938...  Training loss: 5.1906...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1939...  Training loss: 5.2629...  0.3450 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1940...  Training loss: 5.1980...  0.3402 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1941...  Training loss: 5.2325...  0.3439 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1942...  Training loss: 5.2438...  0.3425 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1943...  Training loss: 5.4429...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1944...  Training loss: 5.2836...  0.3405 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1945...  Training loss: 5.3513...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1946...  Training loss: 5.3774...  0.3447 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1947...  Training loss: 5.3134...  0.3450 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1948...  Training loss: 5.1021...  0.3400 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1949...  Training loss: 5.1335...  0.3417 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1950...  Training loss: 5.1196...  0.3448 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1951...  Training loss: 5.1248...  0.3427 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1952...  Training loss: 5.2102...  0.3437 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1953...  Training loss: 5.1867...  0.3439 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1954...  Training loss: 5.1774...  0.3418 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1955...  Training loss: 5.0754...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1956...  Training loss: 5.0679...  0.3459 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1957...  Training loss: 5.2724...  0.3419 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1958...  Training loss: 5.1210...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1959...  Training loss: 5.0945...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1960...  Training loss: 5.1721...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1961...  Training loss: 5.3738...  0.3424 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1962...  Training loss: 5.2876...  0.3457 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1963...  Training loss: 5.1615...  0.3402 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1964...  Training loss: 5.1503...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1965...  Training loss: 5.0741...  0.3445 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1966...  Training loss: 5.0964...  0.3468 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1967...  Training loss: 5.1675...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1968...  Training loss: 5.2153...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1969...  Training loss: 5.3442...  0.3428 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100...  Training Step: 1970...  Training loss: 5.1571...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1971...  Training loss: 5.1851...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1972...  Training loss: 5.2240...  0.3430 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1973...  Training loss: 5.2488...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1974...  Training loss: 5.0255...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1975...  Training loss: 5.1040...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1976...  Training loss: 5.0159...  0.3425 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1977...  Training loss: 5.3049...  0.3424 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1978...  Training loss: 5.1107...  0.3409 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1979...  Training loss: 5.2054...  0.3424 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1980...  Training loss: 5.6428...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1981...  Training loss: 5.5929...  0.3415 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1982...  Training loss: 5.3886...  0.3416 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1983...  Training loss: 5.1522...  0.3448 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1984...  Training loss: 5.1052...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1985...  Training loss: 5.0981...  0.3410 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1986...  Training loss: 5.1564...  0.3419 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1987...  Training loss: 5.2923...  0.3420 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1988...  Training loss: 5.2129...  0.3444 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1989...  Training loss: 5.2237...  0.3424 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1990...  Training loss: 5.1783...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1991...  Training loss: 5.1912...  0.3403 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1992...  Training loss: 4.9833...  0.3460 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1993...  Training loss: 5.1995...  0.3441 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1994...  Training loss: 5.1252...  0.3449 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1995...  Training loss: 5.2051...  0.3451 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1996...  Training loss: 5.1833...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1997...  Training loss: 5.2718...  0.3434 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1998...  Training loss: 5.2817...  0.3429 sec/batch\n",
      "Epoch: 11/100...  Training Step: 1999...  Training loss: 5.1896...  0.3449 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2000...  Training loss: 5.1647...  0.3445 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2001...  Training loss: 5.1742...  0.3797 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2002...  Training loss: 5.2466...  0.3431 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2003...  Training loss: 5.2592...  0.3443 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2004...  Training loss: 5.2824...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2005...  Training loss: 5.4455...  0.3419 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2006...  Training loss: 5.3586...  0.3429 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2007...  Training loss: 5.2720...  0.3426 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2008...  Training loss: 5.2063...  0.3423 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2009...  Training loss: 5.1839...  0.3446 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2010...  Training loss: 5.1492...  0.3456 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2011...  Training loss: 5.1308...  0.3460 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2012...  Training loss: 5.1904...  0.3452 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2013...  Training loss: 5.2824...  0.3460 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2014...  Training loss: 5.2094...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2015...  Training loss: 5.2168...  0.3436 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2016...  Training loss: 5.1325...  0.3460 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2017...  Training loss: 5.2177...  0.3449 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2018...  Training loss: 5.1536...  0.3414 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2019...  Training loss: 5.1402...  0.3424 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2020...  Training loss: 5.1632...  0.3410 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2021...  Training loss: 5.5732...  0.3438 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2022...  Training loss: 5.4416...  0.3449 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2023...  Training loss: 5.5034...  0.3450 sec/batch\n",
      "Epoch: 11/100...  Training Step: 2024...  Training loss: 5.4239...  0.3428 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2025...  Training loss: 5.5621...  0.3441 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2026...  Training loss: 5.6437...  0.3451 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2027...  Training loss: 5.4203...  0.3456 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2028...  Training loss: 5.3505...  0.3455 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2029...  Training loss: 5.3597...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2030...  Training loss: 5.2440...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2031...  Training loss: 5.4870...  0.3447 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2032...  Training loss: 5.3015...  0.3420 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2033...  Training loss: 5.3814...  0.3426 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2034...  Training loss: 5.2204...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2035...  Training loss: 5.2415...  0.3418 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2036...  Training loss: 5.2131...  0.3453 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2037...  Training loss: 5.1933...  0.3448 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2038...  Training loss: 5.3434...  0.3424 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2039...  Training loss: 5.3238...  0.3442 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2040...  Training loss: 5.3162...  0.3446 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2041...  Training loss: 5.3399...  0.3452 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2042...  Training loss: 5.2026...  0.3466 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2043...  Training loss: 5.3341...  0.3439 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2044...  Training loss: 5.2126...  0.3444 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2045...  Training loss: 5.2377...  0.3451 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2046...  Training loss: 5.2307...  0.3396 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2047...  Training loss: 5.2217...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2048...  Training loss: 5.2670...  0.3416 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2049...  Training loss: 5.2393...  0.3432 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2050...  Training loss: 5.1548...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2051...  Training loss: 5.2057...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2052...  Training loss: 5.1894...  0.3406 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2053...  Training loss: 5.2237...  0.3390 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2054...  Training loss: 5.2255...  0.3425 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2055...  Training loss: 5.1587...  0.3402 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2056...  Training loss: 5.1401...  0.3424 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2057...  Training loss: 4.8724...  0.3435 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2058...  Training loss: 4.9709...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2059...  Training loss: 5.1144...  0.3392 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2060...  Training loss: 5.3014...  0.3389 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2061...  Training loss: 5.2017...  0.3435 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2062...  Training loss: 5.2737...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2063...  Training loss: 5.1473...  0.3433 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2064...  Training loss: 5.1852...  0.3424 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2065...  Training loss: 5.2819...  0.3408 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2066...  Training loss: 5.2354...  0.3442 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100...  Training Step: 2067...  Training loss: 5.3205...  0.3425 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2068...  Training loss: 5.2984...  0.3447 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2069...  Training loss: 5.1887...  0.3400 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2070...  Training loss: 5.1742...  0.3405 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2071...  Training loss: 5.2463...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2072...  Training loss: 5.0990...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2073...  Training loss: 5.1770...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2074...  Training loss: 5.3787...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2075...  Training loss: 5.2655...  0.3416 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2076...  Training loss: 5.1604...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2077...  Training loss: 5.2478...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2078...  Training loss: 5.2901...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2079...  Training loss: 5.2846...  0.3405 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2080...  Training loss: 5.1926...  0.3423 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2081...  Training loss: 5.0899...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2082...  Training loss: 5.2550...  0.3411 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2083...  Training loss: 5.0379...  0.3413 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2084...  Training loss: 5.1057...  0.3399 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2085...  Training loss: 5.0991...  0.3439 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2086...  Training loss: 5.1838...  0.3428 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2087...  Training loss: 5.2391...  0.3439 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2088...  Training loss: 5.2608...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2089...  Training loss: 5.0954...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2090...  Training loss: 5.1274...  0.3402 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2091...  Training loss: 5.1560...  0.3403 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2092...  Training loss: 5.0336...  0.3387 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2093...  Training loss: 5.2734...  0.3390 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2094...  Training loss: 5.1673...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2095...  Training loss: 5.2015...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2096...  Training loss: 5.0062...  0.3405 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2097...  Training loss: 5.0465...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2098...  Training loss: 5.1033...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2099...  Training loss: 5.0741...  0.3398 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2100...  Training loss: 5.0876...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2101...  Training loss: 5.1787...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2102...  Training loss: 5.2027...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2103...  Training loss: 5.2412...  0.3406 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2104...  Training loss: 5.4326...  0.3425 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2105...  Training loss: 5.3710...  0.3434 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2106...  Training loss: 5.3394...  0.3404 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2107...  Training loss: 5.3123...  0.3395 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2108...  Training loss: 5.2797...  0.3402 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2109...  Training loss: 5.3953...  0.3444 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2110...  Training loss: 5.2786...  0.3408 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2111...  Training loss: 5.2236...  0.3421 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2112...  Training loss: 5.3293...  0.3432 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2113...  Training loss: 5.3948...  0.3447 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2114...  Training loss: 5.3750...  0.3390 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2115...  Training loss: 5.3377...  0.3389 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2116...  Training loss: 5.0933...  0.3418 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2117...  Training loss: 5.3639...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2118...  Training loss: 5.3978...  0.3421 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2119...  Training loss: 5.3036...  0.3415 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2120...  Training loss: 5.2074...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2121...  Training loss: 5.1929...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2122...  Training loss: 5.1715...  0.3436 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2123...  Training loss: 5.2500...  0.3416 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2124...  Training loss: 5.1730...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2125...  Training loss: 5.2018...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2126...  Training loss: 5.2130...  0.3426 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2127...  Training loss: 5.4258...  0.3399 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2128...  Training loss: 5.2719...  0.3399 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2129...  Training loss: 5.3578...  0.3411 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2130...  Training loss: 5.3674...  0.3402 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2131...  Training loss: 5.2933...  0.3393 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2132...  Training loss: 5.0874...  0.3440 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2133...  Training loss: 5.1170...  0.3445 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2134...  Training loss: 5.1105...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2135...  Training loss: 5.1109...  0.3395 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2136...  Training loss: 5.2009...  0.3389 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2137...  Training loss: 5.1837...  0.3415 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2138...  Training loss: 5.1642...  0.3427 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2139...  Training loss: 5.0558...  0.3390 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2140...  Training loss: 5.0742...  0.3407 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2141...  Training loss: 5.2463...  0.3404 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2142...  Training loss: 5.1038...  0.3433 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2143...  Training loss: 5.0850...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2144...  Training loss: 5.1667...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2145...  Training loss: 5.3574...  0.3401 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2146...  Training loss: 5.2822...  0.3395 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2147...  Training loss: 5.1580...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2148...  Training loss: 5.1524...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2149...  Training loss: 5.0699...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2150...  Training loss: 5.0834...  0.3421 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2151...  Training loss: 5.1651...  0.3403 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2152...  Training loss: 5.2047...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2153...  Training loss: 5.3167...  0.3400 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2154...  Training loss: 5.1486...  0.3430 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2155...  Training loss: 5.1710...  0.3400 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2156...  Training loss: 5.2082...  0.3405 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2157...  Training loss: 5.2492...  0.3430 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2158...  Training loss: 5.0143...  0.3436 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2159...  Training loss: 5.0856...  0.3415 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2160...  Training loss: 5.0106...  0.3448 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2161...  Training loss: 5.2831...  0.3400 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2162...  Training loss: 5.0876...  0.3432 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2163...  Training loss: 5.1768...  0.3411 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100...  Training Step: 2164...  Training loss: 5.6046...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2165...  Training loss: 5.5798...  0.3422 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2166...  Training loss: 5.3828...  0.3391 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2167...  Training loss: 5.1348...  0.3418 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2168...  Training loss: 5.0901...  0.3398 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2169...  Training loss: 5.0826...  0.3427 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2170...  Training loss: 5.1492...  0.3417 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2171...  Training loss: 5.2746...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2172...  Training loss: 5.2047...  0.3415 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2173...  Training loss: 5.2167...  0.3416 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2174...  Training loss: 5.1499...  0.3403 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2175...  Training loss: 5.1714...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2176...  Training loss: 4.9770...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2177...  Training loss: 5.1946...  0.3406 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2178...  Training loss: 5.1210...  0.3416 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2179...  Training loss: 5.1892...  0.3424 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2180...  Training loss: 5.1696...  0.3429 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2181...  Training loss: 5.2512...  0.3418 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2182...  Training loss: 5.2651...  0.3409 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2183...  Training loss: 5.1591...  0.3389 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2184...  Training loss: 5.1416...  0.3387 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2185...  Training loss: 5.1715...  0.3395 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2186...  Training loss: 5.2131...  0.3439 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2187...  Training loss: 5.2434...  0.3418 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2188...  Training loss: 5.2740...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2189...  Training loss: 5.4261...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2190...  Training loss: 5.3390...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2191...  Training loss: 5.2577...  0.3396 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2192...  Training loss: 5.1907...  0.3399 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2193...  Training loss: 5.1643...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2194...  Training loss: 5.1262...  0.3397 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2195...  Training loss: 5.1209...  0.3405 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2196...  Training loss: 5.1706...  0.3414 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2197...  Training loss: 5.2825...  0.3393 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2198...  Training loss: 5.2090...  0.3421 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2199...  Training loss: 5.2089...  0.3401 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2200...  Training loss: 5.1283...  0.3429 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2201...  Training loss: 5.1997...  0.3408 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2202...  Training loss: 5.1553...  0.3419 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2203...  Training loss: 5.1332...  0.3427 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2204...  Training loss: 5.1592...  0.3453 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2205...  Training loss: 5.5581...  0.3411 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2206...  Training loss: 5.4370...  0.3412 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2207...  Training loss: 5.4987...  0.3410 sec/batch\n",
      "Epoch: 12/100...  Training Step: 2208...  Training loss: 5.3945...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2209...  Training loss: 5.5374...  0.3390 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2210...  Training loss: 5.6074...  0.3392 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2211...  Training loss: 5.3936...  0.3419 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2212...  Training loss: 5.3304...  0.3433 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2213...  Training loss: 5.3460...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2214...  Training loss: 5.2237...  0.3400 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2215...  Training loss: 5.4603...  0.3409 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2216...  Training loss: 5.2777...  0.3423 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2217...  Training loss: 5.3488...  0.3401 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2218...  Training loss: 5.1988...  0.3408 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2219...  Training loss: 5.2276...  0.3402 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2220...  Training loss: 5.1949...  0.3391 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2221...  Training loss: 5.1827...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2222...  Training loss: 5.3214...  0.3400 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2223...  Training loss: 5.2963...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2224...  Training loss: 5.2975...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2225...  Training loss: 5.3259...  0.3418 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2226...  Training loss: 5.2027...  0.3413 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2227...  Training loss: 5.3249...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2228...  Training loss: 5.2065...  0.3413 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2229...  Training loss: 5.2296...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2230...  Training loss: 5.2121...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2231...  Training loss: 5.2122...  0.3412 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2232...  Training loss: 5.2638...  0.3397 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2233...  Training loss: 5.2290...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2234...  Training loss: 5.1412...  0.3412 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2235...  Training loss: 5.1945...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2236...  Training loss: 5.1739...  0.3380 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2237...  Training loss: 5.1889...  0.3398 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2238...  Training loss: 5.1886...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2239...  Training loss: 5.1485...  0.3425 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2240...  Training loss: 5.1197...  0.3433 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2241...  Training loss: 4.8513...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2242...  Training loss: 4.9573...  0.3410 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2243...  Training loss: 5.0951...  0.3428 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2244...  Training loss: 5.2981...  0.3431 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2245...  Training loss: 5.1899...  0.3424 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2246...  Training loss: 5.2613...  0.3381 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2247...  Training loss: 5.1473...  0.3407 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2248...  Training loss: 5.1720...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2249...  Training loss: 5.2724...  0.3439 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2250...  Training loss: 5.2193...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2251...  Training loss: 5.2939...  0.3428 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2252...  Training loss: 5.2859...  0.3410 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2253...  Training loss: 5.1795...  0.3389 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2254...  Training loss: 5.1695...  0.3426 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2255...  Training loss: 5.2420...  0.3440 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2256...  Training loss: 5.0829...  0.3398 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2257...  Training loss: 5.1684...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2258...  Training loss: 5.3652...  0.3396 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2259...  Training loss: 5.2499...  0.3395 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2260...  Training loss: 5.1540...  0.3416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100...  Training Step: 2261...  Training loss: 5.2535...  0.3402 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2262...  Training loss: 5.2781...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2263...  Training loss: 5.2856...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2264...  Training loss: 5.1943...  0.3387 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2265...  Training loss: 5.0830...  0.3387 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2266...  Training loss: 5.2368...  0.3401 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2267...  Training loss: 5.0402...  0.3412 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2268...  Training loss: 5.0841...  0.3412 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2269...  Training loss: 5.0917...  0.3410 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2270...  Training loss: 5.1555...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2271...  Training loss: 5.2171...  0.3394 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2272...  Training loss: 5.2387...  0.3408 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2273...  Training loss: 5.0796...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2274...  Training loss: 5.1072...  0.3431 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2275...  Training loss: 5.1441...  0.3438 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2276...  Training loss: 5.0223...  0.3387 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2277...  Training loss: 5.2615...  0.3408 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2278...  Training loss: 5.1548...  0.3391 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2279...  Training loss: 5.1903...  0.3425 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2280...  Training loss: 4.9917...  0.3383 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2281...  Training loss: 5.0303...  0.3425 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2282...  Training loss: 5.0999...  0.3443 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2283...  Training loss: 5.0649...  0.3439 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2284...  Training loss: 5.0761...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2285...  Training loss: 5.1694...  0.3419 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2286...  Training loss: 5.1845...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2287...  Training loss: 5.2220...  0.3407 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2288...  Training loss: 5.4166...  0.3435 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2289...  Training loss: 5.3547...  0.3444 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2290...  Training loss: 5.3212...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2291...  Training loss: 5.3035...  0.3394 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2292...  Training loss: 5.2666...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2293...  Training loss: 5.3960...  0.3395 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2294...  Training loss: 5.2778...  0.3394 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2295...  Training loss: 5.2195...  0.3400 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2296...  Training loss: 5.3226...  0.3388 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2297...  Training loss: 5.4043...  0.3443 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2298...  Training loss: 5.3749...  0.3413 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2299...  Training loss: 5.3496...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2300...  Training loss: 5.0675...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2301...  Training loss: 5.3573...  0.3411 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2302...  Training loss: 5.3854...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2303...  Training loss: 5.3033...  0.3418 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2304...  Training loss: 5.2099...  0.3403 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2305...  Training loss: 5.1847...  0.3425 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2306...  Training loss: 5.1667...  0.3420 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2307...  Training loss: 5.2310...  0.3431 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2308...  Training loss: 5.1623...  0.3407 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2309...  Training loss: 5.1989...  0.3435 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2310...  Training loss: 5.2024...  0.3390 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2311...  Training loss: 5.3942...  0.3421 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2312...  Training loss: 5.2694...  0.3421 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2313...  Training loss: 5.3226...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2314...  Training loss: 5.3476...  0.3415 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2315...  Training loss: 5.2925...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2316...  Training loss: 5.0717...  0.3392 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2317...  Training loss: 5.1070...  0.3395 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2318...  Training loss: 5.1000...  0.3440 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2319...  Training loss: 5.0947...  0.3396 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2320...  Training loss: 5.1895...  0.3435 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2321...  Training loss: 5.1537...  0.3423 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2322...  Training loss: 5.1526...  0.3452 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2323...  Training loss: 5.0509...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2324...  Training loss: 5.0575...  0.3445 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2325...  Training loss: 5.2438...  0.3392 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2326...  Training loss: 5.0990...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2327...  Training loss: 5.0678...  0.3397 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2328...  Training loss: 5.1417...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2329...  Training loss: 5.3401...  0.3443 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2330...  Training loss: 5.2597...  0.3422 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2331...  Training loss: 5.1490...  0.3402 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2332...  Training loss: 5.1350...  0.3397 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2333...  Training loss: 5.0479...  0.3436 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2334...  Training loss: 5.0764...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2335...  Training loss: 5.1529...  0.3421 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2336...  Training loss: 5.1909...  0.3403 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2337...  Training loss: 5.3217...  0.3389 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2338...  Training loss: 5.1513...  0.3392 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2339...  Training loss: 5.1585...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2340...  Training loss: 5.2005...  0.3388 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2341...  Training loss: 5.2383...  0.3430 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2342...  Training loss: 4.9996...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2343...  Training loss: 5.0705...  0.3385 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2344...  Training loss: 5.0040...  0.3417 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2345...  Training loss: 5.2803...  0.3425 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2346...  Training loss: 5.0818...  0.3403 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2347...  Training loss: 5.1651...  0.3435 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2348...  Training loss: 5.5797...  0.3394 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2349...  Training loss: 5.5459...  0.3390 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2350...  Training loss: 5.3432...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2351...  Training loss: 5.1268...  0.3423 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2352...  Training loss: 5.0843...  0.3397 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2353...  Training loss: 5.0726...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2354...  Training loss: 5.1255...  0.3417 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2355...  Training loss: 5.2609...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2356...  Training loss: 5.1956...  0.3409 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2357...  Training loss: 5.2115...  0.3430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100...  Training Step: 2358...  Training loss: 5.1436...  0.3417 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2359...  Training loss: 5.1644...  0.3398 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2360...  Training loss: 4.9590...  0.3394 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2361...  Training loss: 5.1697...  0.3439 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2362...  Training loss: 5.0935...  0.3422 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2363...  Training loss: 5.1884...  0.3388 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2364...  Training loss: 5.1530...  0.3428 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2365...  Training loss: 5.2258...  0.3428 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2366...  Training loss: 5.2601...  0.3404 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2367...  Training loss: 5.1491...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2368...  Training loss: 5.1420...  0.3388 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2369...  Training loss: 5.1521...  0.3410 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2370...  Training loss: 5.1982...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2371...  Training loss: 5.2293...  0.3397 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2372...  Training loss: 5.2502...  0.3418 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2373...  Training loss: 5.4098...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2374...  Training loss: 5.3162...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2375...  Training loss: 5.2440...  0.3378 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2376...  Training loss: 5.1830...  0.3399 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2377...  Training loss: 5.1562...  0.3427 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2378...  Training loss: 5.1243...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2379...  Training loss: 5.0925...  0.3440 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2380...  Training loss: 5.1648...  0.3398 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2381...  Training loss: 5.2617...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2382...  Training loss: 5.1943...  0.3405 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2383...  Training loss: 5.1978...  0.3416 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2384...  Training loss: 5.1188...  0.3443 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2385...  Training loss: 5.1753...  0.3423 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2386...  Training loss: 5.1409...  0.3401 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2387...  Training loss: 5.1193...  0.3439 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2388...  Training loss: 5.1555...  0.3414 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2389...  Training loss: 5.5405...  0.3412 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2390...  Training loss: 5.4153...  0.3406 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2391...  Training loss: 5.4752...  0.3426 sec/batch\n",
      "Epoch: 13/100...  Training Step: 2392...  Training loss: 5.3815...  0.3414 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2393...  Training loss: 5.5190...  0.3433 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2394...  Training loss: 5.6030...  0.3400 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2395...  Training loss: 5.3845...  0.3400 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2396...  Training loss: 5.3080...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2397...  Training loss: 5.3333...  0.3435 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2398...  Training loss: 5.2071...  0.3422 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2399...  Training loss: 5.4581...  0.3417 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2400...  Training loss: 5.2572...  0.3459 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2401...  Training loss: 5.3299...  0.3432 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2402...  Training loss: 5.1898...  0.3417 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2403...  Training loss: 5.2040...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2404...  Training loss: 5.1667...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2405...  Training loss: 5.1578...  0.3418 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2406...  Training loss: 5.2886...  0.3403 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2407...  Training loss: 5.2750...  0.3422 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2408...  Training loss: 5.2649...  0.3449 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2409...  Training loss: 5.3195...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2410...  Training loss: 5.1870...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2411...  Training loss: 5.3036...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2412...  Training loss: 5.2028...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2413...  Training loss: 5.2151...  0.3394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2414...  Training loss: 5.2074...  0.3453 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2415...  Training loss: 5.2024...  0.3429 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2416...  Training loss: 5.2556...  0.3438 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2417...  Training loss: 5.2097...  0.3440 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2418...  Training loss: 5.1425...  0.3441 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2419...  Training loss: 5.1862...  0.3426 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2420...  Training loss: 5.1721...  0.3402 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2421...  Training loss: 5.1810...  0.3401 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2422...  Training loss: 5.1865...  0.3381 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2423...  Training loss: 5.1394...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2424...  Training loss: 5.1097...  0.3399 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2425...  Training loss: 4.8293...  0.3400 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2426...  Training loss: 4.9297...  0.3407 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2427...  Training loss: 5.0837...  0.3396 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2428...  Training loss: 5.3043...  0.3393 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2429...  Training loss: 5.1913...  0.3391 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2430...  Training loss: 5.2532...  0.3415 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2431...  Training loss: 5.1270...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2432...  Training loss: 5.1651...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2433...  Training loss: 5.2691...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2434...  Training loss: 5.2213...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2435...  Training loss: 5.2937...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2436...  Training loss: 5.2823...  0.3396 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2437...  Training loss: 5.1603...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2438...  Training loss: 5.1525...  0.3422 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2439...  Training loss: 5.2184...  0.3449 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2440...  Training loss: 5.0723...  0.3431 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2441...  Training loss: 5.1570...  0.3450 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2442...  Training loss: 5.3444...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2443...  Training loss: 5.2418...  0.3426 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2444...  Training loss: 5.1384...  0.3431 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2445...  Training loss: 5.2350...  0.3384 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2446...  Training loss: 5.2638...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2447...  Training loss: 5.2774...  0.3397 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2448...  Training loss: 5.1712...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2449...  Training loss: 5.0721...  0.3397 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2450...  Training loss: 5.2215...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2451...  Training loss: 5.0265...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2452...  Training loss: 5.0746...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2453...  Training loss: 5.0758...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2454...  Training loss: 5.1590...  0.3399 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100...  Training Step: 2455...  Training loss: 5.2171...  0.3391 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2456...  Training loss: 5.2254...  0.3396 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2457...  Training loss: 5.0718...  0.3417 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2458...  Training loss: 5.0878...  0.3422 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2459...  Training loss: 5.1376...  0.3397 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2460...  Training loss: 5.0111...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2461...  Training loss: 5.2354...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2462...  Training loss: 5.1524...  0.3395 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2463...  Training loss: 5.1882...  0.3406 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2464...  Training loss: 4.9887...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2465...  Training loss: 5.0257...  0.3394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2466...  Training loss: 5.0953...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2467...  Training loss: 5.0510...  0.3401 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2468...  Training loss: 5.0636...  0.3407 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2469...  Training loss: 5.1486...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2470...  Training loss: 5.1682...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2471...  Training loss: 5.2223...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2472...  Training loss: 5.3925...  0.3441 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2473...  Training loss: 5.3415...  0.3434 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2474...  Training loss: 5.3214...  0.3404 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2475...  Training loss: 5.2919...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2476...  Training loss: 5.2532...  0.3415 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2477...  Training loss: 5.3644...  0.3408 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2478...  Training loss: 5.2576...  0.3408 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2479...  Training loss: 5.2068...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2480...  Training loss: 5.3068...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2481...  Training loss: 5.3720...  0.3394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2482...  Training loss: 5.3502...  0.3400 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2483...  Training loss: 5.3272...  0.3432 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2484...  Training loss: 5.0691...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2485...  Training loss: 5.3542...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2486...  Training loss: 5.3566...  0.3389 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2487...  Training loss: 5.2915...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2488...  Training loss: 5.1984...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2489...  Training loss: 5.1710...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2490...  Training loss: 5.1516...  0.3394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2491...  Training loss: 5.2175...  0.3404 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2492...  Training loss: 5.1560...  0.3406 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2493...  Training loss: 5.1919...  0.3438 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2494...  Training loss: 5.2062...  0.3423 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2495...  Training loss: 5.3878...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2496...  Training loss: 5.2471...  0.3439 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2497...  Training loss: 5.3146...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2498...  Training loss: 5.3470...  0.3409 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2499...  Training loss: 5.2642...  0.3424 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2500...  Training loss: 5.0662...  0.3392 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2501...  Training loss: 5.0952...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2502...  Training loss: 5.0852...  0.3429 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2503...  Training loss: 5.0866...  0.3436 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2504...  Training loss: 5.1758...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2505...  Training loss: 5.1619...  0.3404 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2506...  Training loss: 5.1306...  0.3409 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2507...  Training loss: 5.0423...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2508...  Training loss: 5.0389...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2509...  Training loss: 5.2289...  0.3439 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2510...  Training loss: 5.0749...  0.3414 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2511...  Training loss: 5.0592...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2512...  Training loss: 5.1265...  0.3388 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2513...  Training loss: 5.3293...  0.3417 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2514...  Training loss: 5.2476...  0.3399 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2515...  Training loss: 5.1246...  0.3396 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2516...  Training loss: 5.1151...  0.3407 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2517...  Training loss: 5.0334...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2518...  Training loss: 5.0795...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2519...  Training loss: 5.1378...  0.3403 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2520...  Training loss: 5.1757...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2521...  Training loss: 5.2985...  0.3405 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2522...  Training loss: 5.1417...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2523...  Training loss: 5.1540...  0.3408 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2524...  Training loss: 5.1736...  0.3408 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2525...  Training loss: 5.2174...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2526...  Training loss: 4.9942...  0.3393 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2527...  Training loss: 5.0622...  0.3412 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2528...  Training loss: 4.9769...  0.3405 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2529...  Training loss: 5.2487...  0.3416 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2530...  Training loss: 5.0584...  0.3394 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2531...  Training loss: 5.1486...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2532...  Training loss: 5.5552...  0.3402 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2533...  Training loss: 5.5271...  0.3407 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2534...  Training loss: 5.3394...  0.3403 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2535...  Training loss: 5.1119...  0.3392 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2536...  Training loss: 5.0565...  0.3407 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2537...  Training loss: 5.0586...  0.3431 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2538...  Training loss: 5.1126...  0.3397 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2539...  Training loss: 5.2561...  0.3410 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2540...  Training loss: 5.1688...  0.3443 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2541...  Training loss: 5.1933...  0.3417 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2542...  Training loss: 5.1331...  0.3404 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2543...  Training loss: 5.1561...  0.3401 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2544...  Training loss: 4.9554...  0.3418 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2545...  Training loss: 5.1529...  0.3393 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2546...  Training loss: 5.0829...  0.3396 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2547...  Training loss: 5.1793...  0.3390 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2548...  Training loss: 5.1380...  0.3415 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2549...  Training loss: 5.2208...  0.3408 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2550...  Training loss: 5.2439...  0.3434 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2551...  Training loss: 5.1299...  0.3415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100...  Training Step: 2552...  Training loss: 5.1144...  0.3392 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2553...  Training loss: 5.1357...  0.3413 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2554...  Training loss: 5.1921...  0.3384 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2555...  Training loss: 5.2132...  0.3418 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2556...  Training loss: 5.2337...  0.3424 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2557...  Training loss: 5.3989...  0.3428 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2558...  Training loss: 5.3194...  0.3438 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2559...  Training loss: 5.2348...  0.3400 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2560...  Training loss: 5.1677...  0.3434 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2561...  Training loss: 5.1364...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2562...  Training loss: 5.1076...  0.3393 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2563...  Training loss: 5.0939...  0.3395 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2564...  Training loss: 5.1488...  0.3411 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2565...  Training loss: 5.2644...  0.3406 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2566...  Training loss: 5.1890...  0.3421 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2567...  Training loss: 5.1784...  0.3420 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2568...  Training loss: 5.1114...  0.3423 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2569...  Training loss: 5.1780...  0.3434 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2570...  Training loss: 5.1336...  0.3415 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2571...  Training loss: 5.1101...  0.3409 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2572...  Training loss: 5.1322...  0.3405 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2573...  Training loss: 5.5216...  0.3405 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2574...  Training loss: 5.3886...  0.3405 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2575...  Training loss: 5.4564...  0.3406 sec/batch\n",
      "Epoch: 14/100...  Training Step: 2576...  Training loss: 5.3599...  0.3398 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2577...  Training loss: 5.4969...  0.3452 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2578...  Training loss: 5.5788...  0.3417 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2579...  Training loss: 5.3614...  0.3392 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2580...  Training loss: 5.2930...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2581...  Training loss: 5.3110...  0.3441 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2582...  Training loss: 5.1865...  0.3406 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2583...  Training loss: 5.4226...  0.3391 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2584...  Training loss: 5.2460...  0.3393 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2585...  Training loss: 5.3126...  0.3405 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2586...  Training loss: 5.1696...  0.3398 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2587...  Training loss: 5.1805...  0.3395 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2588...  Training loss: 5.1416...  0.3404 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2589...  Training loss: 5.1322...  0.3393 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2590...  Training loss: 5.2710...  0.3440 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2591...  Training loss: 5.2511...  0.3413 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2592...  Training loss: 5.2382...  0.3405 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2593...  Training loss: 5.2945...  0.3458 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2594...  Training loss: 5.1557...  0.3461 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2595...  Training loss: 5.2957...  0.3460 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2596...  Training loss: 5.1777...  0.3433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2597...  Training loss: 5.2055...  0.3457 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2598...  Training loss: 5.1801...  0.3450 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2599...  Training loss: 5.1909...  0.3403 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2600...  Training loss: 5.2307...  0.3439 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2601...  Training loss: 5.2140...  0.3419 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2602...  Training loss: 5.1292...  0.3426 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2603...  Training loss: 5.1807...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2604...  Training loss: 5.1640...  0.3412 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2605...  Training loss: 5.1605...  0.3406 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2606...  Training loss: 5.1662...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2607...  Training loss: 5.1251...  0.3436 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2608...  Training loss: 5.0977...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2609...  Training loss: 4.8183...  0.3450 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2610...  Training loss: 4.9280...  0.3416 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2611...  Training loss: 5.0663...  0.3420 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2612...  Training loss: 5.2775...  0.3423 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2613...  Training loss: 5.1608...  0.3454 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2614...  Training loss: 5.2393...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2615...  Training loss: 5.1237...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2616...  Training loss: 5.1630...  0.3437 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2617...  Training loss: 5.2614...  0.3416 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2618...  Training loss: 5.2063...  0.3440 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2619...  Training loss: 5.2835...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2620...  Training loss: 5.2644...  0.3464 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2621...  Training loss: 5.1702...  0.3411 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2622...  Training loss: 5.1526...  0.3388 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2623...  Training loss: 5.2083...  0.3430 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2624...  Training loss: 5.0612...  0.3391 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2625...  Training loss: 5.1465...  0.3451 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2626...  Training loss: 5.3295...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2627...  Training loss: 5.2181...  0.3429 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2628...  Training loss: 5.1224...  0.3434 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2629...  Training loss: 5.2217...  0.3430 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2630...  Training loss: 5.2504...  0.3438 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2631...  Training loss: 5.2601...  0.3425 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2632...  Training loss: 5.1579...  0.3435 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2633...  Training loss: 5.0655...  0.3405 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2634...  Training loss: 5.2100...  0.3422 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2635...  Training loss: 5.0186...  0.3428 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2636...  Training loss: 5.0721...  0.3432 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2637...  Training loss: 5.0670...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2638...  Training loss: 5.1468...  0.3412 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2639...  Training loss: 5.1971...  0.3419 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2640...  Training loss: 5.2070...  0.3413 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2641...  Training loss: 5.0508...  0.3437 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2642...  Training loss: 5.0833...  0.3425 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2643...  Training loss: 5.1163...  0.3396 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2644...  Training loss: 4.9935...  0.3446 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2645...  Training loss: 5.2219...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2646...  Training loss: 5.1327...  0.3457 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2647...  Training loss: 5.1599...  0.3450 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2648...  Training loss: 4.9643...  0.3430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100...  Training Step: 2649...  Training loss: 5.0169...  0.3430 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2650...  Training loss: 5.0737...  0.3426 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2651...  Training loss: 5.0448...  0.3409 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2652...  Training loss: 5.0473...  0.3408 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2653...  Training loss: 5.1375...  0.3417 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2654...  Training loss: 5.1481...  0.3394 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2655...  Training loss: 5.2006...  0.3412 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2656...  Training loss: 5.3668...  0.3408 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2657...  Training loss: 5.3171...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2658...  Training loss: 5.3037...  0.3413 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2659...  Training loss: 5.2829...  0.3416 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2660...  Training loss: 5.2255...  0.3443 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2661...  Training loss: 5.3532...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2662...  Training loss: 5.2355...  0.3451 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2663...  Training loss: 5.1941...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2664...  Training loss: 5.2943...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2665...  Training loss: 5.3560...  0.3420 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2666...  Training loss: 5.3358...  0.3432 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2667...  Training loss: 5.3160...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2668...  Training loss: 5.0417...  0.3401 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2669...  Training loss: 5.3304...  0.3391 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2670...  Training loss: 5.3449...  0.3413 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2671...  Training loss: 5.2557...  0.3408 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2672...  Training loss: 5.1660...  0.3399 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2673...  Training loss: 5.1615...  0.3400 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2674...  Training loss: 5.1245...  0.3425 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2675...  Training loss: 5.1926...  0.3438 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2676...  Training loss: 5.1329...  0.3451 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2677...  Training loss: 5.1651...  0.3458 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2678...  Training loss: 5.1749...  0.3419 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2679...  Training loss: 5.3401...  0.3412 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2680...  Training loss: 5.2151...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2681...  Training loss: 5.2738...  0.3397 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2682...  Training loss: 5.3114...  0.3398 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2683...  Training loss: 5.2576...  0.3436 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2684...  Training loss: 5.0612...  0.3433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2685...  Training loss: 5.0721...  0.3384 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2686...  Training loss: 5.0706...  0.3407 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2687...  Training loss: 5.0618...  0.3403 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2688...  Training loss: 5.1770...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2689...  Training loss: 5.1540...  0.3404 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2690...  Training loss: 5.1182...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2691...  Training loss: 5.0306...  0.3414 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2692...  Training loss: 5.0304...  0.3443 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2693...  Training loss: 5.2159...  0.3400 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2694...  Training loss: 5.0593...  0.3417 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2695...  Training loss: 5.0545...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2696...  Training loss: 5.1191...  0.3436 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2697...  Training loss: 5.3142...  0.3452 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2698...  Training loss: 5.2239...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2699...  Training loss: 5.1262...  0.3399 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2700...  Training loss: 5.1070...  0.3415 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2701...  Training loss: 5.0176...  0.3435 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2702...  Training loss: 5.0501...  0.3432 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2703...  Training loss: 5.1336...  0.3419 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2704...  Training loss: 5.1592...  0.3406 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2705...  Training loss: 5.2765...  0.3424 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2706...  Training loss: 5.1211...  0.3432 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2707...  Training loss: 5.1370...  0.3417 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2708...  Training loss: 5.1576...  0.3404 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2709...  Training loss: 5.2134...  0.3401 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2710...  Training loss: 4.9783...  0.3422 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2711...  Training loss: 5.0543...  0.3443 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2712...  Training loss: 4.9724...  0.3427 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2713...  Training loss: 5.2283...  0.3427 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2714...  Training loss: 5.0460...  0.3437 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2715...  Training loss: 5.1353...  0.3428 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2716...  Training loss: 5.5242...  0.3442 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2717...  Training loss: 5.4992...  0.3429 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2718...  Training loss: 5.3195...  0.3395 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2719...  Training loss: 5.0774...  0.3423 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2720...  Training loss: 5.0389...  0.3405 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2721...  Training loss: 5.0369...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2722...  Training loss: 5.1096...  0.3401 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2723...  Training loss: 5.2342...  0.3444 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2724...  Training loss: 5.1636...  0.3433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2725...  Training loss: 5.1856...  0.3445 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2726...  Training loss: 5.1107...  0.3440 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2727...  Training loss: 5.1439...  0.3443 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2728...  Training loss: 4.9460...  0.3406 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2729...  Training loss: 5.1478...  0.3425 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2730...  Training loss: 5.0723...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2731...  Training loss: 5.1610...  0.3426 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2732...  Training loss: 5.1232...  0.3407 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2733...  Training loss: 5.2009...  0.3441 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2734...  Training loss: 5.2239...  0.3445 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2735...  Training loss: 5.1258...  0.3403 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2736...  Training loss: 5.1233...  0.3431 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2737...  Training loss: 5.1186...  0.3445 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2738...  Training loss: 5.1818...  0.3420 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2739...  Training loss: 5.2014...  0.3432 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2740...  Training loss: 5.2227...  0.3431 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2741...  Training loss: 5.3706...  0.3444 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2742...  Training loss: 5.2915...  0.3446 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2743...  Training loss: 5.2178...  0.3412 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2744...  Training loss: 5.1641...  0.3433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2745...  Training loss: 5.1254...  0.3428 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100...  Training Step: 2746...  Training loss: 5.0917...  0.3445 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2747...  Training loss: 5.0933...  0.3417 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2748...  Training loss: 5.1360...  0.3418 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2749...  Training loss: 5.2316...  0.3455 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2750...  Training loss: 5.1797...  0.3427 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2751...  Training loss: 5.1742...  0.3421 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2752...  Training loss: 5.1098...  0.3403 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2753...  Training loss: 5.1681...  0.3442 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2754...  Training loss: 5.1206...  0.3404 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2755...  Training loss: 5.1000...  0.3438 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2756...  Training loss: 5.1290...  0.3458 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2757...  Training loss: 5.5211...  0.3433 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2758...  Training loss: 5.3783...  0.3440 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2759...  Training loss: 5.4484...  0.3430 sec/batch\n",
      "Epoch: 15/100...  Training Step: 2760...  Training loss: 5.3432...  0.3437 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2761...  Training loss: 5.5045...  0.3459 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2762...  Training loss: 5.5716...  0.3408 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2763...  Training loss: 5.3528...  0.3424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2764...  Training loss: 5.2944...  0.3432 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2765...  Training loss: 5.3058...  0.3439 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2766...  Training loss: 5.1912...  0.3440 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2767...  Training loss: 5.4237...  0.3442 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2768...  Training loss: 5.2343...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2769...  Training loss: 5.3171...  0.3444 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2770...  Training loss: 5.1660...  0.3441 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2771...  Training loss: 5.1890...  0.3423 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2772...  Training loss: 5.1424...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2773...  Training loss: 5.1152...  0.3396 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2774...  Training loss: 5.2430...  0.3424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2775...  Training loss: 5.2448...  0.3430 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2776...  Training loss: 5.2173...  0.3409 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2777...  Training loss: 5.2578...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2778...  Training loss: 5.1501...  0.3408 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2779...  Training loss: 5.2965...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2780...  Training loss: 5.1729...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2781...  Training loss: 5.1777...  0.3436 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2782...  Training loss: 5.1622...  0.3427 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2783...  Training loss: 5.1746...  0.3391 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2784...  Training loss: 5.2191...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2785...  Training loss: 5.1832...  0.3411 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2786...  Training loss: 5.1137...  0.3414 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2787...  Training loss: 5.1613...  0.3424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2788...  Training loss: 5.1426...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2789...  Training loss: 5.1636...  0.3437 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2790...  Training loss: 5.1645...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2791...  Training loss: 5.1201...  0.3422 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2792...  Training loss: 5.0892...  0.3438 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2793...  Training loss: 4.8314...  0.3409 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2794...  Training loss: 4.9229...  0.3395 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2795...  Training loss: 5.0570...  0.3447 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2796...  Training loss: 5.2550...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2797...  Training loss: 5.1521...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2798...  Training loss: 5.2287...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2799...  Training loss: 5.0860...  0.3414 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2800...  Training loss: 5.1319...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2801...  Training loss: 5.2315...  0.3436 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2802...  Training loss: 5.1998...  0.3408 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2803...  Training loss: 5.2648...  0.3414 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2804...  Training loss: 5.2483...  0.3409 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2805...  Training loss: 5.1480...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2806...  Training loss: 5.1374...  0.3439 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2807...  Training loss: 5.2109...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2808...  Training loss: 5.0622...  0.3403 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2809...  Training loss: 5.1321...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2810...  Training loss: 5.3211...  0.3414 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2811...  Training loss: 5.2113...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2812...  Training loss: 5.1037...  0.3405 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2813...  Training loss: 5.2058...  0.3404 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2814...  Training loss: 5.2410...  0.3427 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2815...  Training loss: 5.2475...  0.3441 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2816...  Training loss: 5.1495...  0.3442 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2817...  Training loss: 5.0529...  0.3425 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2818...  Training loss: 5.1900...  0.3421 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2819...  Training loss: 4.9979...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2820...  Training loss: 5.0553...  0.3414 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2821...  Training loss: 5.0528...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2822...  Training loss: 5.1259...  0.3417 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2823...  Training loss: 5.1827...  0.3424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2824...  Training loss: 5.2012...  0.3418 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2825...  Training loss: 5.0522...  0.3407 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2826...  Training loss: 5.0612...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2827...  Training loss: 5.1058...  0.3385 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2828...  Training loss: 4.9919...  0.3396 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2829...  Training loss: 5.2144...  0.3393 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2830...  Training loss: 5.1246...  0.3411 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2831...  Training loss: 5.1609...  0.3431 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2832...  Training loss: 4.9533...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2833...  Training loss: 4.9946...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2834...  Training loss: 5.0606...  0.3441 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2835...  Training loss: 5.0154...  0.3431 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2836...  Training loss: 5.0404...  0.3415 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2837...  Training loss: 5.1215...  0.3406 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2838...  Training loss: 5.1322...  0.3386 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2839...  Training loss: 5.1875...  0.3404 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2840...  Training loss: 5.3589...  0.3403 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2841...  Training loss: 5.2945...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2842...  Training loss: 5.2732...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/100...  Training Step: 2843...  Training loss: 5.2628...  0.3444 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2844...  Training loss: 5.2210...  0.3424 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2845...  Training loss: 5.3487...  0.3430 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2846...  Training loss: 5.2367...  0.3426 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2847...  Training loss: 5.1616...  0.3446 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2848...  Training loss: 5.2725...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2849...  Training loss: 5.3452...  0.3426 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2850...  Training loss: 5.3349...  0.3428 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2851...  Training loss: 5.3018...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2852...  Training loss: 5.0443...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2853...  Training loss: 5.3143...  0.3425 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2854...  Training loss: 5.3147...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2855...  Training loss: 5.2516...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2856...  Training loss: 5.1715...  0.3405 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2857...  Training loss: 5.1337...  0.3400 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2858...  Training loss: 5.0991...  0.3428 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2859...  Training loss: 5.1798...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2860...  Training loss: 5.1014...  0.3415 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2861...  Training loss: 5.1241...  0.3388 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2862...  Training loss: 5.1440...  0.3444 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2863...  Training loss: 5.3308...  0.3434 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2864...  Training loss: 5.2104...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2865...  Training loss: 5.2804...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2866...  Training loss: 5.3029...  0.3443 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2867...  Training loss: 5.2397...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2868...  Training loss: 5.0474...  0.3397 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2869...  Training loss: 5.0584...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2870...  Training loss: 5.0592...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2871...  Training loss: 5.0507...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2872...  Training loss: 5.1569...  0.3402 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2873...  Training loss: 5.1206...  0.3405 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2874...  Training loss: 5.0934...  0.3443 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2875...  Training loss: 5.0086...  0.3404 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2876...  Training loss: 5.0116...  0.3391 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2877...  Training loss: 5.2032...  0.3439 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2878...  Training loss: 5.0397...  0.3429 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2879...  Training loss: 5.0437...  0.3409 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2880...  Training loss: 5.1119...  0.3455 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2881...  Training loss: 5.3048...  0.3428 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2882...  Training loss: 5.2085...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2883...  Training loss: 5.1120...  0.3438 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2884...  Training loss: 5.0997...  0.3437 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2885...  Training loss: 5.0121...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2886...  Training loss: 5.0381...  0.3408 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2887...  Training loss: 5.1220...  0.3418 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2888...  Training loss: 5.1376...  0.3443 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2889...  Training loss: 5.2665...  0.3456 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2890...  Training loss: 5.1107...  0.3460 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2891...  Training loss: 5.1252...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2892...  Training loss: 5.1587...  0.3388 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2893...  Training loss: 5.1973...  0.3433 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2894...  Training loss: 4.9719...  0.3455 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2895...  Training loss: 5.0367...  0.3443 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2896...  Training loss: 4.9702...  0.3401 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2897...  Training loss: 5.2240...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2898...  Training loss: 5.0323...  0.3422 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2899...  Training loss: 5.1100...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2900...  Training loss: 5.5079...  0.3419 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2901...  Training loss: 5.4801...  0.3430 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2902...  Training loss: 5.2943...  0.3422 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2903...  Training loss: 5.0741...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2904...  Training loss: 5.0399...  0.3422 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2905...  Training loss: 5.0217...  0.3411 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2906...  Training loss: 5.0779...  0.3436 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2907...  Training loss: 5.2311...  0.3418 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2908...  Training loss: 5.1456...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2909...  Training loss: 5.1655...  0.3452 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2910...  Training loss: 5.1006...  0.3403 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2911...  Training loss: 5.1416...  0.3401 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2912...  Training loss: 4.9441...  0.3416 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2913...  Training loss: 5.1512...  0.3425 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2914...  Training loss: 5.0641...  0.3394 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2915...  Training loss: 5.1508...  0.3392 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2916...  Training loss: 5.1268...  0.3430 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2917...  Training loss: 5.1948...  0.3404 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2918...  Training loss: 5.2319...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2919...  Training loss: 5.1112...  0.3415 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2920...  Training loss: 5.0982...  0.3395 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2921...  Training loss: 5.1162...  0.3426 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2922...  Training loss: 5.1622...  0.3435 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2923...  Training loss: 5.1912...  0.3412 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2924...  Training loss: 5.2056...  0.3402 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2925...  Training loss: 5.3425...  0.3402 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2926...  Training loss: 5.2645...  0.3401 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2927...  Training loss: 5.2084...  0.3410 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2928...  Training loss: 5.1466...  0.3451 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2929...  Training loss: 5.1179...  0.3392 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2930...  Training loss: 5.0872...  0.3406 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2931...  Training loss: 5.0675...  0.3390 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2932...  Training loss: 5.1332...  0.3395 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2933...  Training loss: 5.2289...  0.3417 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2934...  Training loss: 5.1538...  0.3399 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2935...  Training loss: 5.1643...  0.3420 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2936...  Training loss: 5.0893...  0.3415 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2937...  Training loss: 5.1663...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2938...  Training loss: 5.1164...  0.3451 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2939...  Training loss: 5.0775...  0.3437 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/100...  Training Step: 2940...  Training loss: 5.1106...  0.3396 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2941...  Training loss: 5.4934...  0.3415 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2942...  Training loss: 5.3669...  0.3429 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2943...  Training loss: 5.4117...  0.3413 sec/batch\n",
      "Epoch: 16/100...  Training Step: 2944...  Training loss: 5.3182...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2945...  Training loss: 5.4772...  0.3443 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2946...  Training loss: 5.5527...  0.3435 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2947...  Training loss: 5.3133...  0.3435 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2948...  Training loss: 5.2779...  0.3449 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2949...  Training loss: 5.2755...  0.3428 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2950...  Training loss: 5.1581...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2951...  Training loss: 5.4099...  0.3430 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2952...  Training loss: 5.2184...  0.3421 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2953...  Training loss: 5.2835...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2954...  Training loss: 5.1446...  0.3444 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2955...  Training loss: 5.1637...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2956...  Training loss: 5.1230...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2957...  Training loss: 5.1120...  0.3456 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2958...  Training loss: 5.2420...  0.3451 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2959...  Training loss: 5.2256...  0.3416 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2960...  Training loss: 5.2004...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2961...  Training loss: 5.2559...  0.3451 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2962...  Training loss: 5.1373...  0.3450 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2963...  Training loss: 5.2664...  0.3416 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2964...  Training loss: 5.1527...  0.3430 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2965...  Training loss: 5.1637...  0.3437 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2966...  Training loss: 5.1524...  0.3429 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2967...  Training loss: 5.1494...  0.3452 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2968...  Training loss: 5.1777...  0.3408 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2969...  Training loss: 5.1571...  0.3450 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2970...  Training loss: 5.0897...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2971...  Training loss: 5.1396...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2972...  Training loss: 5.1173...  0.3428 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2973...  Training loss: 5.1420...  0.3451 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2974...  Training loss: 5.1556...  0.3426 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2975...  Training loss: 5.1129...  0.3428 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2976...  Training loss: 5.0604...  0.3402 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2977...  Training loss: 4.7972...  0.3422 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2978...  Training loss: 4.9056...  0.3436 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2979...  Training loss: 5.0331...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2980...  Training loss: 5.2447...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2981...  Training loss: 5.1194...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2982...  Training loss: 5.2139...  0.3423 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2983...  Training loss: 5.0866...  0.3445 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2984...  Training loss: 5.1133...  0.3406 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2985...  Training loss: 5.2227...  0.3428 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2986...  Training loss: 5.1657...  0.3447 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2987...  Training loss: 5.2539...  0.3423 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2988...  Training loss: 5.2288...  0.3400 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2989...  Training loss: 5.1311...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2990...  Training loss: 5.1127...  0.3416 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2991...  Training loss: 5.1892...  0.3393 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2992...  Training loss: 5.0353...  0.3438 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2993...  Training loss: 5.1298...  0.3419 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2994...  Training loss: 5.3040...  0.3432 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2995...  Training loss: 5.2013...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2996...  Training loss: 5.1022...  0.3412 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2997...  Training loss: 5.1953...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2998...  Training loss: 5.2249...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 2999...  Training loss: 5.2164...  0.3438 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3000...  Training loss: 5.1256...  0.3414 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3001...  Training loss: 5.0498...  0.3839 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3002...  Training loss: 5.1727...  0.3433 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3003...  Training loss: 4.9998...  0.3400 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3004...  Training loss: 5.0283...  0.3413 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3005...  Training loss: 5.0317...  0.3421 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3006...  Training loss: 5.1156...  0.3427 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3007...  Training loss: 5.1674...  0.3395 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3008...  Training loss: 5.1801...  0.3433 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3009...  Training loss: 5.0317...  0.3406 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3010...  Training loss: 5.0533...  0.3433 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3011...  Training loss: 5.0836...  0.3408 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3012...  Training loss: 4.9808...  0.3404 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3013...  Training loss: 5.1855...  0.3404 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3014...  Training loss: 5.0928...  0.3403 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3015...  Training loss: 5.1359...  0.3416 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3016...  Training loss: 4.9400...  0.3427 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3017...  Training loss: 4.9862...  0.3401 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3018...  Training loss: 5.0508...  0.3410 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3019...  Training loss: 5.0120...  0.3425 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3020...  Training loss: 5.0322...  0.3408 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3021...  Training loss: 5.1031...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3022...  Training loss: 5.1151...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3023...  Training loss: 5.1629...  0.3442 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3024...  Training loss: 5.3305...  0.3433 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3025...  Training loss: 5.2831...  0.3394 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3026...  Training loss: 5.2647...  0.3412 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3027...  Training loss: 5.2324...  0.3403 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3028...  Training loss: 5.1947...  0.3406 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3029...  Training loss: 5.3066...  0.3412 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3030...  Training loss: 5.2017...  0.3387 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3031...  Training loss: 5.1546...  0.3430 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3032...  Training loss: 5.2608...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3033...  Training loss: 5.3364...  0.3401 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3034...  Training loss: 5.3134...  0.3432 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3035...  Training loss: 5.2879...  0.3412 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3036...  Training loss: 5.0349...  0.3433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100...  Training Step: 3037...  Training loss: 5.2977...  0.3427 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3038...  Training loss: 5.3000...  0.3421 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3039...  Training loss: 5.2178...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3040...  Training loss: 5.1709...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3041...  Training loss: 5.1312...  0.3410 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3042...  Training loss: 5.1065...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3043...  Training loss: 5.1517...  0.3383 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3044...  Training loss: 5.0979...  0.3422 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3045...  Training loss: 5.1065...  0.3424 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3046...  Training loss: 5.1110...  0.3414 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3047...  Training loss: 5.2943...  0.3437 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3048...  Training loss: 5.1879...  0.3424 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3049...  Training loss: 5.2503...  0.3412 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3050...  Training loss: 5.2893...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3051...  Training loss: 5.2266...  0.3429 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3052...  Training loss: 5.0200...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3053...  Training loss: 5.0363...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3054...  Training loss: 5.0398...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3055...  Training loss: 5.0265...  0.3408 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3056...  Training loss: 5.1303...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3057...  Training loss: 5.1207...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3058...  Training loss: 5.0909...  0.3393 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3059...  Training loss: 4.9944...  0.3403 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3060...  Training loss: 4.9854...  0.3419 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3061...  Training loss: 5.1866...  0.3398 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3062...  Training loss: 5.0311...  0.3421 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3063...  Training loss: 5.0264...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3064...  Training loss: 5.0917...  0.3418 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3065...  Training loss: 5.2969...  0.3404 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3066...  Training loss: 5.1958...  0.3403 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3067...  Training loss: 5.0976...  0.3404 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3068...  Training loss: 5.0913...  0.3425 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3069...  Training loss: 5.0041...  0.3403 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3070...  Training loss: 5.0268...  0.3418 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3071...  Training loss: 5.1111...  0.3439 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3072...  Training loss: 5.1407...  0.3413 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3073...  Training loss: 5.2459...  0.3399 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3074...  Training loss: 5.1078...  0.3428 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3075...  Training loss: 5.1153...  0.3426 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3076...  Training loss: 5.1486...  0.3426 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3077...  Training loss: 5.1740...  0.3420 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3078...  Training loss: 4.9502...  0.3424 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3079...  Training loss: 5.0267...  0.3437 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3080...  Training loss: 4.9609...  0.3431 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3081...  Training loss: 5.1893...  0.3407 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3082...  Training loss: 5.0166...  0.3427 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3083...  Training loss: 5.1074...  0.3411 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3084...  Training loss: 5.4833...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3085...  Training loss: 5.4427...  0.3408 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3086...  Training loss: 5.2798...  0.3396 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3087...  Training loss: 5.0519...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3088...  Training loss: 5.0146...  0.3433 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3089...  Training loss: 5.0009...  0.3411 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3090...  Training loss: 5.0515...  0.3441 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3091...  Training loss: 5.1755...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3092...  Training loss: 5.1218...  0.3398 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3093...  Training loss: 5.1329...  0.3404 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3094...  Training loss: 5.0729...  0.3401 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3095...  Training loss: 5.1036...  0.3410 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3096...  Training loss: 4.9153...  0.3423 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3097...  Training loss: 5.1274...  0.3415 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3098...  Training loss: 5.0495...  0.3414 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3099...  Training loss: 5.1403...  0.3443 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3100...  Training loss: 5.1092...  0.3420 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3101...  Training loss: 5.1848...  0.3419 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3102...  Training loss: 5.2062...  0.3461 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3103...  Training loss: 5.0991...  0.3430 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3104...  Training loss: 5.0930...  0.3411 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3105...  Training loss: 5.1010...  0.3451 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3106...  Training loss: 5.1590...  0.3420 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3107...  Training loss: 5.1846...  0.3411 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3108...  Training loss: 5.1879...  0.3449 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3109...  Training loss: 5.3375...  0.3449 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3110...  Training loss: 5.2445...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3111...  Training loss: 5.1916...  0.3409 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3112...  Training loss: 5.1404...  0.3398 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3113...  Training loss: 5.1036...  0.3399 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3114...  Training loss: 5.0937...  0.3424 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3115...  Training loss: 5.0618...  0.3417 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3116...  Training loss: 5.1204...  0.3406 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3117...  Training loss: 5.2052...  0.3437 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3118...  Training loss: 5.1636...  0.3429 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3119...  Training loss: 5.1569...  0.3398 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3120...  Training loss: 5.0925...  0.3406 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3121...  Training loss: 5.1632...  0.3426 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3122...  Training loss: 5.1024...  0.3396 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3123...  Training loss: 5.0894...  0.3395 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3124...  Training loss: 5.0949...  0.3414 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3125...  Training loss: 5.4744...  0.3416 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3126...  Training loss: 5.3264...  0.3405 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3127...  Training loss: 5.3811...  0.3401 sec/batch\n",
      "Epoch: 17/100...  Training Step: 3128...  Training loss: 5.2877...  0.3423 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3129...  Training loss: 5.4509...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3130...  Training loss: 5.5175...  0.3433 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3131...  Training loss: 5.2980...  0.3398 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3132...  Training loss: 5.2615...  0.3413 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3133...  Training loss: 5.2785...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/100...  Training Step: 3134...  Training loss: 5.1357...  0.3406 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3135...  Training loss: 5.4002...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3136...  Training loss: 5.2074...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3137...  Training loss: 5.2790...  0.3455 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3138...  Training loss: 5.1439...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3139...  Training loss: 5.1674...  0.3410 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3140...  Training loss: 5.1197...  0.3441 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3141...  Training loss: 5.1008...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3142...  Training loss: 5.2277...  0.3437 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3143...  Training loss: 5.2217...  0.3402 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3144...  Training loss: 5.2194...  0.3400 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3145...  Training loss: 5.2518...  0.3388 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3146...  Training loss: 5.1218...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3147...  Training loss: 5.2617...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3148...  Training loss: 5.1302...  0.3449 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3149...  Training loss: 5.1407...  0.3414 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3150...  Training loss: 5.1333...  0.3405 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3151...  Training loss: 5.1426...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3152...  Training loss: 5.1752...  0.3414 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3153...  Training loss: 5.1528...  0.3397 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3154...  Training loss: 5.0950...  0.3405 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3155...  Training loss: 5.1187...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3156...  Training loss: 5.0904...  0.3411 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3157...  Training loss: 5.1066...  0.3403 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3158...  Training loss: 5.1202...  0.3402 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3159...  Training loss: 5.0903...  0.3422 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3160...  Training loss: 5.0440...  0.3431 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3161...  Training loss: 4.7507...  0.3395 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3162...  Training loss: 4.8713...  0.3429 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3163...  Training loss: 5.0260...  0.3440 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3164...  Training loss: 5.2345...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3165...  Training loss: 5.1330...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3166...  Training loss: 5.1805...  0.3392 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3167...  Training loss: 5.0620...  0.3392 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3168...  Training loss: 5.1031...  0.3434 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3169...  Training loss: 5.2058...  0.3402 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3170...  Training loss: 5.1727...  0.3446 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3171...  Training loss: 5.2410...  0.3378 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3172...  Training loss: 5.2189...  0.3390 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3173...  Training loss: 5.1169...  0.3421 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3174...  Training loss: 5.1004...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3175...  Training loss: 5.1723...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3176...  Training loss: 5.0324...  0.3400 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3177...  Training loss: 5.1163...  0.3438 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3178...  Training loss: 5.2859...  0.3421 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3179...  Training loss: 5.1852...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3180...  Training loss: 5.0840...  0.3436 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3181...  Training loss: 5.1768...  0.3394 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3182...  Training loss: 5.2053...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3183...  Training loss: 5.2090...  0.3427 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3184...  Training loss: 5.1091...  0.3397 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3185...  Training loss: 5.0347...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3186...  Training loss: 5.1830...  0.3445 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3187...  Training loss: 4.9938...  0.3437 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3188...  Training loss: 5.0255...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3189...  Training loss: 5.0298...  0.3395 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3190...  Training loss: 5.0982...  0.3406 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3191...  Training loss: 5.1469...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3192...  Training loss: 5.1505...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3193...  Training loss: 5.0310...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3194...  Training loss: 5.0638...  0.3431 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3195...  Training loss: 5.0841...  0.3417 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3196...  Training loss: 4.9669...  0.3413 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3197...  Training loss: 5.1945...  0.3417 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3198...  Training loss: 5.0942...  0.3437 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3199...  Training loss: 5.1306...  0.3439 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3200...  Training loss: 4.9356...  0.3425 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3201...  Training loss: 4.9841...  0.3433 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3202...  Training loss: 5.0397...  0.3405 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3203...  Training loss: 5.0170...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3204...  Training loss: 5.0172...  0.3391 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3205...  Training loss: 5.0886...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3206...  Training loss: 5.1098...  0.3422 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3207...  Training loss: 5.1511...  0.3399 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3208...  Training loss: 5.2979...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3209...  Training loss: 5.2685...  0.3395 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3210...  Training loss: 5.2330...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3211...  Training loss: 5.2131...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3212...  Training loss: 5.1717...  0.3384 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3213...  Training loss: 5.3009...  0.3419 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3214...  Training loss: 5.1933...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3215...  Training loss: 5.1388...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3216...  Training loss: 5.2435...  0.3400 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3217...  Training loss: 5.3244...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3218...  Training loss: 5.3217...  0.3424 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3219...  Training loss: 5.2700...  0.3411 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3220...  Training loss: 5.0005...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3221...  Training loss: 5.2901...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3222...  Training loss: 5.3169...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3223...  Training loss: 5.2405...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3224...  Training loss: 5.1724...  0.3399 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3225...  Training loss: 5.1368...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3226...  Training loss: 5.1093...  0.3406 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3227...  Training loss: 5.1785...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3228...  Training loss: 5.1002...  0.3399 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3229...  Training loss: 5.1141...  0.3424 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3230...  Training loss: 5.1003...  0.3416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/100...  Training Step: 3231...  Training loss: 5.2985...  0.3430 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3232...  Training loss: 5.1798...  0.3435 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3233...  Training loss: 5.2526...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3234...  Training loss: 5.2868...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3235...  Training loss: 5.2226...  0.3422 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3236...  Training loss: 5.0253...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3237...  Training loss: 5.0389...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3238...  Training loss: 5.0208...  0.3410 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3239...  Training loss: 5.0258...  0.3424 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3240...  Training loss: 5.1403...  0.3426 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3241...  Training loss: 5.0927...  0.3398 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3242...  Training loss: 5.0689...  0.3399 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3243...  Training loss: 4.9878...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3244...  Training loss: 4.9889...  0.3388 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3245...  Training loss: 5.1476...  0.3400 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3246...  Training loss: 5.0114...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3247...  Training loss: 5.0063...  0.3422 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3248...  Training loss: 5.0578...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3249...  Training loss: 5.2615...  0.3443 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3250...  Training loss: 5.1808...  0.3425 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3251...  Training loss: 5.1093...  0.3392 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3252...  Training loss: 5.0854...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3253...  Training loss: 5.0007...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3254...  Training loss: 5.0238...  0.3416 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3255...  Training loss: 5.1145...  0.3404 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3256...  Training loss: 5.1337...  0.3421 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3257...  Training loss: 5.2410...  0.3406 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3258...  Training loss: 5.1195...  0.3409 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3259...  Training loss: 5.0986...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3260...  Training loss: 5.1311...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3261...  Training loss: 5.1782...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3262...  Training loss: 4.9597...  0.3419 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3263...  Training loss: 5.0146...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3264...  Training loss: 4.9559...  0.3427 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3265...  Training loss: 5.1792...  0.3437 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3266...  Training loss: 4.9996...  0.3429 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3267...  Training loss: 5.0973...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3268...  Training loss: 5.4665...  0.3395 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3269...  Training loss: 5.4311...  0.3429 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3270...  Training loss: 5.2455...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3271...  Training loss: 5.0248...  0.3430 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3272...  Training loss: 4.9833...  0.3385 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3273...  Training loss: 4.9915...  0.3415 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3274...  Training loss: 5.0474...  0.3421 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3275...  Training loss: 5.1655...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3276...  Training loss: 5.1035...  0.3411 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3277...  Training loss: 5.1297...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3278...  Training loss: 5.0577...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3279...  Training loss: 5.0846...  0.3391 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3280...  Training loss: 4.9085...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3281...  Training loss: 5.0998...  0.3429 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3282...  Training loss: 5.0326...  0.3414 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3283...  Training loss: 5.1408...  0.3388 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3284...  Training loss: 5.0927...  0.3396 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3285...  Training loss: 5.1657...  0.3419 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3286...  Training loss: 5.1827...  0.3411 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3287...  Training loss: 5.0866...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3288...  Training loss: 5.0774...  0.3443 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3289...  Training loss: 5.0966...  0.3429 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3290...  Training loss: 5.1306...  0.3390 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3291...  Training loss: 5.1545...  0.3401 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3292...  Training loss: 5.1787...  0.3403 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3293...  Training loss: 5.3180...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3294...  Training loss: 5.2301...  0.3418 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3295...  Training loss: 5.1678...  0.3403 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3296...  Training loss: 5.1327...  0.3398 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3297...  Training loss: 5.0940...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3298...  Training loss: 5.0620...  0.3406 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3299...  Training loss: 5.0693...  0.3407 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3300...  Training loss: 5.1151...  0.3410 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3301...  Training loss: 5.1934...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3302...  Training loss: 5.1457...  0.3408 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3303...  Training loss: 5.1467...  0.3420 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3304...  Training loss: 5.0663...  0.3419 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3305...  Training loss: 5.1434...  0.3399 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3306...  Training loss: 5.0885...  0.3425 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3307...  Training loss: 5.0758...  0.3412 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3308...  Training loss: 5.0879...  0.3405 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3309...  Training loss: 5.4775...  0.3417 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3310...  Training loss: 5.3207...  0.3397 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3311...  Training loss: 5.3845...  0.3402 sec/batch\n",
      "Epoch: 18/100...  Training Step: 3312...  Training loss: 5.2816...  0.3387 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3313...  Training loss: 5.4322...  0.3440 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3314...  Training loss: 5.4795...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3315...  Training loss: 5.2769...  0.3437 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3316...  Training loss: 5.2316...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3317...  Training loss: 5.2411...  0.3391 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3318...  Training loss: 5.1136...  0.3433 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3319...  Training loss: 5.3501...  0.3426 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3320...  Training loss: 5.1832...  0.3400 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3321...  Training loss: 5.2539...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3322...  Training loss: 5.1137...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3323...  Training loss: 5.1216...  0.3422 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3324...  Training loss: 5.1037...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3325...  Training loss: 5.0741...  0.3421 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3326...  Training loss: 5.1891...  0.3454 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3327...  Training loss: 5.2000...  0.3440 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/100...  Training Step: 3328...  Training loss: 5.1763...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3329...  Training loss: 5.2061...  0.3409 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3330...  Training loss: 5.0888...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3331...  Training loss: 5.2117...  0.3413 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3332...  Training loss: 5.1097...  0.3412 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3333...  Training loss: 5.1315...  0.3450 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3334...  Training loss: 5.1098...  0.3423 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3335...  Training loss: 5.1132...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3336...  Training loss: 5.1415...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3337...  Training loss: 5.1369...  0.3429 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3338...  Training loss: 5.0805...  0.3444 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3339...  Training loss: 5.1104...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3340...  Training loss: 5.0772...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3341...  Training loss: 5.1044...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3342...  Training loss: 5.1016...  0.3423 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3343...  Training loss: 5.0746...  0.3462 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3344...  Training loss: 5.0233...  0.3419 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3345...  Training loss: 4.7466...  0.3409 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3346...  Training loss: 4.8517...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3347...  Training loss: 4.9996...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3348...  Training loss: 5.2018...  0.3411 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3349...  Training loss: 5.1190...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3350...  Training loss: 5.1804...  0.3436 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3351...  Training loss: 5.0696...  0.3402 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3352...  Training loss: 5.0899...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3353...  Training loss: 5.1989...  0.3401 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3354...  Training loss: 5.1392...  0.3387 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3355...  Training loss: 5.2277...  0.3417 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3356...  Training loss: 5.1991...  0.3419 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3357...  Training loss: 5.0919...  0.3389 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3358...  Training loss: 5.0780...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3359...  Training loss: 5.1496...  0.3415 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3360...  Training loss: 5.0131...  0.3436 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3361...  Training loss: 5.1019...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3362...  Training loss: 5.2771...  0.3437 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3363...  Training loss: 5.1603...  0.3450 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3364...  Training loss: 5.0791...  0.3417 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3365...  Training loss: 5.1529...  0.3417 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3366...  Training loss: 5.1933...  0.3442 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3367...  Training loss: 5.1878...  0.3401 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3368...  Training loss: 5.0877...  0.3403 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3369...  Training loss: 5.0135...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3370...  Training loss: 5.1453...  0.3399 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3371...  Training loss: 4.9785...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3372...  Training loss: 5.0149...  0.3412 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3373...  Training loss: 5.0119...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3374...  Training loss: 5.0773...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3375...  Training loss: 5.1332...  0.3396 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3376...  Training loss: 5.1490...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3377...  Training loss: 5.0010...  0.3403 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3378...  Training loss: 5.0353...  0.3430 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3379...  Training loss: 5.0781...  0.3423 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3380...  Training loss: 4.9763...  0.3396 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3381...  Training loss: 5.1682...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3382...  Training loss: 5.0888...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3383...  Training loss: 5.1208...  0.3394 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3384...  Training loss: 4.9293...  0.3398 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3385...  Training loss: 4.9715...  0.3416 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3386...  Training loss: 5.0149...  0.3399 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3387...  Training loss: 4.9865...  0.3426 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3388...  Training loss: 5.0074...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3389...  Training loss: 5.0981...  0.3429 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3390...  Training loss: 5.1003...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3391...  Training loss: 5.1448...  0.3402 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3392...  Training loss: 5.2897...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3393...  Training loss: 5.2645...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3394...  Training loss: 5.2226...  0.3403 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3395...  Training loss: 5.2048...  0.3426 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3396...  Training loss: 5.1503...  0.3402 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3397...  Training loss: 5.2758...  0.3421 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3398...  Training loss: 5.1716...  0.3412 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3399...  Training loss: 5.1234...  0.3431 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3400...  Training loss: 5.2171...  0.3416 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3401...  Training loss: 5.3040...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3402...  Training loss: 5.2910...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3403...  Training loss: 5.2590...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3404...  Training loss: 5.0011...  0.3389 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3405...  Training loss: 5.2802...  0.3382 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3406...  Training loss: 5.2874...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3407...  Training loss: 5.2127...  0.3413 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3408...  Training loss: 5.1399...  0.3440 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3409...  Training loss: 5.1137...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3410...  Training loss: 5.0778...  0.3418 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3411...  Training loss: 5.1618...  0.3403 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3412...  Training loss: 5.1086...  0.3397 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3413...  Training loss: 5.1281...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3414...  Training loss: 5.1309...  0.3431 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3415...  Training loss: 5.2713...  0.3399 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3416...  Training loss: 5.1408...  0.3429 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3417...  Training loss: 5.2156...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3418...  Training loss: 5.2314...  0.3429 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3419...  Training loss: 5.1762...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3420...  Training loss: 4.9997...  0.3449 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3421...  Training loss: 5.0050...  0.3440 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3422...  Training loss: 5.0204...  0.3442 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3423...  Training loss: 5.0217...  0.3436 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3424...  Training loss: 5.1164...  0.3397 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/100...  Training Step: 3425...  Training loss: 5.0834...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3426...  Training loss: 5.0585...  0.3400 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3427...  Training loss: 4.9868...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3428...  Training loss: 4.9670...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3429...  Training loss: 5.1486...  0.3412 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3430...  Training loss: 5.0010...  0.3389 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3431...  Training loss: 4.9818...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3432...  Training loss: 5.0612...  0.3391 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3433...  Training loss: 5.2438...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3434...  Training loss: 5.1567...  0.3393 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3435...  Training loss: 5.0766...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3436...  Training loss: 5.0631...  0.3404 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3437...  Training loss: 4.9800...  0.3432 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3438...  Training loss: 5.0004...  0.3394 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3439...  Training loss: 5.0694...  0.3418 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3440...  Training loss: 5.1136...  0.3395 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3441...  Training loss: 5.2284...  0.3415 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3442...  Training loss: 5.0931...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3443...  Training loss: 5.0942...  0.3393 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3444...  Training loss: 5.1123...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3445...  Training loss: 5.1566...  0.3418 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3446...  Training loss: 4.9383...  0.3401 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3447...  Training loss: 5.0145...  0.3410 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3448...  Training loss: 4.9431...  0.3442 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3449...  Training loss: 5.1608...  0.3437 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3450...  Training loss: 4.9899...  0.3397 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3451...  Training loss: 5.0795...  0.3414 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3452...  Training loss: 5.4221...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3453...  Training loss: 5.4033...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3454...  Training loss: 5.2390...  0.3429 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3455...  Training loss: 5.0177...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3456...  Training loss: 4.9767...  0.3440 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3457...  Training loss: 4.9687...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3458...  Training loss: 5.0417...  0.3434 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3459...  Training loss: 5.1767...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3460...  Training loss: 5.0992...  0.3407 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3461...  Training loss: 5.1107...  0.3412 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3462...  Training loss: 5.0479...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3463...  Training loss: 5.0811...  0.3431 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3464...  Training loss: 4.8985...  0.3434 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3465...  Training loss: 5.0825...  0.3433 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3466...  Training loss: 5.0178...  0.3419 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3467...  Training loss: 5.1099...  0.3418 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3468...  Training loss: 5.0748...  0.3411 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3469...  Training loss: 5.1601...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3470...  Training loss: 5.1708...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3471...  Training loss: 5.0732...  0.3411 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3472...  Training loss: 5.0624...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3473...  Training loss: 5.0735...  0.3415 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3474...  Training loss: 5.1133...  0.3423 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3475...  Training loss: 5.1410...  0.3419 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3476...  Training loss: 5.1489...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3477...  Training loss: 5.2939...  0.3402 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3478...  Training loss: 5.2155...  0.3406 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3479...  Training loss: 5.1490...  0.3414 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3480...  Training loss: 5.1159...  0.3398 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3481...  Training loss: 5.1084...  0.3423 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3482...  Training loss: 5.0654...  0.3405 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3483...  Training loss: 5.0513...  0.3414 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3484...  Training loss: 5.0816...  0.3389 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3485...  Training loss: 5.1542...  0.3419 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3486...  Training loss: 5.1150...  0.3397 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3487...  Training loss: 5.1165...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3488...  Training loss: 5.0582...  0.3408 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3489...  Training loss: 5.1215...  0.3424 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3490...  Training loss: 5.0841...  0.3399 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3491...  Training loss: 5.0490...  0.3427 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3492...  Training loss: 5.0754...  0.3400 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3493...  Training loss: 5.4634...  0.3397 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3494...  Training loss: 5.3032...  0.3420 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3495...  Training loss: 5.3713...  0.3391 sec/batch\n",
      "Epoch: 19/100...  Training Step: 3496...  Training loss: 5.2671...  0.3412 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3497...  Training loss: 5.4193...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3498...  Training loss: 5.4814...  0.3431 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3499...  Training loss: 5.2570...  0.3398 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3500...  Training loss: 5.2293...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3501...  Training loss: 5.2306...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3502...  Training loss: 5.1038...  0.3440 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3503...  Training loss: 5.3355...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3504...  Training loss: 5.1668...  0.3424 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3505...  Training loss: 5.2215...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3506...  Training loss: 5.0977...  0.3416 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3507...  Training loss: 5.1179...  0.3411 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3508...  Training loss: 5.0779...  0.3416 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3509...  Training loss: 5.0398...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3510...  Training loss: 5.1727...  0.3392 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3511...  Training loss: 5.1623...  0.3411 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3512...  Training loss: 5.1345...  0.3402 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3513...  Training loss: 5.2014...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3514...  Training loss: 5.0800...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3515...  Training loss: 5.1995...  0.3424 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3516...  Training loss: 5.0824...  0.3425 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3517...  Training loss: 5.0869...  0.3419 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3518...  Training loss: 5.0973...  0.3422 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3519...  Training loss: 5.0931...  0.3411 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3520...  Training loss: 5.1411...  0.3416 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3521...  Training loss: 5.1207...  0.3390 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100...  Training Step: 3522...  Training loss: 5.0430...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3523...  Training loss: 5.1004...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3524...  Training loss: 5.0797...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3525...  Training loss: 5.0964...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3526...  Training loss: 5.0929...  0.3429 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3527...  Training loss: 5.0683...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3528...  Training loss: 5.0356...  0.3424 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3529...  Training loss: 4.7807...  0.3425 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3530...  Training loss: 4.8627...  0.3423 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3531...  Training loss: 5.0026...  0.3395 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3532...  Training loss: 5.2087...  0.3405 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3533...  Training loss: 5.1249...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3534...  Training loss: 5.1637...  0.3449 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3535...  Training loss: 5.0404...  0.3436 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3536...  Training loss: 5.0970...  0.3398 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3537...  Training loss: 5.2106...  0.3438 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3538...  Training loss: 5.1678...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3539...  Training loss: 5.2095...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3540...  Training loss: 5.2046...  0.3403 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3541...  Training loss: 5.0873...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3542...  Training loss: 5.0513...  0.3409 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3543...  Training loss: 5.1368...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3544...  Training loss: 5.0018...  0.3419 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3545...  Training loss: 5.0940...  0.3402 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3546...  Training loss: 5.2540...  0.3388 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3547...  Training loss: 5.1372...  0.3402 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3548...  Training loss: 5.0636...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3549...  Training loss: 5.1309...  0.3389 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3550...  Training loss: 5.1702...  0.3424 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3551...  Training loss: 5.1902...  0.3409 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3552...  Training loss: 5.0804...  0.3396 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3553...  Training loss: 4.9943...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3554...  Training loss: 5.1269...  0.3419 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3555...  Training loss: 4.9471...  0.3430 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3556...  Training loss: 5.0035...  0.3430 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3557...  Training loss: 4.9924...  0.3400 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3558...  Training loss: 5.0552...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3559...  Training loss: 5.1129...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3560...  Training loss: 5.1351...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3561...  Training loss: 4.9938...  0.3446 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3562...  Training loss: 5.0064...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3563...  Training loss: 5.0378...  0.3397 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3564...  Training loss: 4.9230...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3565...  Training loss: 5.1412...  0.3420 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3566...  Training loss: 5.0684...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3567...  Training loss: 5.1198...  0.3394 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3568...  Training loss: 4.9155...  0.3412 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3569...  Training loss: 4.9364...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3570...  Training loss: 5.0030...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3571...  Training loss: 4.9829...  0.3436 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3572...  Training loss: 4.9739...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3573...  Training loss: 5.0564...  0.3402 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3574...  Training loss: 5.0629...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3575...  Training loss: 5.1208...  0.3427 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3576...  Training loss: 5.2680...  0.3397 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3577...  Training loss: 5.2340...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3578...  Training loss: 5.2050...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3579...  Training loss: 5.1757...  0.3439 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3580...  Training loss: 5.1401...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3581...  Training loss: 5.2482...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3582...  Training loss: 5.1309...  0.3412 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3583...  Training loss: 5.0919...  0.3400 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3584...  Training loss: 5.1892...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3585...  Training loss: 5.2739...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3586...  Training loss: 5.2645...  0.3390 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3587...  Training loss: 5.2369...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3588...  Training loss: 4.9819...  0.3390 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3589...  Training loss: 5.2406...  0.3393 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3590...  Training loss: 5.2643...  0.3433 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3591...  Training loss: 5.1865...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3592...  Training loss: 5.1271...  0.3405 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3593...  Training loss: 5.0925...  0.3420 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3594...  Training loss: 5.0633...  0.3426 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3595...  Training loss: 5.1374...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3596...  Training loss: 5.0718...  0.3438 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3597...  Training loss: 5.0976...  0.3442 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3598...  Training loss: 5.0913...  0.3425 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3599...  Training loss: 5.2438...  0.3397 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3600...  Training loss: 5.1197...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3601...  Training loss: 5.1852...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3602...  Training loss: 5.2139...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3603...  Training loss: 5.1589...  0.3450 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3604...  Training loss: 4.9707...  0.3407 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3605...  Training loss: 4.9788...  0.3431 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3606...  Training loss: 4.9895...  0.3431 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3607...  Training loss: 4.9746...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3608...  Training loss: 5.0929...  0.3443 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3609...  Training loss: 5.0425...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3610...  Training loss: 5.0178...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3611...  Training loss: 4.9654...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3612...  Training loss: 4.9472...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3613...  Training loss: 5.1310...  0.3403 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3614...  Training loss: 4.9888...  0.3390 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3615...  Training loss: 4.9716...  0.3430 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3616...  Training loss: 5.0227...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3617...  Training loss: 5.2286...  0.3439 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3618...  Training loss: 5.1446...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100...  Training Step: 3619...  Training loss: 5.0520...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3620...  Training loss: 5.0339...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3621...  Training loss: 4.9640...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3622...  Training loss: 4.9919...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3623...  Training loss: 5.0672...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3624...  Training loss: 5.1017...  0.3409 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3625...  Training loss: 5.2105...  0.3394 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3626...  Training loss: 5.0702...  0.3399 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3627...  Training loss: 5.0723...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3628...  Training loss: 5.0932...  0.3412 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3629...  Training loss: 5.1347...  0.3398 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3630...  Training loss: 4.9252...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3631...  Training loss: 4.9982...  0.3448 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3632...  Training loss: 4.9290...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3633...  Training loss: 5.1613...  0.3449 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3634...  Training loss: 4.9888...  0.3417 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3635...  Training loss: 5.0704...  0.3425 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3636...  Training loss: 5.4170...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3637...  Training loss: 5.3985...  0.3403 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3638...  Training loss: 5.2271...  0.3391 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3639...  Training loss: 4.9914...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3640...  Training loss: 4.9605...  0.3397 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3641...  Training loss: 4.9470...  0.3407 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3642...  Training loss: 4.9964...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3643...  Training loss: 5.1280...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3644...  Training loss: 5.0669...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3645...  Training loss: 5.0875...  0.3397 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3646...  Training loss: 5.0334...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3647...  Training loss: 5.0558...  0.3411 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3648...  Training loss: 4.8723...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3649...  Training loss: 5.0773...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3650...  Training loss: 5.0043...  0.3416 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3651...  Training loss: 5.1003...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3652...  Training loss: 5.0471...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3653...  Training loss: 5.1251...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3654...  Training loss: 5.1393...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3655...  Training loss: 5.0660...  0.3413 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3656...  Training loss: 5.0408...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3657...  Training loss: 5.0570...  0.3419 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3658...  Training loss: 5.1059...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3659...  Training loss: 5.1145...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3660...  Training loss: 5.1338...  0.3410 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3661...  Training loss: 5.2872...  0.3428 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3662...  Training loss: 5.2172...  0.3430 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3663...  Training loss: 5.1586...  0.3422 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3664...  Training loss: 5.0857...  0.3401 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3665...  Training loss: 5.0705...  0.3436 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3666...  Training loss: 5.0450...  0.3421 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3667...  Training loss: 5.0390...  0.3422 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3668...  Training loss: 5.0745...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3669...  Training loss: 5.1788...  0.3403 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3670...  Training loss: 5.1016...  0.3411 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3671...  Training loss: 5.1010...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3672...  Training loss: 5.0440...  0.3404 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3673...  Training loss: 5.1200...  0.3408 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3674...  Training loss: 5.0832...  0.3414 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3675...  Training loss: 5.0435...  0.3431 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3676...  Training loss: 5.0731...  0.3407 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3677...  Training loss: 5.4058...  0.3415 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3678...  Training loss: 5.2704...  0.3418 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3679...  Training loss: 5.3388...  0.3406 sec/batch\n",
      "Epoch: 20/100...  Training Step: 3680...  Training loss: 5.2308...  0.3411 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3681...  Training loss: 5.3790...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3682...  Training loss: 5.4514...  0.3426 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3683...  Training loss: 5.2324...  0.3424 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3684...  Training loss: 5.1971...  0.3399 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3685...  Training loss: 5.2142...  0.3416 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3686...  Training loss: 5.0863...  0.3440 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3687...  Training loss: 5.3435...  0.3414 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3688...  Training loss: 5.1558...  0.3396 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3689...  Training loss: 5.2280...  0.3435 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3690...  Training loss: 5.0773...  0.3388 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3691...  Training loss: 5.0868...  0.3413 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3692...  Training loss: 5.0594...  0.3413 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3693...  Training loss: 5.0500...  0.3426 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3694...  Training loss: 5.1719...  0.3400 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3695...  Training loss: 5.1435...  0.3409 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3696...  Training loss: 5.1217...  0.3397 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3697...  Training loss: 5.1910...  0.3399 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3698...  Training loss: 5.0629...  0.3411 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3699...  Training loss: 5.1955...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3700...  Training loss: 5.0671...  0.3399 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3701...  Training loss: 5.0753...  0.3414 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3702...  Training loss: 5.0661...  0.3431 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3703...  Training loss: 5.0890...  0.3442 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3704...  Training loss: 5.1200...  0.3451 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3705...  Training loss: 5.0829...  0.3434 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3706...  Training loss: 5.0222...  0.3402 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3707...  Training loss: 5.0789...  0.3418 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3708...  Training loss: 5.0530...  0.3409 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3709...  Training loss: 5.0718...  0.3441 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3710...  Training loss: 5.0891...  0.3431 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3711...  Training loss: 5.0504...  0.3432 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3712...  Training loss: 5.0293...  0.3430 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3713...  Training loss: 4.7564...  0.3410 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3714...  Training loss: 4.8735...  0.3408 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3715...  Training loss: 4.9919...  0.3438 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100...  Training Step: 3716...  Training loss: 5.1802...  0.3414 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3717...  Training loss: 5.0704...  0.3384 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3718...  Training loss: 5.1242...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3719...  Training loss: 5.0112...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3720...  Training loss: 5.0471...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3721...  Training loss: 5.1582...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3722...  Training loss: 5.1241...  0.3405 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3723...  Training loss: 5.1884...  0.3395 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3724...  Training loss: 5.1731...  0.3398 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3725...  Training loss: 5.0882...  0.3436 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3726...  Training loss: 5.0581...  0.3390 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3727...  Training loss: 5.1181...  0.3397 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3728...  Training loss: 4.9820...  0.3429 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3729...  Training loss: 5.0621...  0.3406 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3730...  Training loss: 5.2194...  0.3411 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3731...  Training loss: 5.1205...  0.3416 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3732...  Training loss: 5.0242...  0.3443 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3733...  Training loss: 5.1150...  0.3417 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3734...  Training loss: 5.1524...  0.3398 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3735...  Training loss: 5.1767...  0.3411 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3736...  Training loss: 5.0596...  0.3438 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3737...  Training loss: 4.9780...  0.3455 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3738...  Training loss: 5.1047...  0.3393 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3739...  Training loss: 4.9223...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3740...  Training loss: 4.9680...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3741...  Training loss: 4.9786...  0.3436 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3742...  Training loss: 5.0396...  0.3445 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3743...  Training loss: 5.0998...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3744...  Training loss: 5.0860...  0.3406 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3745...  Training loss: 4.9528...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3746...  Training loss: 4.9771...  0.3416 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3747...  Training loss: 5.0261...  0.3405 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3748...  Training loss: 4.9207...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3749...  Training loss: 5.1223...  0.3418 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3750...  Training loss: 5.0411...  0.3395 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3751...  Training loss: 5.0663...  0.3442 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3752...  Training loss: 4.8860...  0.3431 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3753...  Training loss: 4.9123...  0.3414 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3754...  Training loss: 4.9859...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3755...  Training loss: 4.9595...  0.3418 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3756...  Training loss: 4.9644...  0.3386 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3757...  Training loss: 5.0437...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3758...  Training loss: 5.0373...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3759...  Training loss: 5.0964...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3760...  Training loss: 5.2363...  0.3434 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3761...  Training loss: 5.2061...  0.3398 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3762...  Training loss: 5.1715...  0.3398 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3763...  Training loss: 5.1534...  0.3396 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3764...  Training loss: 5.1180...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3765...  Training loss: 5.2413...  0.3418 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3766...  Training loss: 5.1313...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3767...  Training loss: 5.0565...  0.3440 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3768...  Training loss: 5.1648...  0.3445 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3769...  Training loss: 5.2336...  0.3431 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3770...  Training loss: 5.2345...  0.3405 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3771...  Training loss: 5.1982...  0.3421 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3772...  Training loss: 4.9703...  0.3463 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3773...  Training loss: 5.2196...  0.3458 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3774...  Training loss: 5.2148...  0.3459 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3775...  Training loss: 5.1632...  0.3457 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3776...  Training loss: 5.1081...  0.3444 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3777...  Training loss: 5.0712...  0.3407 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3778...  Training loss: 5.0358...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3779...  Training loss: 5.0971...  0.3441 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3780...  Training loss: 5.0347...  0.3446 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3781...  Training loss: 5.0598...  0.3452 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3782...  Training loss: 5.0379...  0.3443 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3783...  Training loss: 5.2244...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3784...  Training loss: 5.1002...  0.3438 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3785...  Training loss: 5.1570...  0.3397 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3786...  Training loss: 5.2099...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3787...  Training loss: 5.1567...  0.3418 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3788...  Training loss: 4.9728...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3789...  Training loss: 4.9903...  0.3442 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3790...  Training loss: 4.9953...  0.3431 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3791...  Training loss: 4.9600...  0.3463 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3792...  Training loss: 5.0732...  0.3394 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3793...  Training loss: 5.0298...  0.3444 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3794...  Training loss: 5.0091...  0.3407 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3795...  Training loss: 4.9453...  0.3407 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3796...  Training loss: 4.9175...  0.3393 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3797...  Training loss: 5.0943...  0.3403 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3798...  Training loss: 4.9486...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3799...  Training loss: 4.9430...  0.3440 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3800...  Training loss: 5.0072...  0.3428 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3801...  Training loss: 5.2089...  0.3417 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3802...  Training loss: 5.1342...  0.3421 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3803...  Training loss: 5.0375...  0.3426 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3804...  Training loss: 5.0160...  0.3422 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3805...  Training loss: 4.9339...  0.3384 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3806...  Training loss: 4.9656...  0.3410 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3807...  Training loss: 5.0302...  0.3407 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3808...  Training loss: 5.0771...  0.3432 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3809...  Training loss: 5.1943...  0.3399 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3810...  Training loss: 5.0664...  0.3399 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3811...  Training loss: 5.0672...  0.3403 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3812...  Training loss: 5.0809...  0.3420 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100...  Training Step: 3813...  Training loss: 5.1326...  0.3414 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3814...  Training loss: 4.9067...  0.3391 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3815...  Training loss: 4.9721...  0.3427 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3816...  Training loss: 4.9145...  0.3420 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3817...  Training loss: 5.1332...  0.3442 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3818...  Training loss: 4.9779...  0.3417 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3819...  Training loss: 5.0642...  0.3436 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3820...  Training loss: 5.3647...  0.3410 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3821...  Training loss: 5.3572...  0.3404 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3822...  Training loss: 5.2027...  0.3437 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3823...  Training loss: 4.9666...  0.3388 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3824...  Training loss: 4.9291...  0.3412 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3825...  Training loss: 4.9341...  0.3393 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3826...  Training loss: 4.9992...  0.3394 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3827...  Training loss: 5.1367...  0.3438 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3828...  Training loss: 5.0631...  0.3405 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3829...  Training loss: 5.0601...  0.3417 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3830...  Training loss: 5.0008...  0.3447 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3831...  Training loss: 5.0406...  0.3408 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3832...  Training loss: 4.8690...  0.3386 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3833...  Training loss: 5.0602...  0.3423 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3834...  Training loss: 4.9718...  0.3407 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3835...  Training loss: 5.0783...  0.3394 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3836...  Training loss: 5.0284...  0.3396 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3837...  Training loss: 5.1112...  0.3438 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3838...  Training loss: 5.1421...  0.3403 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3839...  Training loss: 5.0370...  0.3409 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3840...  Training loss: 5.0105...  0.3413 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3841...  Training loss: 5.0383...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3842...  Training loss: 5.0946...  0.3435 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3843...  Training loss: 5.0991...  0.3444 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3844...  Training loss: 5.1216...  0.3440 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3845...  Training loss: 5.2537...  0.3413 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3846...  Training loss: 5.1832...  0.3397 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3847...  Training loss: 5.1188...  0.3402 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3848...  Training loss: 5.0775...  0.3406 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3849...  Training loss: 5.0576...  0.3388 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3850...  Training loss: 5.0369...  0.3396 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3851...  Training loss: 5.0090...  0.3437 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3852...  Training loss: 5.0454...  0.3441 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3853...  Training loss: 5.1376...  0.3422 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3854...  Training loss: 5.0866...  0.3439 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3855...  Training loss: 5.0850...  0.3437 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3856...  Training loss: 5.0239...  0.3430 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3857...  Training loss: 5.0821...  0.3400 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3858...  Training loss: 5.0321...  0.3434 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3859...  Training loss: 5.0129...  0.3405 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3860...  Training loss: 5.0416...  0.3428 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3861...  Training loss: 5.3934...  0.3406 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3862...  Training loss: 5.2575...  0.3429 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3863...  Training loss: 5.3068...  0.3433 sec/batch\n",
      "Epoch: 21/100...  Training Step: 3864...  Training loss: 5.1994...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3865...  Training loss: 5.3417...  0.3447 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3866...  Training loss: 5.3943...  0.3446 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3867...  Training loss: 5.1982...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3868...  Training loss: 5.1761...  0.3388 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3869...  Training loss: 5.1831...  0.3448 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3870...  Training loss: 5.0493...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3871...  Training loss: 5.2932...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3872...  Training loss: 5.1195...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3873...  Training loss: 5.1869...  0.3404 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3874...  Training loss: 5.0726...  0.3423 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3875...  Training loss: 5.0741...  0.3409 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3876...  Training loss: 5.0458...  0.3445 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3877...  Training loss: 5.0278...  0.3450 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3878...  Training loss: 5.1428...  0.3407 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3879...  Training loss: 5.1449...  0.3396 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3880...  Training loss: 5.1178...  0.3404 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3881...  Training loss: 5.1446...  0.3416 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3882...  Training loss: 5.0229...  0.3416 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3883...  Training loss: 5.1553...  0.3430 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3884...  Training loss: 5.0533...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3885...  Training loss: 5.0733...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3886...  Training loss: 5.0497...  0.3412 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3887...  Training loss: 5.0520...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3888...  Training loss: 5.1310...  0.3428 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3889...  Training loss: 5.0739...  0.3414 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3890...  Training loss: 5.0169...  0.3429 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3891...  Training loss: 5.0716...  0.3415 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3892...  Training loss: 5.0475...  0.3421 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3893...  Training loss: 5.0852...  0.3420 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3894...  Training loss: 5.0784...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3895...  Training loss: 5.0175...  0.3407 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3896...  Training loss: 4.9949...  0.3398 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3897...  Training loss: 4.7349...  0.3410 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3898...  Training loss: 4.8557...  0.3435 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3899...  Training loss: 4.9807...  0.3402 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3900...  Training loss: 5.1508...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3901...  Training loss: 5.0668...  0.3415 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3902...  Training loss: 5.1137...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3903...  Training loss: 5.0099...  0.3402 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3904...  Training loss: 5.0506...  0.3397 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3905...  Training loss: 5.1418...  0.3395 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3906...  Training loss: 5.0991...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3907...  Training loss: 5.1845...  0.3406 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3908...  Training loss: 5.1704...  0.3417 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3909...  Training loss: 5.0624...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100...  Training Step: 3910...  Training loss: 5.0322...  0.3426 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3911...  Training loss: 5.1156...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3912...  Training loss: 4.9703...  0.3434 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3913...  Training loss: 5.0474...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3914...  Training loss: 5.2002...  0.3409 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3915...  Training loss: 5.0963...  0.3427 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3916...  Training loss: 5.0172...  0.3402 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3917...  Training loss: 5.0925...  0.3426 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3918...  Training loss: 5.1253...  0.3444 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3919...  Training loss: 5.1369...  0.3438 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3920...  Training loss: 5.0461...  0.3401 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3921...  Training loss: 4.9735...  0.3403 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3922...  Training loss: 5.0899...  0.3407 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3923...  Training loss: 4.9166...  0.3417 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3924...  Training loss: 4.9449...  0.3387 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3925...  Training loss: 4.9419...  0.3397 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3926...  Training loss: 5.0039...  0.3397 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3927...  Training loss: 5.0707...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3928...  Training loss: 5.0716...  0.3414 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3929...  Training loss: 4.9390...  0.3410 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3930...  Training loss: 4.9736...  0.3421 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3931...  Training loss: 5.0003...  0.3398 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3932...  Training loss: 4.8887...  0.3395 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3933...  Training loss: 5.1185...  0.3436 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3934...  Training loss: 5.0307...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3935...  Training loss: 5.0579...  0.3438 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3936...  Training loss: 4.8728...  0.3401 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3937...  Training loss: 4.8897...  0.3416 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3938...  Training loss: 4.9624...  0.3446 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3939...  Training loss: 4.9294...  0.3428 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3940...  Training loss: 4.9474...  0.3450 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3941...  Training loss: 5.0325...  0.3394 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3942...  Training loss: 5.0238...  0.3424 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3943...  Training loss: 5.0612...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3944...  Training loss: 5.2208...  0.3405 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3945...  Training loss: 5.1681...  0.3406 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3946...  Training loss: 5.1474...  0.3405 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3947...  Training loss: 5.1291...  0.3446 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3948...  Training loss: 5.0811...  0.3442 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3949...  Training loss: 5.1964...  0.3390 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3950...  Training loss: 5.1087...  0.3423 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3951...  Training loss: 5.0547...  0.3450 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3952...  Training loss: 5.1454...  0.3412 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3953...  Training loss: 5.2142...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3954...  Training loss: 5.2184...  0.3434 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3955...  Training loss: 5.1504...  0.3421 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3956...  Training loss: 4.9341...  0.3428 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3957...  Training loss: 5.1824...  0.3425 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3958...  Training loss: 5.1919...  0.3418 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3959...  Training loss: 5.1255...  0.3410 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3960...  Training loss: 5.0763...  0.3386 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3961...  Training loss: 5.0353...  0.3423 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3962...  Training loss: 4.9897...  0.3422 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3963...  Training loss: 5.0600...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3964...  Training loss: 5.0059...  0.3416 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3965...  Training loss: 5.0277...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3966...  Training loss: 5.0162...  0.3398 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3967...  Training loss: 5.1693...  0.3406 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3968...  Training loss: 5.0453...  0.3427 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3969...  Training loss: 5.1133...  0.3420 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3970...  Training loss: 5.1598...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3971...  Training loss: 5.1255...  0.3421 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3972...  Training loss: 4.9380...  0.3399 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3973...  Training loss: 4.9325...  0.3403 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3974...  Training loss: 4.9643...  0.3434 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3975...  Training loss: 4.9461...  0.3415 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3976...  Training loss: 5.0422...  0.3415 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3977...  Training loss: 5.0122...  0.3442 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3978...  Training loss: 4.9869...  0.3403 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3979...  Training loss: 4.9383...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3980...  Training loss: 4.9226...  0.3402 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3981...  Training loss: 5.0860...  0.3449 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3982...  Training loss: 4.9300...  0.3414 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3983...  Training loss: 4.9191...  0.3403 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3984...  Training loss: 4.9766...  0.3413 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3985...  Training loss: 5.2031...  0.3397 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3986...  Training loss: 5.1150...  0.3409 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3987...  Training loss: 5.0277...  0.3403 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3988...  Training loss: 5.0172...  0.3432 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3989...  Training loss: 4.9182...  0.3401 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3990...  Training loss: 4.9548...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3991...  Training loss: 5.0171...  0.3424 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3992...  Training loss: 5.0616...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3993...  Training loss: 5.1573...  0.3435 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3994...  Training loss: 5.0443...  0.3427 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3995...  Training loss: 5.0504...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3996...  Training loss: 5.0702...  0.3425 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3997...  Training loss: 5.1013...  0.3407 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3998...  Training loss: 4.8903...  0.3404 sec/batch\n",
      "Epoch: 22/100...  Training Step: 3999...  Training loss: 4.9631...  0.3392 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4000...  Training loss: 4.8810...  0.3389 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4001...  Training loss: 5.1151...  0.3794 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4002...  Training loss: 4.9485...  0.3451 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4003...  Training loss: 5.0399...  0.3430 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4004...  Training loss: 5.3270...  0.3439 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4005...  Training loss: 5.3208...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4006...  Training loss: 5.1872...  0.3438 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100...  Training Step: 4007...  Training loss: 4.9618...  0.3433 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4008...  Training loss: 4.9374...  0.3434 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4009...  Training loss: 4.9172...  0.3404 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4010...  Training loss: 4.9810...  0.3399 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4011...  Training loss: 5.0941...  0.3397 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4012...  Training loss: 5.0396...  0.3416 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4013...  Training loss: 5.0564...  0.3419 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4014...  Training loss: 4.9973...  0.3413 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4015...  Training loss: 5.0226...  0.3388 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4016...  Training loss: 4.8414...  0.3421 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4017...  Training loss: 5.0353...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4018...  Training loss: 4.9714...  0.3390 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4019...  Training loss: 5.0484...  0.3440 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4020...  Training loss: 5.0033...  0.3441 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4021...  Training loss: 5.0934...  0.3411 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4022...  Training loss: 5.1209...  0.3432 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4023...  Training loss: 5.0160...  0.3414 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4024...  Training loss: 5.0006...  0.3400 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4025...  Training loss: 5.0152...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4026...  Training loss: 5.0715...  0.3401 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4027...  Training loss: 5.0848...  0.3406 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4028...  Training loss: 5.1073...  0.3417 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4029...  Training loss: 5.2584...  0.3402 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4030...  Training loss: 5.1685...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4031...  Training loss: 5.1158...  0.3393 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4032...  Training loss: 5.0554...  0.3408 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4033...  Training loss: 5.0386...  0.3409 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4034...  Training loss: 5.0090...  0.3418 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4035...  Training loss: 5.0032...  0.3388 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4036...  Training loss: 5.0371...  0.3431 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4037...  Training loss: 5.1170...  0.3445 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4038...  Training loss: 5.0566...  0.3445 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4039...  Training loss: 5.0539...  0.3425 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4040...  Training loss: 5.0058...  0.3445 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4041...  Training loss: 5.0752...  0.3460 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4042...  Training loss: 5.0183...  0.3468 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4043...  Training loss: 4.9849...  0.3438 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4044...  Training loss: 5.0193...  0.3457 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4045...  Training loss: 5.3711...  0.3428 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4046...  Training loss: 5.2477...  0.3420 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4047...  Training loss: 5.2756...  0.3445 sec/batch\n",
      "Epoch: 22/100...  Training Step: 4048...  Training loss: 5.1858...  0.3429 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4049...  Training loss: 5.3424...  0.3447 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4050...  Training loss: 5.4191...  0.3427 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4051...  Training loss: 5.1703...  0.3434 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4052...  Training loss: 5.1390...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4053...  Training loss: 5.1411...  0.3447 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4054...  Training loss: 5.0223...  0.3460 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4055...  Training loss: 5.2726...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4056...  Training loss: 5.0983...  0.3444 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4057...  Training loss: 5.1571...  0.3452 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4058...  Training loss: 5.0520...  0.3438 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4059...  Training loss: 5.0651...  0.3452 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4060...  Training loss: 5.0206...  0.3424 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4061...  Training loss: 5.0003...  0.3434 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4062...  Training loss: 5.1007...  0.3433 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4063...  Training loss: 5.1005...  0.3435 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4064...  Training loss: 5.0706...  0.3446 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4065...  Training loss: 5.1178...  0.3416 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4066...  Training loss: 4.9977...  0.3403 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4067...  Training loss: 5.1234...  0.3450 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4068...  Training loss: 5.0248...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4069...  Training loss: 5.0315...  0.3429 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4070...  Training loss: 5.0189...  0.3442 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4071...  Training loss: 5.0364...  0.3456 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4072...  Training loss: 5.0813...  0.3444 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4073...  Training loss: 5.0394...  0.3452 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4074...  Training loss: 4.9847...  0.3449 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4075...  Training loss: 5.0301...  0.3435 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4076...  Training loss: 5.0084...  0.3448 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4077...  Training loss: 5.0079...  0.3445 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4078...  Training loss: 5.0490...  0.3445 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4079...  Training loss: 4.9937...  0.3433 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4080...  Training loss: 4.9712...  0.3435 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4081...  Training loss: 4.7122...  0.3448 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4082...  Training loss: 4.8217...  0.3435 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4083...  Training loss: 4.9757...  0.3465 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4084...  Training loss: 5.1235...  0.3426 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4085...  Training loss: 5.0586...  0.3417 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4086...  Training loss: 5.0951...  0.3434 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4087...  Training loss: 4.9927...  0.3420 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4088...  Training loss: 5.0217...  0.3452 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4089...  Training loss: 5.1073...  0.3452 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4090...  Training loss: 5.0750...  0.3408 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4091...  Training loss: 5.1573...  0.3462 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4092...  Training loss: 5.1335...  0.3405 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4093...  Training loss: 5.0340...  0.3449 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4094...  Training loss: 5.0162...  0.3396 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4095...  Training loss: 5.0935...  0.3406 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4096...  Training loss: 4.9359...  0.3432 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4097...  Training loss: 5.0230...  0.3425 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4098...  Training loss: 5.1992...  0.3418 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4099...  Training loss: 5.0930...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4100...  Training loss: 5.0047...  0.3426 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4101...  Training loss: 5.0875...  0.3401 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4102...  Training loss: 5.1076...  0.3435 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4103...  Training loss: 5.1034...  0.3427 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/100...  Training Step: 4104...  Training loss: 5.0485...  0.3415 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4105...  Training loss: 4.9510...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4106...  Training loss: 5.0767...  0.3410 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4107...  Training loss: 4.8992...  0.3448 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4108...  Training loss: 4.9279...  0.3445 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4109...  Training loss: 4.9378...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4110...  Training loss: 5.0017...  0.3436 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4111...  Training loss: 5.0603...  0.3495 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4112...  Training loss: 5.0581...  0.3443 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4113...  Training loss: 4.9196...  0.3457 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4114...  Training loss: 4.9459...  0.3444 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4115...  Training loss: 4.9923...  0.3429 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4116...  Training loss: 4.8835...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4117...  Training loss: 5.0886...  0.3448 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4118...  Training loss: 5.0199...  0.3449 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4119...  Training loss: 5.0427...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4120...  Training loss: 4.8665...  0.3420 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4121...  Training loss: 4.8965...  0.3415 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4122...  Training loss: 4.9544...  0.3398 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4123...  Training loss: 4.9174...  0.3407 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4124...  Training loss: 4.9223...  0.3415 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4125...  Training loss: 5.0058...  0.3391 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4126...  Training loss: 4.9841...  0.3406 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4127...  Training loss: 5.0531...  0.3383 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4128...  Training loss: 5.1742...  0.3400 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4129...  Training loss: 5.1576...  0.3415 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4130...  Training loss: 5.1306...  0.3429 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4131...  Training loss: 5.0991...  0.3392 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4132...  Training loss: 5.0583...  0.3406 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4133...  Training loss: 5.1603...  0.3428 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4134...  Training loss: 5.0735...  0.3431 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4135...  Training loss: 5.0241...  0.3419 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4136...  Training loss: 5.1134...  0.3420 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4137...  Training loss: 5.1820...  0.3447 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4138...  Training loss: 5.1883...  0.3423 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4139...  Training loss: 5.1395...  0.3422 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4140...  Training loss: 4.9300...  0.3423 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4141...  Training loss: 5.1483...  0.3413 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4142...  Training loss: 5.1801...  0.3388 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4143...  Training loss: 5.1024...  0.3401 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4144...  Training loss: 5.0610...  0.3409 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4145...  Training loss: 5.0141...  0.3402 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4146...  Training loss: 4.9749...  0.3440 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4147...  Training loss: 5.0310...  0.3439 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4148...  Training loss: 4.9739...  0.3392 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4149...  Training loss: 4.9922...  0.3409 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4150...  Training loss: 4.9806...  0.3413 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4151...  Training loss: 5.1433...  0.3397 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4152...  Training loss: 5.0455...  0.3425 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4153...  Training loss: 5.1181...  0.3444 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4154...  Training loss: 5.1570...  0.3432 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4155...  Training loss: 5.1019...  0.3401 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4156...  Training loss: 4.9275...  0.3390 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4157...  Training loss: 4.9369...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4158...  Training loss: 4.9424...  0.3417 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4159...  Training loss: 4.9447...  0.3404 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4160...  Training loss: 5.0246...  0.3436 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4161...  Training loss: 4.9841...  0.3400 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4162...  Training loss: 4.9666...  0.3418 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4163...  Training loss: 4.9187...  0.3411 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4164...  Training loss: 4.9027...  0.3431 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4165...  Training loss: 5.0568...  0.3430 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4166...  Training loss: 4.9158...  0.3423 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4167...  Training loss: 4.9209...  0.3438 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4168...  Training loss: 4.9633...  0.3417 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4169...  Training loss: 5.1668...  0.3432 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4170...  Training loss: 5.0934...  0.3416 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4171...  Training loss: 5.0035...  0.3420 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4172...  Training loss: 4.9942...  0.3410 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4173...  Training loss: 4.9114...  0.3402 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4174...  Training loss: 4.9271...  0.3418 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4175...  Training loss: 4.9991...  0.3411 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4176...  Training loss: 5.0385...  0.3393 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4177...  Training loss: 5.1606...  0.3407 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4178...  Training loss: 5.0188...  0.3398 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4179...  Training loss: 5.0253...  0.3416 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4180...  Training loss: 5.0540...  0.3391 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4181...  Training loss: 5.0850...  0.3401 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4182...  Training loss: 4.8792...  0.3399 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4183...  Training loss: 4.9520...  0.3426 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4184...  Training loss: 4.8809...  0.3418 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4185...  Training loss: 5.0959...  0.3404 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4186...  Training loss: 4.9437...  0.3388 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4187...  Training loss: 5.0273...  0.3396 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4188...  Training loss: 5.3119...  0.3405 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4189...  Training loss: 5.2998...  0.3405 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4190...  Training loss: 5.1685...  0.3404 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4191...  Training loss: 4.9456...  0.3402 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4192...  Training loss: 4.9143...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4193...  Training loss: 4.8940...  0.3428 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4194...  Training loss: 4.9671...  0.3404 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4195...  Training loss: 5.0694...  0.3420 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4196...  Training loss: 5.0138...  0.3403 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4197...  Training loss: 5.0253...  0.3405 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4198...  Training loss: 4.9555...  0.3407 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4199...  Training loss: 5.0032...  0.3417 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4200...  Training loss: 4.8209...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/100...  Training Step: 4201...  Training loss: 5.0190...  0.3409 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4202...  Training loss: 4.9411...  0.3408 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4203...  Training loss: 5.0410...  0.3436 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4204...  Training loss: 4.9751...  0.3432 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4205...  Training loss: 5.0480...  0.3419 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4206...  Training loss: 5.0880...  0.3427 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4207...  Training loss: 4.9863...  0.3412 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4208...  Training loss: 4.9652...  0.3436 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4209...  Training loss: 5.0087...  0.3426 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4210...  Training loss: 5.0487...  0.3394 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4211...  Training loss: 5.0509...  0.3431 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4212...  Training loss: 5.0758...  0.3440 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4213...  Training loss: 5.2303...  0.3409 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4214...  Training loss: 5.1341...  0.3408 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4215...  Training loss: 5.0613...  0.3399 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4216...  Training loss: 5.0373...  0.3408 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4217...  Training loss: 5.0038...  0.3415 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4218...  Training loss: 4.9935...  0.3411 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4219...  Training loss: 4.9753...  0.3419 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4220...  Training loss: 5.0104...  0.3425 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4221...  Training loss: 5.0819...  0.3413 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4222...  Training loss: 5.0398...  0.3392 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4223...  Training loss: 5.0406...  0.3424 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4224...  Training loss: 4.9826...  0.3429 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4225...  Training loss: 5.0458...  0.3456 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4226...  Training loss: 5.0145...  0.3428 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4227...  Training loss: 4.9805...  0.3402 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4228...  Training loss: 5.0195...  0.3428 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4229...  Training loss: 5.3386...  0.3396 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4230...  Training loss: 5.2221...  0.3412 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4231...  Training loss: 5.2555...  0.3421 sec/batch\n",
      "Epoch: 23/100...  Training Step: 4232...  Training loss: 5.1664...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4233...  Training loss: 5.3591...  0.3408 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4234...  Training loss: 5.3794...  0.3397 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4235...  Training loss: 5.1361...  0.3415 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4236...  Training loss: 5.1020...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4237...  Training loss: 5.1120...  0.3442 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4238...  Training loss: 4.9999...  0.3410 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4239...  Training loss: 5.2229...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4240...  Training loss: 5.0745...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4241...  Training loss: 5.1309...  0.3404 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4242...  Training loss: 5.0148...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4243...  Training loss: 5.0189...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4244...  Training loss: 4.9823...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4245...  Training loss: 4.9719...  0.3435 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4246...  Training loss: 5.0687...  0.3448 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4247...  Training loss: 5.0662...  0.3395 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4248...  Training loss: 5.0345...  0.3392 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4249...  Training loss: 5.0979...  0.3391 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4250...  Training loss: 4.9658...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4251...  Training loss: 5.1137...  0.3415 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4252...  Training loss: 5.0025...  0.3412 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4253...  Training loss: 4.9895...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4254...  Training loss: 5.0072...  0.3413 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4255...  Training loss: 5.0004...  0.3409 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4256...  Training loss: 5.0437...  0.3388 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4257...  Training loss: 5.0360...  0.3422 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4258...  Training loss: 4.9787...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4259...  Training loss: 5.0161...  0.3410 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4260...  Training loss: 4.9973...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4261...  Training loss: 4.9941...  0.3407 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4262...  Training loss: 5.0222...  0.3439 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4263...  Training loss: 4.9725...  0.3444 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4264...  Training loss: 4.9375...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4265...  Training loss: 4.6931...  0.3445 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4266...  Training loss: 4.7936...  0.3416 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4267...  Training loss: 4.9362...  0.3453 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4268...  Training loss: 5.1016...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4269...  Training loss: 5.0333...  0.3387 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4270...  Training loss: 5.0579...  0.3407 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4271...  Training loss: 4.9668...  0.3429 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4272...  Training loss: 4.9989...  0.3410 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4273...  Training loss: 5.0823...  0.3420 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4274...  Training loss: 5.0440...  0.3436 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4275...  Training loss: 5.1197...  0.3401 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4276...  Training loss: 5.1070...  0.3403 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4277...  Training loss: 5.0091...  0.3435 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4278...  Training loss: 4.9781...  0.3395 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4279...  Training loss: 5.0711...  0.3451 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4280...  Training loss: 4.9231...  0.3400 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4281...  Training loss: 4.9962...  0.3448 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4282...  Training loss: 5.1581...  0.3400 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4283...  Training loss: 5.0646...  0.3407 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4284...  Training loss: 4.9886...  0.3405 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4285...  Training loss: 5.0585...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4286...  Training loss: 5.0959...  0.3421 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4287...  Training loss: 5.0927...  0.3446 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4288...  Training loss: 5.0060...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4289...  Training loss: 4.9210...  0.3388 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4290...  Training loss: 5.0670...  0.3393 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4291...  Training loss: 4.8900...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4292...  Training loss: 4.9134...  0.3417 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4293...  Training loss: 4.9122...  0.3391 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4294...  Training loss: 4.9861...  0.3428 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4295...  Training loss: 5.0392...  0.3434 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4296...  Training loss: 5.0298...  0.3402 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4297...  Training loss: 4.8936...  0.3428 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100...  Training Step: 4298...  Training loss: 4.9254...  0.3424 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4299...  Training loss: 4.9442...  0.3396 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4300...  Training loss: 4.8585...  0.3432 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4301...  Training loss: 5.0666...  0.3434 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4302...  Training loss: 4.9741...  0.3392 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4303...  Training loss: 5.0006...  0.3405 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4304...  Training loss: 4.8183...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4305...  Training loss: 4.8835...  0.3434 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4306...  Training loss: 4.9352...  0.3403 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4307...  Training loss: 4.9004...  0.3387 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4308...  Training loss: 4.9276...  0.3426 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4309...  Training loss: 5.0046...  0.3398 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4310...  Training loss: 4.9934...  0.3445 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4311...  Training loss: 5.0335...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4312...  Training loss: 5.1696...  0.3387 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4313...  Training loss: 5.1484...  0.3412 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4314...  Training loss: 5.1169...  0.3417 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4315...  Training loss: 5.0947...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4316...  Training loss: 5.0454...  0.3393 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4317...  Training loss: 5.1505...  0.3424 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4318...  Training loss: 5.0683...  0.3421 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4319...  Training loss: 5.0054...  0.3407 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4320...  Training loss: 5.0905...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4321...  Training loss: 5.1496...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4322...  Training loss: 5.1566...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4323...  Training loss: 5.1357...  0.3423 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4324...  Training loss: 4.8962...  0.3426 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4325...  Training loss: 5.1307...  0.3389 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4326...  Training loss: 5.1352...  0.3402 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4327...  Training loss: 5.0661...  0.3443 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4328...  Training loss: 5.0252...  0.3403 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4329...  Training loss: 4.9932...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4330...  Training loss: 4.9468...  0.3425 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4331...  Training loss: 5.0058...  0.3396 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4332...  Training loss: 4.9626...  0.3397 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4333...  Training loss: 4.9884...  0.3426 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4334...  Training loss: 4.9646...  0.3428 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4335...  Training loss: 5.1032...  0.3386 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4336...  Training loss: 5.0060...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4337...  Training loss: 5.0937...  0.3391 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4338...  Training loss: 5.1312...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4339...  Training loss: 5.0967...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4340...  Training loss: 4.9094...  0.3417 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4341...  Training loss: 4.9324...  0.3412 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4342...  Training loss: 4.9440...  0.3395 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4343...  Training loss: 4.9146...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4344...  Training loss: 5.0060...  0.3416 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4345...  Training loss: 4.9743...  0.3429 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4346...  Training loss: 4.9514...  0.3405 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4347...  Training loss: 4.9046...  0.3445 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4348...  Training loss: 4.8779...  0.3402 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4349...  Training loss: 5.0346...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4350...  Training loss: 4.8963...  0.3429 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4351...  Training loss: 4.8926...  0.3425 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4352...  Training loss: 4.9382...  0.3454 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4353...  Training loss: 5.1311...  0.3408 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4354...  Training loss: 5.0371...  0.3408 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4355...  Training loss: 4.9680...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4356...  Training loss: 4.9756...  0.3422 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4357...  Training loss: 4.8804...  0.3413 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4358...  Training loss: 4.9174...  0.3432 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4359...  Training loss: 4.9685...  0.3434 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4360...  Training loss: 5.0056...  0.3412 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4361...  Training loss: 5.1131...  0.3419 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4362...  Training loss: 4.9739...  0.3426 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4363...  Training loss: 4.9878...  0.3405 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4364...  Training loss: 5.0177...  0.3422 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4365...  Training loss: 5.0620...  0.3402 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4366...  Training loss: 4.8629...  0.3402 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4367...  Training loss: 4.9285...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4368...  Training loss: 4.8613...  0.3448 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4369...  Training loss: 5.0764...  0.3432 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4370...  Training loss: 4.9249...  0.3431 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4371...  Training loss: 5.0158...  0.3390 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4372...  Training loss: 5.2950...  0.3441 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4373...  Training loss: 5.2764...  0.3417 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4374...  Training loss: 5.1298...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4375...  Training loss: 4.9270...  0.3394 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4376...  Training loss: 4.9045...  0.3397 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4377...  Training loss: 4.8945...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4378...  Training loss: 4.9670...  0.3400 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4379...  Training loss: 5.0635...  0.3427 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4380...  Training loss: 5.0050...  0.3417 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4381...  Training loss: 5.0001...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4382...  Training loss: 4.9410...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4383...  Training loss: 4.9903...  0.3405 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4384...  Training loss: 4.8001...  0.3410 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4385...  Training loss: 4.9927...  0.3434 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4386...  Training loss: 4.9141...  0.3401 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4387...  Training loss: 5.0214...  0.3429 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4388...  Training loss: 4.9669...  0.3441 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4389...  Training loss: 5.0359...  0.3409 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4390...  Training loss: 5.0605...  0.3420 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4391...  Training loss: 4.9481...  0.3416 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4392...  Training loss: 4.9423...  0.3399 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4393...  Training loss: 4.9603...  0.3421 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4394...  Training loss: 5.0247...  0.3418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100...  Training Step: 4395...  Training loss: 5.0222...  0.3401 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4396...  Training loss: 5.0699...  0.3415 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4397...  Training loss: 5.2122...  0.3424 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4398...  Training loss: 5.1145...  0.3392 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4399...  Training loss: 5.0576...  0.3393 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4400...  Training loss: 5.0171...  0.3407 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4401...  Training loss: 4.9973...  0.3409 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4402...  Training loss: 4.9837...  0.3400 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4403...  Training loss: 4.9540...  0.3413 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4404...  Training loss: 4.9709...  0.3437 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4405...  Training loss: 5.0496...  0.3418 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4406...  Training loss: 5.0136...  0.3386 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4407...  Training loss: 5.0196...  0.3408 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4408...  Training loss: 4.9674...  0.3401 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4409...  Training loss: 5.0361...  0.3408 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4410...  Training loss: 4.9917...  0.3414 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4411...  Training loss: 4.9720...  0.3406 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4412...  Training loss: 4.9750...  0.3415 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4413...  Training loss: 5.3232...  0.3440 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4414...  Training loss: 5.1949...  0.3429 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4415...  Training loss: 5.2312...  0.3411 sec/batch\n",
      "Epoch: 24/100...  Training Step: 4416...  Training loss: 5.1183...  0.3440 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4417...  Training loss: 5.2930...  0.3421 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4418...  Training loss: 5.3142...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4419...  Training loss: 5.0987...  0.3412 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4420...  Training loss: 5.0777...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4421...  Training loss: 5.0735...  0.3398 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4422...  Training loss: 4.9717...  0.3410 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4423...  Training loss: 5.1743...  0.3403 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4424...  Training loss: 5.0313...  0.3397 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4425...  Training loss: 5.0924...  0.3416 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4426...  Training loss: 4.9932...  0.3407 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4427...  Training loss: 4.9788...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4428...  Training loss: 4.9421...  0.3392 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4429...  Training loss: 4.9352...  0.3407 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4430...  Training loss: 5.0371...  0.3391 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4431...  Training loss: 5.0372...  0.3404 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4432...  Training loss: 5.0154...  0.3405 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4433...  Training loss: 5.0667...  0.3428 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4434...  Training loss: 4.9443...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4435...  Training loss: 5.0874...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4436...  Training loss: 4.9781...  0.3406 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4437...  Training loss: 4.9720...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4438...  Training loss: 4.9616...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4439...  Training loss: 4.9769...  0.3419 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4440...  Training loss: 5.0219...  0.3406 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4441...  Training loss: 4.9857...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4442...  Training loss: 4.9479...  0.3416 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4443...  Training loss: 4.9827...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4444...  Training loss: 4.9638...  0.3397 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4445...  Training loss: 4.9851...  0.3419 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4446...  Training loss: 5.0049...  0.3406 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4447...  Training loss: 4.9696...  0.3405 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4448...  Training loss: 4.9185...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4449...  Training loss: 4.6719...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4450...  Training loss: 4.7795...  0.3407 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4451...  Training loss: 4.9216...  0.3398 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4452...  Training loss: 5.1052...  0.3394 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4453...  Training loss: 5.0158...  0.3424 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4454...  Training loss: 5.0613...  0.3391 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4455...  Training loss: 4.9578...  0.3405 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4456...  Training loss: 4.9890...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4457...  Training loss: 5.0550...  0.3432 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4458...  Training loss: 5.0406...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4459...  Training loss: 5.1013...  0.3410 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4460...  Training loss: 5.0879...  0.3409 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4461...  Training loss: 4.9886...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4462...  Training loss: 4.9553...  0.3429 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4463...  Training loss: 5.0502...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4464...  Training loss: 4.9042...  0.3435 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4465...  Training loss: 4.9839...  0.3431 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4466...  Training loss: 5.1347...  0.3427 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4467...  Training loss: 5.0390...  0.3441 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4468...  Training loss: 4.9622...  0.3428 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4469...  Training loss: 5.0354...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4470...  Training loss: 5.0719...  0.3430 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4471...  Training loss: 5.0875...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4472...  Training loss: 4.9900...  0.3440 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4473...  Training loss: 4.9257...  0.3398 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4474...  Training loss: 5.0435...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4475...  Training loss: 4.8626...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4476...  Training loss: 4.8895...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4477...  Training loss: 4.8936...  0.3395 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4478...  Training loss: 4.9621...  0.3429 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4479...  Training loss: 5.0113...  0.3438 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4480...  Training loss: 5.0152...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4481...  Training loss: 4.9019...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4482...  Training loss: 4.9083...  0.3399 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4483...  Training loss: 4.9416...  0.3434 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4484...  Training loss: 4.8317...  0.3433 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4485...  Training loss: 5.0362...  0.3458 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4486...  Training loss: 4.9560...  0.3428 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4487...  Training loss: 4.9912...  0.3435 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4488...  Training loss: 4.8106...  0.3432 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4489...  Training loss: 4.8494...  0.3403 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4490...  Training loss: 4.9040...  0.3435 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4491...  Training loss: 4.8784...  0.3429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/100...  Training Step: 4492...  Training loss: 4.9022...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4493...  Training loss: 4.9941...  0.3414 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4494...  Training loss: 4.9561...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4495...  Training loss: 5.0185...  0.3399 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4496...  Training loss: 5.1521...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4497...  Training loss: 5.1091...  0.3420 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4498...  Training loss: 5.0820...  0.3423 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4499...  Training loss: 5.0741...  0.3403 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4500...  Training loss: 5.0443...  0.3395 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4501...  Training loss: 5.1648...  0.3407 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4502...  Training loss: 5.0637...  0.3430 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4503...  Training loss: 4.9918...  0.3439 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4504...  Training loss: 5.0917...  0.3409 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4505...  Training loss: 5.1753...  0.3394 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4506...  Training loss: 5.1809...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4507...  Training loss: 5.1116...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4508...  Training loss: 4.8902...  0.3448 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4509...  Training loss: 5.1013...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4510...  Training loss: 5.1359...  0.3430 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4511...  Training loss: 5.0857...  0.3414 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4512...  Training loss: 5.0134...  0.3397 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4513...  Training loss: 4.9786...  0.3427 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4514...  Training loss: 4.9439...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4515...  Training loss: 5.0031...  0.3448 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4516...  Training loss: 4.9549...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4517...  Training loss: 4.9921...  0.3397 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4518...  Training loss: 4.9795...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4519...  Training loss: 5.1155...  0.3403 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4520...  Training loss: 4.9848...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4521...  Training loss: 5.0437...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4522...  Training loss: 5.1021...  0.3432 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4523...  Training loss: 5.0513...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4524...  Training loss: 4.8908...  0.3409 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4525...  Training loss: 4.8977...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4526...  Training loss: 4.9190...  0.3434 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4527...  Training loss: 4.8999...  0.3436 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4528...  Training loss: 4.9811...  0.3429 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4529...  Training loss: 4.9518...  0.3415 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4530...  Training loss: 4.9338...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4531...  Training loss: 4.8796...  0.3412 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4532...  Training loss: 4.8790...  0.3424 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4533...  Training loss: 5.0236...  0.3443 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4534...  Training loss: 4.8669...  0.3394 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4535...  Training loss: 4.8599...  0.3410 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4536...  Training loss: 4.9255...  0.3426 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4537...  Training loss: 5.1175...  0.3424 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4538...  Training loss: 5.0415...  0.3423 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4539...  Training loss: 4.9480...  0.3427 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4540...  Training loss: 4.9434...  0.3388 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4541...  Training loss: 4.8651...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4542...  Training loss: 4.9002...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4543...  Training loss: 4.9506...  0.3409 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4544...  Training loss: 4.9876...  0.3432 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4545...  Training loss: 5.0979...  0.3419 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4546...  Training loss: 4.9709...  0.3449 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4547...  Training loss: 4.9778...  0.3402 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4548...  Training loss: 4.9819...  0.3418 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4549...  Training loss: 5.0472...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4550...  Training loss: 4.8451...  0.3427 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4551...  Training loss: 4.9162...  0.3433 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4552...  Training loss: 4.8418...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4553...  Training loss: 5.0616...  0.3414 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4554...  Training loss: 4.8987...  0.3414 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4555...  Training loss: 4.9927...  0.3408 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4556...  Training loss: 5.2691...  0.3425 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4557...  Training loss: 5.2491...  0.3410 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4558...  Training loss: 5.1332...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4559...  Training loss: 4.9144...  0.3407 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4560...  Training loss: 4.8788...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4561...  Training loss: 4.8763...  0.3400 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4562...  Training loss: 4.9434...  0.3412 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4563...  Training loss: 5.0327...  0.3394 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4564...  Training loss: 4.9919...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4565...  Training loss: 4.9971...  0.3410 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4566...  Training loss: 4.9168...  0.3433 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4567...  Training loss: 4.9942...  0.3431 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4568...  Training loss: 4.8308...  0.3419 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4569...  Training loss: 4.9863...  0.3387 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4570...  Training loss: 4.9186...  0.3424 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4571...  Training loss: 4.9972...  0.3411 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4572...  Training loss: 4.9537...  0.3418 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4573...  Training loss: 5.0208...  0.3431 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4574...  Training loss: 5.0635...  0.3428 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4575...  Training loss: 4.9380...  0.3451 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4576...  Training loss: 4.9247...  0.3405 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4577...  Training loss: 4.9515...  0.3423 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4578...  Training loss: 4.9912...  0.3421 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4579...  Training loss: 5.0128...  0.3414 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4580...  Training loss: 5.0391...  0.3446 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4581...  Training loss: 5.1694...  0.3442 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4582...  Training loss: 5.0881...  0.3426 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4583...  Training loss: 5.0255...  0.3421 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4584...  Training loss: 4.9862...  0.3421 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4585...  Training loss: 4.9797...  0.3441 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4586...  Training loss: 4.9716...  0.3413 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4587...  Training loss: 4.9678...  0.3405 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4588...  Training loss: 4.9812...  0.3425 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/100...  Training Step: 4589...  Training loss: 5.0295...  0.3398 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4590...  Training loss: 4.9926...  0.3417 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4591...  Training loss: 4.9745...  0.3404 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4592...  Training loss: 4.9321...  0.3427 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4593...  Training loss: 5.0026...  0.3422 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4594...  Training loss: 4.9726...  0.3421 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4595...  Training loss: 4.9463...  0.3406 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4596...  Training loss: 4.9698...  0.3401 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4597...  Training loss: 5.3022...  0.3419 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4598...  Training loss: 5.1800...  0.3416 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4599...  Training loss: 5.1851...  0.3416 sec/batch\n",
      "Epoch: 25/100...  Training Step: 4600...  Training loss: 5.1116...  0.3410 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4601...  Training loss: 5.2609...  0.3434 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4602...  Training loss: 5.2826...  0.3458 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4603...  Training loss: 5.0682...  0.3431 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4604...  Training loss: 5.0718...  0.3403 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4605...  Training loss: 5.0572...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4606...  Training loss: 4.9465...  0.3407 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4607...  Training loss: 5.1516...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4608...  Training loss: 5.0124...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4609...  Training loss: 5.0466...  0.3421 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4610...  Training loss: 4.9694...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4611...  Training loss: 4.9601...  0.3429 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4612...  Training loss: 4.9238...  0.3433 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4613...  Training loss: 4.9086...  0.3426 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4614...  Training loss: 5.0076...  0.3436 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4615...  Training loss: 5.0070...  0.3421 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4616...  Training loss: 4.9567...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4617...  Training loss: 5.0527...  0.3418 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4618...  Training loss: 4.9388...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4619...  Training loss: 5.0497...  0.3402 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4620...  Training loss: 4.9526...  0.3411 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4621...  Training loss: 4.9585...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4622...  Training loss: 4.9363...  0.3440 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4623...  Training loss: 4.9384...  0.3440 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4624...  Training loss: 4.9834...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4625...  Training loss: 4.9817...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4626...  Training loss: 4.9145...  0.3436 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4627...  Training loss: 4.9219...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4628...  Training loss: 4.9379...  0.3437 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4629...  Training loss: 4.9631...  0.3403 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4630...  Training loss: 4.9866...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4631...  Training loss: 4.9518...  0.3416 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4632...  Training loss: 4.8950...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4633...  Training loss: 4.6389...  0.3397 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4634...  Training loss: 4.7481...  0.3418 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4635...  Training loss: 4.8918...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4636...  Training loss: 5.0576...  0.3390 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4637...  Training loss: 4.9920...  0.3430 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4638...  Training loss: 5.0373...  0.3445 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4639...  Training loss: 4.9426...  0.3427 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4640...  Training loss: 4.9796...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4641...  Training loss: 5.0549...  0.3397 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4642...  Training loss: 5.0279...  0.3437 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4643...  Training loss: 5.1012...  0.3453 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4644...  Training loss: 5.0527...  0.3396 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4645...  Training loss: 4.9714...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4646...  Training loss: 4.9344...  0.3395 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4647...  Training loss: 5.0275...  0.3402 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4648...  Training loss: 4.8704...  0.3403 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4649...  Training loss: 4.9491...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4650...  Training loss: 5.1232...  0.3411 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4651...  Training loss: 5.0378...  0.3392 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4652...  Training loss: 4.9652...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4653...  Training loss: 5.0367...  0.3392 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4654...  Training loss: 5.0613...  0.3420 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4655...  Training loss: 5.0596...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4656...  Training loss: 4.9714...  0.3417 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4657...  Training loss: 4.9012...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4658...  Training loss: 5.0244...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4659...  Training loss: 4.8583...  0.3398 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4660...  Training loss: 4.8942...  0.3403 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4661...  Training loss: 4.9004...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4662...  Training loss: 4.9543...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4663...  Training loss: 5.0007...  0.3417 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4664...  Training loss: 4.9912...  0.3393 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4665...  Training loss: 4.8790...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4666...  Training loss: 4.9049...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4667...  Training loss: 4.9357...  0.3432 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4668...  Training loss: 4.8207...  0.3433 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4669...  Training loss: 4.9951...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4670...  Training loss: 4.9346...  0.3416 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4671...  Training loss: 4.9682...  0.3430 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4672...  Training loss: 4.7997...  0.3390 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4673...  Training loss: 4.8358...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4674...  Training loss: 4.8885...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4675...  Training loss: 4.8553...  0.3417 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4676...  Training loss: 4.8696...  0.3421 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4677...  Training loss: 4.9451...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4678...  Training loss: 4.9304...  0.3424 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4679...  Training loss: 5.0023...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4680...  Training loss: 5.1293...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4681...  Training loss: 5.1015...  0.3418 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4682...  Training loss: 5.0561...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4683...  Training loss: 5.0199...  0.3390 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4684...  Training loss: 5.0021...  0.3428 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4685...  Training loss: 5.1062...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/100...  Training Step: 4686...  Training loss: 5.0271...  0.3396 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4687...  Training loss: 4.9740...  0.3391 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4688...  Training loss: 5.0542...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4689...  Training loss: 5.1435...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4690...  Training loss: 5.1532...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4691...  Training loss: 5.0820...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4692...  Training loss: 4.8464...  0.3386 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4693...  Training loss: 5.0591...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4694...  Training loss: 5.0812...  0.3418 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4695...  Training loss: 5.0226...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4696...  Training loss: 4.9625...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4697...  Training loss: 4.9567...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4698...  Training loss: 4.9124...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4699...  Training loss: 4.9589...  0.3401 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4700...  Training loss: 4.9189...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4701...  Training loss: 4.9393...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4702...  Training loss: 4.9242...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4703...  Training loss: 5.0709...  0.3407 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4704...  Training loss: 4.9441...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4705...  Training loss: 5.0113...  0.3417 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4706...  Training loss: 5.0574...  0.3414 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4707...  Training loss: 5.0161...  0.3395 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4708...  Training loss: 4.8520...  0.3396 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4709...  Training loss: 4.8605...  0.3420 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4710...  Training loss: 4.8880...  0.3383 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4711...  Training loss: 4.8637...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4712...  Training loss: 4.9653...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4713...  Training loss: 4.9305...  0.3420 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4714...  Training loss: 4.9159...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4715...  Training loss: 4.8695...  0.3416 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4716...  Training loss: 4.8431...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4717...  Training loss: 5.0021...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4718...  Training loss: 4.8493...  0.3402 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4719...  Training loss: 4.8453...  0.3401 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4720...  Training loss: 4.9140...  0.3418 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4721...  Training loss: 5.1066...  0.3439 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4722...  Training loss: 5.0031...  0.3411 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4723...  Training loss: 4.9364...  0.3398 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4724...  Training loss: 4.9262...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4725...  Training loss: 4.8454...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4726...  Training loss: 4.8730...  0.3397 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4727...  Training loss: 4.9207...  0.3414 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4728...  Training loss: 4.9585...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4729...  Training loss: 5.0625...  0.3414 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4730...  Training loss: 4.9479...  0.3398 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4731...  Training loss: 4.9564...  0.3412 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4732...  Training loss: 4.9698...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4733...  Training loss: 5.0076...  0.3412 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4734...  Training loss: 4.8305...  0.3410 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4735...  Training loss: 4.8984...  0.3404 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4736...  Training loss: 4.8213...  0.3416 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4737...  Training loss: 5.0346...  0.3399 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4738...  Training loss: 4.8708...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4739...  Training loss: 4.9757...  0.3436 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4740...  Training loss: 5.2333...  0.3393 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4741...  Training loss: 5.2184...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4742...  Training loss: 5.1156...  0.3434 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4743...  Training loss: 4.8966...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4744...  Training loss: 4.8762...  0.3415 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4745...  Training loss: 4.8773...  0.3405 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4746...  Training loss: 4.9429...  0.3419 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4747...  Training loss: 5.0205...  0.3415 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4748...  Training loss: 4.9774...  0.3419 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4749...  Training loss: 4.9877...  0.3407 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4750...  Training loss: 4.9010...  0.3413 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4751...  Training loss: 4.9677...  0.3428 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4752...  Training loss: 4.7717...  0.3430 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4753...  Training loss: 4.9620...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4754...  Training loss: 4.8821...  0.3431 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4755...  Training loss: 4.9814...  0.3401 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4756...  Training loss: 4.9413...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4757...  Training loss: 4.9881...  0.3439 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4758...  Training loss: 5.0125...  0.3409 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4759...  Training loss: 4.9061...  0.3397 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4760...  Training loss: 4.9009...  0.3414 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4761...  Training loss: 4.9217...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4762...  Training loss: 4.9870...  0.3390 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4763...  Training loss: 4.9891...  0.3440 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4764...  Training loss: 4.9770...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4765...  Training loss: 5.1265...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4766...  Training loss: 5.0486...  0.3425 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4767...  Training loss: 4.9946...  0.3420 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4768...  Training loss: 4.9690...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4769...  Training loss: 4.9531...  0.3406 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4770...  Training loss: 4.9537...  0.3411 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4771...  Training loss: 4.9392...  0.3423 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4772...  Training loss: 4.9446...  0.3441 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4773...  Training loss: 5.0235...  0.3452 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4774...  Training loss: 4.9530...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4775...  Training loss: 4.9638...  0.3408 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4776...  Training loss: 4.9117...  0.3394 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4777...  Training loss: 4.9689...  0.3433 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4778...  Training loss: 4.9479...  0.3436 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4779...  Training loss: 4.9220...  0.3417 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4780...  Training loss: 4.9237...  0.3422 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4781...  Training loss: 5.2596...  0.3400 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4782...  Training loss: 5.1423...  0.3422 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/100...  Training Step: 4783...  Training loss: 5.1904...  0.3403 sec/batch\n",
      "Epoch: 26/100...  Training Step: 4784...  Training loss: 5.1054...  0.3420 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4785...  Training loss: 5.2492...  0.3429 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4786...  Training loss: 5.2465...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4787...  Training loss: 5.0538...  0.3411 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4788...  Training loss: 5.0284...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4789...  Training loss: 5.0325...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4790...  Training loss: 4.9351...  0.3386 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4791...  Training loss: 5.1236...  0.3441 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4792...  Training loss: 4.9976...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4793...  Training loss: 5.0249...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4794...  Training loss: 4.9412...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4795...  Training loss: 4.9213...  0.3418 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4796...  Training loss: 4.8772...  0.3406 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4797...  Training loss: 4.8792...  0.3411 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4798...  Training loss: 4.9754...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4799...  Training loss: 4.9683...  0.3406 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4800...  Training loss: 4.9428...  0.3425 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4801...  Training loss: 5.0214...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4802...  Training loss: 4.8940...  0.3437 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4803...  Training loss: 5.0151...  0.3434 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4804...  Training loss: 4.9526...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4805...  Training loss: 4.9301...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4806...  Training loss: 4.9181...  0.3417 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4807...  Training loss: 4.9129...  0.3427 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4808...  Training loss: 4.9539...  0.3406 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4809...  Training loss: 4.9592...  0.3406 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4810...  Training loss: 4.8901...  0.3417 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4811...  Training loss: 4.8966...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4812...  Training loss: 4.9112...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4813...  Training loss: 4.9399...  0.3434 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4814...  Training loss: 4.9619...  0.3399 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4815...  Training loss: 4.9496...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4816...  Training loss: 4.9036...  0.3419 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4817...  Training loss: 4.6288...  0.3440 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4818...  Training loss: 4.7438...  0.3394 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4819...  Training loss: 4.8761...  0.3411 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4820...  Training loss: 5.0545...  0.3444 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4821...  Training loss: 4.9526...  0.3399 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4822...  Training loss: 5.0030...  0.3451 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4823...  Training loss: 4.9164...  0.3420 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4824...  Training loss: 4.9539...  0.3441 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4825...  Training loss: 5.0351...  0.3392 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4826...  Training loss: 5.0094...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4827...  Training loss: 5.0751...  0.3406 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4828...  Training loss: 5.0429...  0.3436 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4829...  Training loss: 4.9510...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4830...  Training loss: 4.9153...  0.3431 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4831...  Training loss: 4.9962...  0.3426 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4832...  Training loss: 4.8553...  0.3443 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4833...  Training loss: 4.9230...  0.3438 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4834...  Training loss: 5.0896...  0.3402 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4835...  Training loss: 4.9967...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4836...  Training loss: 4.9132...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4837...  Training loss: 4.9969...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4838...  Training loss: 5.0365...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4839...  Training loss: 5.0361...  0.3444 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4840...  Training loss: 4.9474...  0.3419 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4841...  Training loss: 4.8663...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4842...  Training loss: 4.9882...  0.3458 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4843...  Training loss: 4.8422...  0.3425 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4844...  Training loss: 4.8664...  0.3426 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4845...  Training loss: 4.8888...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4846...  Training loss: 4.9441...  0.3434 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4847...  Training loss: 4.9976...  0.3409 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4848...  Training loss: 4.9825...  0.3440 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4849...  Training loss: 4.8436...  0.3387 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4850...  Training loss: 4.8747...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4851...  Training loss: 4.8978...  0.3402 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4852...  Training loss: 4.8001...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4853...  Training loss: 4.9829...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4854...  Training loss: 4.9076...  0.3438 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4855...  Training loss: 4.9215...  0.3422 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4856...  Training loss: 4.7621...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4857...  Training loss: 4.8053...  0.3391 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4858...  Training loss: 4.8601...  0.3425 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4859...  Training loss: 4.8450...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4860...  Training loss: 4.8526...  0.3438 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4861...  Training loss: 4.9222...  0.3426 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4862...  Training loss: 4.9061...  0.3382 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4863...  Training loss: 4.9774...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4864...  Training loss: 5.1045...  0.3409 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4865...  Training loss: 5.0429...  0.3395 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4866...  Training loss: 5.0507...  0.3418 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4867...  Training loss: 4.9968...  0.3428 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4868...  Training loss: 4.9828...  0.3427 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4869...  Training loss: 5.0610...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4870...  Training loss: 4.9723...  0.3409 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4871...  Training loss: 4.9435...  0.3392 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4872...  Training loss: 4.9952...  0.3386 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4873...  Training loss: 5.0890...  0.3437 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4874...  Training loss: 5.1154...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4875...  Training loss: 5.0403...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4876...  Training loss: 4.8373...  0.3404 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4877...  Training loss: 5.0226...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4878...  Training loss: 5.0339...  0.3391 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4879...  Training loss: 4.9904...  0.3391 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/100...  Training Step: 4880...  Training loss: 4.9230...  0.3405 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4881...  Training loss: 4.8989...  0.3404 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4882...  Training loss: 4.8788...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4883...  Training loss: 4.9118...  0.3405 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4884...  Training loss: 4.8775...  0.3411 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4885...  Training loss: 4.8972...  0.3405 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4886...  Training loss: 4.8996...  0.3421 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4887...  Training loss: 5.0495...  0.3394 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4888...  Training loss: 4.9459...  0.3385 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4889...  Training loss: 4.9942...  0.3396 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4890...  Training loss: 5.0377...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4891...  Training loss: 4.9938...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4892...  Training loss: 4.8282...  0.3431 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4893...  Training loss: 4.8401...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4894...  Training loss: 4.8612...  0.3425 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4895...  Training loss: 4.8303...  0.3433 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4896...  Training loss: 4.9251...  0.3438 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4897...  Training loss: 4.9046...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4898...  Training loss: 4.8639...  0.3436 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4899...  Training loss: 4.8288...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4900...  Training loss: 4.8220...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4901...  Training loss: 4.9852...  0.3435 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4902...  Training loss: 4.8447...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4903...  Training loss: 4.8487...  0.3396 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4904...  Training loss: 4.8965...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4905...  Training loss: 5.0727...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4906...  Training loss: 4.9739...  0.3410 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4907...  Training loss: 4.9021...  0.3428 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4908...  Training loss: 4.8842...  0.3405 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4909...  Training loss: 4.8402...  0.3395 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4910...  Training loss: 4.8704...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4911...  Training loss: 4.9083...  0.3402 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4912...  Training loss: 4.9225...  0.3451 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4913...  Training loss: 5.0228...  0.3404 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4914...  Training loss: 4.9346...  0.3402 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4915...  Training loss: 4.9349...  0.3403 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4916...  Training loss: 4.9622...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4917...  Training loss: 5.0025...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4918...  Training loss: 4.7823...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4919...  Training loss: 4.8600...  0.3439 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4920...  Training loss: 4.8120...  0.3425 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4921...  Training loss: 5.0030...  0.3407 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4922...  Training loss: 4.8753...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4923...  Training loss: 4.9518...  0.3421 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4924...  Training loss: 5.2024...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4925...  Training loss: 5.1848...  0.3392 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4926...  Training loss: 5.0759...  0.3428 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4927...  Training loss: 4.8753...  0.3396 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4928...  Training loss: 4.8466...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4929...  Training loss: 4.8436...  0.3432 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4930...  Training loss: 4.8991...  0.3427 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4931...  Training loss: 4.9840...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4932...  Training loss: 4.9486...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4933...  Training loss: 4.9629...  0.3420 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4934...  Training loss: 4.8809...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4935...  Training loss: 4.9380...  0.3399 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4936...  Training loss: 4.7789...  0.3388 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4937...  Training loss: 4.9298...  0.3428 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4938...  Training loss: 4.8743...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4939...  Training loss: 4.9488...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4940...  Training loss: 4.9190...  0.3421 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4941...  Training loss: 4.9787...  0.3396 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4942...  Training loss: 4.9817...  0.3409 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4943...  Training loss: 4.8823...  0.3431 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4944...  Training loss: 4.8799...  0.3429 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4945...  Training loss: 4.9095...  0.3401 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4946...  Training loss: 4.9545...  0.3417 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4947...  Training loss: 4.9612...  0.3412 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4948...  Training loss: 4.9712...  0.3424 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4949...  Training loss: 5.1056...  0.3402 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4950...  Training loss: 5.0252...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4951...  Training loss: 4.9434...  0.3450 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4952...  Training loss: 4.9435...  0.3433 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4953...  Training loss: 4.9295...  0.3441 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4954...  Training loss: 4.9022...  0.3397 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4955...  Training loss: 4.8993...  0.3420 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4956...  Training loss: 4.9137...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4957...  Training loss: 4.9977...  0.3416 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4958...  Training loss: 4.9432...  0.3441 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4959...  Training loss: 4.9453...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4960...  Training loss: 4.9039...  0.3413 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4961...  Training loss: 4.9703...  0.3414 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4962...  Training loss: 4.9257...  0.3430 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4963...  Training loss: 4.8839...  0.3387 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4964...  Training loss: 4.8971...  0.3411 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4965...  Training loss: 5.2416...  0.3400 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4966...  Training loss: 5.1145...  0.3434 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4967...  Training loss: 5.1569...  0.3417 sec/batch\n",
      "Epoch: 27/100...  Training Step: 4968...  Training loss: 5.0696...  0.3401 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4969...  Training loss: 5.2239...  0.3391 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4970...  Training loss: 5.2172...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4971...  Training loss: 5.0176...  0.3433 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4972...  Training loss: 5.0175...  0.3408 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4973...  Training loss: 4.9976...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4974...  Training loss: 4.9074...  0.3402 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4975...  Training loss: 5.0932...  0.3396 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4976...  Training loss: 4.9782...  0.3429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/100...  Training Step: 4977...  Training loss: 5.0169...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4978...  Training loss: 4.9315...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4979...  Training loss: 4.9176...  0.3397 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4980...  Training loss: 4.8636...  0.3397 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4981...  Training loss: 4.8485...  0.3439 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4982...  Training loss: 4.9376...  0.3433 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4983...  Training loss: 4.9499...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4984...  Training loss: 4.8951...  0.3412 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4985...  Training loss: 4.9798...  0.3400 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4986...  Training loss: 4.8710...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4987...  Training loss: 4.9987...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4988...  Training loss: 4.9103...  0.3388 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4989...  Training loss: 4.8853...  0.3431 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4990...  Training loss: 4.8928...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4991...  Training loss: 4.8833...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4992...  Training loss: 4.9386...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4993...  Training loss: 4.9211...  0.3400 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4994...  Training loss: 4.8827...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4995...  Training loss: 4.8857...  0.3419 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4996...  Training loss: 4.8892...  0.3406 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4997...  Training loss: 4.9066...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4998...  Training loss: 4.9213...  0.3410 sec/batch\n",
      "Epoch: 28/100...  Training Step: 4999...  Training loss: 4.9021...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5000...  Training loss: 4.8566...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5001...  Training loss: 4.6020...  0.3875 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5002...  Training loss: 4.7243...  0.3448 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5003...  Training loss: 4.8559...  0.3410 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5004...  Training loss: 5.0201...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5005...  Training loss: 4.9375...  0.3425 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5006...  Training loss: 4.9959...  0.3417 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5007...  Training loss: 4.9054...  0.3434 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5008...  Training loss: 4.9271...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5009...  Training loss: 5.0173...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5010...  Training loss: 4.9914...  0.3411 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5011...  Training loss: 5.0399...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5012...  Training loss: 5.0327...  0.3453 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5013...  Training loss: 4.9318...  0.3449 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5014...  Training loss: 4.8903...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5015...  Training loss: 4.9764...  0.3417 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5016...  Training loss: 4.8359...  0.3413 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5017...  Training loss: 4.9029...  0.3427 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5018...  Training loss: 5.0488...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5019...  Training loss: 4.9648...  0.3440 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5020...  Training loss: 4.8872...  0.3441 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5021...  Training loss: 4.9831...  0.3446 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5022...  Training loss: 5.0018...  0.3455 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5023...  Training loss: 4.9884...  0.3435 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5024...  Training loss: 4.9260...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5025...  Training loss: 4.8619...  0.3412 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5026...  Training loss: 4.9750...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5027...  Training loss: 4.8257...  0.3427 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5028...  Training loss: 4.8449...  0.3429 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5029...  Training loss: 4.8637...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5030...  Training loss: 4.9149...  0.3428 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5031...  Training loss: 4.9585...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5032...  Training loss: 4.9569...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5033...  Training loss: 4.8445...  0.3448 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5034...  Training loss: 4.8427...  0.3433 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5035...  Training loss: 4.8808...  0.3456 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5036...  Training loss: 4.7765...  0.3455 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5037...  Training loss: 4.9565...  0.3440 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5038...  Training loss: 4.8943...  0.3404 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5039...  Training loss: 4.9112...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5040...  Training loss: 4.7533...  0.3430 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5041...  Training loss: 4.7881...  0.3421 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5042...  Training loss: 4.8365...  0.3438 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5043...  Training loss: 4.7977...  0.3446 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5044...  Training loss: 4.8146...  0.3424 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5045...  Training loss: 4.8830...  0.3459 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5046...  Training loss: 4.8765...  0.3440 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5047...  Training loss: 4.9365...  0.3436 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5048...  Training loss: 5.0476...  0.3414 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5049...  Training loss: 5.0221...  0.3406 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5050...  Training loss: 4.9993...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5051...  Training loss: 4.9766...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5052...  Training loss: 4.9429...  0.3417 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5053...  Training loss: 5.0163...  0.3402 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5054...  Training loss: 4.9502...  0.3410 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5055...  Training loss: 4.9001...  0.3440 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5056...  Training loss: 4.9796...  0.3422 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5057...  Training loss: 5.0472...  0.3415 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5058...  Training loss: 5.0909...  0.3439 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5059...  Training loss: 5.0245...  0.3446 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5060...  Training loss: 4.8125...  0.3452 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5061...  Training loss: 4.9900...  0.3466 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5062...  Training loss: 5.0237...  0.3455 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5063...  Training loss: 4.9714...  0.3424 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5064...  Training loss: 4.9204...  0.3422 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5065...  Training loss: 4.8804...  0.3429 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5066...  Training loss: 4.8398...  0.3464 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5067...  Training loss: 4.8791...  0.3422 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5068...  Training loss: 4.8612...  0.3419 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5069...  Training loss: 4.8637...  0.3434 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5070...  Training loss: 4.8562...  0.3444 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5071...  Training loss: 5.0274...  0.3466 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5072...  Training loss: 4.9071...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5073...  Training loss: 4.9867...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/100...  Training Step: 5074...  Training loss: 5.0254...  0.3407 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5075...  Training loss: 4.9673...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5076...  Training loss: 4.8177...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5077...  Training loss: 4.8131...  0.3457 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5078...  Training loss: 4.8395...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5079...  Training loss: 4.8157...  0.3398 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5080...  Training loss: 4.8888...  0.3413 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5081...  Training loss: 4.8612...  0.3412 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5082...  Training loss: 4.8464...  0.3430 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5083...  Training loss: 4.8120...  0.3444 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5084...  Training loss: 4.8188...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5085...  Training loss: 4.9555...  0.3423 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5086...  Training loss: 4.8159...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5087...  Training loss: 4.8141...  0.3388 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5088...  Training loss: 4.8581...  0.3415 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5089...  Training loss: 5.0428...  0.3423 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5090...  Training loss: 4.9765...  0.3438 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5091...  Training loss: 4.8803...  0.3445 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5092...  Training loss: 4.8697...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5093...  Training loss: 4.8157...  0.3454 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5094...  Training loss: 4.8348...  0.3455 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5095...  Training loss: 4.8849...  0.3450 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5096...  Training loss: 4.9285...  0.3438 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5097...  Training loss: 5.0185...  0.3438 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5098...  Training loss: 4.9139...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5099...  Training loss: 4.9082...  0.3413 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5100...  Training loss: 4.9153...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5101...  Training loss: 4.9642...  0.3398 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5102...  Training loss: 4.7763...  0.3394 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5103...  Training loss: 4.8464...  0.3425 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5104...  Training loss: 4.7804...  0.3426 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5105...  Training loss: 4.9841...  0.3435 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5106...  Training loss: 4.8276...  0.3431 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5107...  Training loss: 4.9319...  0.3440 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5108...  Training loss: 5.1566...  0.3427 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5109...  Training loss: 5.1603...  0.3400 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5110...  Training loss: 5.0593...  0.3401 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5111...  Training loss: 4.8662...  0.3423 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5112...  Training loss: 4.8268...  0.3396 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5113...  Training loss: 4.8418...  0.3434 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5114...  Training loss: 4.8788...  0.3400 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5115...  Training loss: 4.9584...  0.3417 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5116...  Training loss: 4.9310...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5117...  Training loss: 4.9416...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5118...  Training loss: 4.8559...  0.3396 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5119...  Training loss: 4.9088...  0.3394 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5120...  Training loss: 4.7508...  0.3397 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5121...  Training loss: 4.9221...  0.3410 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5122...  Training loss: 4.8737...  0.3443 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5123...  Training loss: 4.9386...  0.3421 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5124...  Training loss: 4.8899...  0.3412 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5125...  Training loss: 4.9536...  0.3418 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5126...  Training loss: 4.9777...  0.3400 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5127...  Training loss: 4.8627...  0.3420 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5128...  Training loss: 4.8682...  0.3407 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5129...  Training loss: 4.8809...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5130...  Training loss: 4.9230...  0.3437 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5131...  Training loss: 4.9213...  0.3429 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5132...  Training loss: 4.9480...  0.3409 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5133...  Training loss: 5.0583...  0.3403 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5134...  Training loss: 4.9869...  0.3425 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5135...  Training loss: 4.9187...  0.3421 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5136...  Training loss: 4.9030...  0.3407 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5137...  Training loss: 4.9092...  0.3414 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5138...  Training loss: 4.8829...  0.3438 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5139...  Training loss: 4.8620...  0.3431 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5140...  Training loss: 4.8790...  0.3424 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5141...  Training loss: 4.9511...  0.3426 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5142...  Training loss: 4.9247...  0.3397 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5143...  Training loss: 4.8932...  0.3402 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5144...  Training loss: 4.8722...  0.3416 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5145...  Training loss: 4.9376...  0.3419 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5146...  Training loss: 4.8986...  0.3424 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5147...  Training loss: 4.8848...  0.3422 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5148...  Training loss: 4.8833...  0.3404 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5149...  Training loss: 5.2244...  0.3419 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5150...  Training loss: 5.0784...  0.3410 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5151...  Training loss: 5.1121...  0.3425 sec/batch\n",
      "Epoch: 28/100...  Training Step: 5152...  Training loss: 5.0326...  0.3426 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5153...  Training loss: 5.1756...  0.3434 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5154...  Training loss: 5.1741...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5155...  Training loss: 4.9991...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5156...  Training loss: 4.9982...  0.3419 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5157...  Training loss: 4.9779...  0.3392 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5158...  Training loss: 4.8918...  0.3418 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5159...  Training loss: 5.0928...  0.3432 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5160...  Training loss: 4.9784...  0.3411 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5161...  Training loss: 5.0205...  0.3420 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5162...  Training loss: 4.9212...  0.3446 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5163...  Training loss: 4.8938...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5164...  Training loss: 4.8640...  0.3434 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5165...  Training loss: 4.8303...  0.3392 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5166...  Training loss: 4.9156...  0.3422 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5167...  Training loss: 4.9174...  0.3393 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5168...  Training loss: 4.8800...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5169...  Training loss: 4.9451...  0.3401 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5170...  Training loss: 4.8153...  0.3419 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/100...  Training Step: 5171...  Training loss: 4.9641...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5172...  Training loss: 4.8719...  0.3402 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5173...  Training loss: 4.8610...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5174...  Training loss: 4.8538...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5175...  Training loss: 4.8684...  0.3395 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5176...  Training loss: 4.8867...  0.3439 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5177...  Training loss: 4.9051...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5178...  Training loss: 4.8461...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5179...  Training loss: 4.8624...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5180...  Training loss: 4.8611...  0.3409 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5181...  Training loss: 4.8828...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5182...  Training loss: 4.9190...  0.3422 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5183...  Training loss: 4.8586...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5184...  Training loss: 4.8370...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5185...  Training loss: 4.5794...  0.3434 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5186...  Training loss: 4.6973...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5187...  Training loss: 4.8291...  0.3433 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5188...  Training loss: 5.0058...  0.3394 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5189...  Training loss: 4.9267...  0.3428 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5190...  Training loss: 4.9657...  0.3406 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5191...  Training loss: 4.8853...  0.3394 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5192...  Training loss: 4.9108...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5193...  Training loss: 4.9965...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5194...  Training loss: 4.9529...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5195...  Training loss: 5.0204...  0.3427 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5196...  Training loss: 5.0039...  0.3393 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5197...  Training loss: 4.8998...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5198...  Training loss: 4.8782...  0.3387 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5199...  Training loss: 4.9666...  0.3439 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5200...  Training loss: 4.8280...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5201...  Training loss: 4.9099...  0.3420 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5202...  Training loss: 5.0427...  0.3391 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5203...  Training loss: 4.9446...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5204...  Training loss: 4.8868...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5205...  Training loss: 4.9429...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5206...  Training loss: 4.9761...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5207...  Training loss: 4.9904...  0.3388 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5208...  Training loss: 4.8981...  0.3395 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5209...  Training loss: 4.8294...  0.3428 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5210...  Training loss: 4.9420...  0.3428 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5211...  Training loss: 4.7735...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5212...  Training loss: 4.8165...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5213...  Training loss: 4.8343...  0.3406 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5214...  Training loss: 4.9025...  0.3390 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5215...  Training loss: 4.9273...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5216...  Training loss: 4.9460...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5217...  Training loss: 4.8396...  0.3437 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5218...  Training loss: 4.8419...  0.3420 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5219...  Training loss: 4.8648...  0.3417 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5220...  Training loss: 4.7567...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5221...  Training loss: 4.9327...  0.3401 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5222...  Training loss: 4.8731...  0.3431 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5223...  Training loss: 4.8757...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5224...  Training loss: 4.7335...  0.3421 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5225...  Training loss: 4.7637...  0.3438 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5226...  Training loss: 4.8052...  0.3428 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5227...  Training loss: 4.7975...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5228...  Training loss: 4.8080...  0.3409 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5229...  Training loss: 4.8661...  0.3410 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5230...  Training loss: 4.8528...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5231...  Training loss: 4.9130...  0.3396 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5232...  Training loss: 5.0239...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5233...  Training loss: 4.9790...  0.3418 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5234...  Training loss: 4.9434...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5235...  Training loss: 4.9474...  0.3440 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5236...  Training loss: 4.9190...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5237...  Training loss: 5.0184...  0.3419 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5238...  Training loss: 4.9141...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5239...  Training loss: 4.8695...  0.3390 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5240...  Training loss: 4.9440...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5241...  Training loss: 4.9981...  0.3418 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5242...  Training loss: 5.0504...  0.3412 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5243...  Training loss: 5.0068...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5244...  Training loss: 4.7587...  0.3447 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5245...  Training loss: 4.9739...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5246...  Training loss: 5.0027...  0.3407 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5247...  Training loss: 4.9444...  0.3413 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5248...  Training loss: 4.9022...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5249...  Training loss: 4.8635...  0.3408 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5250...  Training loss: 4.8014...  0.3422 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5251...  Training loss: 4.8574...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5252...  Training loss: 4.8286...  0.3421 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5253...  Training loss: 4.8428...  0.3434 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5254...  Training loss: 4.8112...  0.3396 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5255...  Training loss: 4.9872...  0.3423 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5256...  Training loss: 4.8899...  0.3418 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5257...  Training loss: 4.9466...  0.3439 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5258...  Training loss: 5.0154...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5259...  Training loss: 4.9722...  0.3427 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5260...  Training loss: 4.8196...  0.3393 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5261...  Training loss: 4.8174...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5262...  Training loss: 4.8560...  0.3414 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5263...  Training loss: 4.8196...  0.3404 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5264...  Training loss: 4.8881...  0.3406 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5265...  Training loss: 4.8678...  0.3406 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5266...  Training loss: 4.8236...  0.3399 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5267...  Training loss: 4.7932...  0.3399 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/100...  Training Step: 5268...  Training loss: 4.7784...  0.3408 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5269...  Training loss: 4.9253...  0.3408 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5270...  Training loss: 4.7722...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5271...  Training loss: 4.7942...  0.3426 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5272...  Training loss: 4.8349...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5273...  Training loss: 5.0158...  0.3438 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5274...  Training loss: 4.9488...  0.3411 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5275...  Training loss: 4.8889...  0.3409 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5276...  Training loss: 4.8631...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5277...  Training loss: 4.8001...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5278...  Training loss: 4.8155...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5279...  Training loss: 4.8802...  0.3420 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5280...  Training loss: 4.9100...  0.3426 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5281...  Training loss: 5.0190...  0.3441 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5282...  Training loss: 4.8925...  0.3430 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5283...  Training loss: 4.8940...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5284...  Training loss: 4.9101...  0.3426 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5285...  Training loss: 4.9562...  0.3417 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5286...  Training loss: 4.7667...  0.3402 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5287...  Training loss: 4.8487...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5288...  Training loss: 4.7786...  0.3438 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5289...  Training loss: 4.9901...  0.3395 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5290...  Training loss: 4.8448...  0.3430 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5291...  Training loss: 4.9380...  0.3391 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5292...  Training loss: 5.1387...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5293...  Training loss: 5.1514...  0.3433 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5294...  Training loss: 5.0380...  0.3437 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5295...  Training loss: 4.8509...  0.3390 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5296...  Training loss: 4.8153...  0.3385 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5297...  Training loss: 4.8352...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5298...  Training loss: 4.8774...  0.3412 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5299...  Training loss: 4.9557...  0.3419 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5300...  Training loss: 4.9280...  0.3421 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5301...  Training loss: 4.9352...  0.3411 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5302...  Training loss: 4.8488...  0.3393 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5303...  Training loss: 4.8825...  0.3391 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5304...  Training loss: 4.7432...  0.3406 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5305...  Training loss: 4.8973...  0.3398 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5306...  Training loss: 4.8499...  0.3398 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5307...  Training loss: 4.9145...  0.3417 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5308...  Training loss: 4.8607...  0.3389 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5309...  Training loss: 4.9325...  0.3405 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5310...  Training loss: 4.9570...  0.3408 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5311...  Training loss: 4.8386...  0.3440 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5312...  Training loss: 4.8363...  0.3408 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5313...  Training loss: 4.8616...  0.3395 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5314...  Training loss: 4.8920...  0.3411 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5315...  Training loss: 4.8922...  0.3407 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5316...  Training loss: 4.9266...  0.3417 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5317...  Training loss: 5.0310...  0.3401 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5318...  Training loss: 4.9403...  0.3412 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5319...  Training loss: 4.9001...  0.3404 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5320...  Training loss: 4.8723...  0.3392 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5321...  Training loss: 4.8634...  0.3403 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5322...  Training loss: 4.8597...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5323...  Training loss: 4.8452...  0.3416 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5324...  Training loss: 4.8611...  0.3410 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5325...  Training loss: 4.9336...  0.3400 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5326...  Training loss: 4.9043...  0.3431 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5327...  Training loss: 4.8882...  0.3442 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5328...  Training loss: 4.8542...  0.3435 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5329...  Training loss: 4.9128...  0.3440 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5330...  Training loss: 4.8838...  0.3432 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5331...  Training loss: 4.8547...  0.3424 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5332...  Training loss: 4.8474...  0.3433 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5333...  Training loss: 5.1997...  0.3415 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5334...  Training loss: 5.0548...  0.3425 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5335...  Training loss: 5.0667...  0.3430 sec/batch\n",
      "Epoch: 29/100...  Training Step: 5336...  Training loss: 4.9928...  0.3420 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5337...  Training loss: 5.1518...  0.3427 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5338...  Training loss: 5.1299...  0.3428 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5339...  Training loss: 4.9593...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5340...  Training loss: 4.9626...  0.3419 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5341...  Training loss: 4.9361...  0.3405 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5342...  Training loss: 4.8516...  0.3395 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5343...  Training loss: 5.0484...  0.3436 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5344...  Training loss: 4.9561...  0.3409 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5345...  Training loss: 4.9858...  0.3402 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5346...  Training loss: 4.8963...  0.3408 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5347...  Training loss: 4.8952...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5348...  Training loss: 4.8613...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5349...  Training loss: 4.8362...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5350...  Training loss: 4.9300...  0.3402 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5351...  Training loss: 4.9081...  0.3443 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5352...  Training loss: 4.8590...  0.3445 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5353...  Training loss: 4.9355...  0.3426 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5354...  Training loss: 4.8275...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5355...  Training loss: 4.9894...  0.3430 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5356...  Training loss: 4.8720...  0.3395 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5357...  Training loss: 4.8523...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5358...  Training loss: 4.8235...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5359...  Training loss: 4.8307...  0.3405 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5360...  Training loss: 4.8885...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5361...  Training loss: 4.8975...  0.3389 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5362...  Training loss: 4.8451...  0.3416 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5363...  Training loss: 4.8335...  0.3431 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5364...  Training loss: 4.8439...  0.3393 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100...  Training Step: 5365...  Training loss: 4.8784...  0.3448 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5366...  Training loss: 4.9042...  0.3390 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5367...  Training loss: 4.8543...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5368...  Training loss: 4.8216...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5369...  Training loss: 4.5663...  0.3427 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5370...  Training loss: 4.6888...  0.3408 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5371...  Training loss: 4.8122...  0.3414 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5372...  Training loss: 4.9699...  0.3430 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5373...  Training loss: 4.8972...  0.3413 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5374...  Training loss: 4.9448...  0.3412 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5375...  Training loss: 4.8674...  0.3398 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5376...  Training loss: 4.8828...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5377...  Training loss: 4.9663...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5378...  Training loss: 4.9395...  0.3406 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5379...  Training loss: 5.0037...  0.3388 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5380...  Training loss: 4.9833...  0.3419 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5381...  Training loss: 4.8828...  0.3396 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5382...  Training loss: 4.8573...  0.3419 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5383...  Training loss: 4.9506...  0.3398 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5384...  Training loss: 4.8081...  0.3402 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5385...  Training loss: 4.8732...  0.3399 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5386...  Training loss: 5.0247...  0.3400 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5387...  Training loss: 4.9653...  0.3391 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5388...  Training loss: 4.8818...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5389...  Training loss: 4.9400...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5390...  Training loss: 4.9707...  0.3409 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5391...  Training loss: 4.9779...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5392...  Training loss: 4.8897...  0.3434 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5393...  Training loss: 4.8169...  0.3431 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5394...  Training loss: 4.9224...  0.3448 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5395...  Training loss: 4.7546...  0.3400 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5396...  Training loss: 4.8049...  0.3413 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5397...  Training loss: 4.8066...  0.3395 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5398...  Training loss: 4.8580...  0.3442 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5399...  Training loss: 4.9079...  0.3423 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5400...  Training loss: 4.9198...  0.3425 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5401...  Training loss: 4.8029...  0.3414 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5402...  Training loss: 4.8158...  0.3422 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5403...  Training loss: 4.8651...  0.3423 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5404...  Training loss: 4.7578...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5405...  Training loss: 4.9221...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5406...  Training loss: 4.8655...  0.3428 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5407...  Training loss: 4.8434...  0.3390 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5408...  Training loss: 4.7051...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5409...  Training loss: 4.7460...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5410...  Training loss: 4.7922...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5411...  Training loss: 4.7781...  0.3414 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5412...  Training loss: 4.7885...  0.3415 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5413...  Training loss: 4.8676...  0.3409 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5414...  Training loss: 4.8389...  0.3406 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5415...  Training loss: 4.9243...  0.3399 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5416...  Training loss: 5.0124...  0.3414 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5417...  Training loss: 4.9686...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5418...  Training loss: 4.9231...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5419...  Training loss: 4.9156...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5420...  Training loss: 4.8987...  0.3399 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5421...  Training loss: 4.9648...  0.3399 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5422...  Training loss: 4.9220...  0.3388 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5423...  Training loss: 4.8869...  0.3405 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5424...  Training loss: 4.9274...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5425...  Training loss: 4.9847...  0.3418 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5426...  Training loss: 5.0220...  0.3426 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5427...  Training loss: 4.9602...  0.3439 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5428...  Training loss: 4.7276...  0.3406 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5429...  Training loss: 4.9257...  0.3435 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5430...  Training loss: 4.9520...  0.3392 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5431...  Training loss: 4.9159...  0.3387 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5432...  Training loss: 4.8845...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5433...  Training loss: 4.8557...  0.3418 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5434...  Training loss: 4.8157...  0.3408 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5435...  Training loss: 4.8592...  0.3404 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5436...  Training loss: 4.8269...  0.3419 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5437...  Training loss: 4.8496...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5438...  Training loss: 4.8236...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5439...  Training loss: 4.9351...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5440...  Training loss: 4.8650...  0.3433 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5441...  Training loss: 4.9286...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5442...  Training loss: 4.9832...  0.3434 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5443...  Training loss: 4.9404...  0.3455 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5444...  Training loss: 4.8152...  0.3404 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5445...  Training loss: 4.8195...  0.3401 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5446...  Training loss: 4.8221...  0.3420 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5447...  Training loss: 4.8223...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5448...  Training loss: 4.9061...  0.3429 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5449...  Training loss: 4.8909...  0.3389 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5450...  Training loss: 4.8563...  0.3396 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5451...  Training loss: 4.8252...  0.3424 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5452...  Training loss: 4.8101...  0.3441 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5453...  Training loss: 4.9427...  0.3420 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5454...  Training loss: 4.8127...  0.3401 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5455...  Training loss: 4.8033...  0.3404 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5456...  Training loss: 4.8500...  0.3433 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5457...  Training loss: 5.0278...  0.3419 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5458...  Training loss: 4.9481...  0.3412 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5459...  Training loss: 4.8598...  0.3438 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5460...  Training loss: 4.8550...  0.3397 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5461...  Training loss: 4.7662...  0.3432 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100...  Training Step: 5462...  Training loss: 4.8056...  0.3413 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5463...  Training loss: 4.8503...  0.3394 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5464...  Training loss: 4.8746...  0.3437 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5465...  Training loss: 4.9803...  0.3423 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5466...  Training loss: 4.8554...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5467...  Training loss: 4.8660...  0.3439 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5468...  Training loss: 4.8616...  0.3451 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5469...  Training loss: 4.9298...  0.3406 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5470...  Training loss: 4.7291...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5471...  Training loss: 4.8103...  0.3441 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5472...  Training loss: 4.7478...  0.3437 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5473...  Training loss: 4.9304...  0.3423 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5474...  Training loss: 4.8019...  0.3408 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5475...  Training loss: 4.8794...  0.3434 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5476...  Training loss: 5.0671...  0.3428 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5477...  Training loss: 5.0994...  0.3431 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5478...  Training loss: 5.0063...  0.3391 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5479...  Training loss: 4.8076...  0.3424 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5480...  Training loss: 4.7889...  0.3409 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5481...  Training loss: 4.8100...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5482...  Training loss: 4.8446...  0.3425 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5483...  Training loss: 4.9278...  0.3395 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5484...  Training loss: 4.9058...  0.3396 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5485...  Training loss: 4.9106...  0.3425 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5486...  Training loss: 4.8344...  0.3416 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5487...  Training loss: 4.8823...  0.3408 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5488...  Training loss: 4.7367...  0.3425 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5489...  Training loss: 4.8905...  0.3403 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5490...  Training loss: 4.8287...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5491...  Training loss: 4.8996...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5492...  Training loss: 4.8686...  0.3407 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5493...  Training loss: 4.9060...  0.3442 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5494...  Training loss: 4.9478...  0.3411 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5495...  Training loss: 4.8118...  0.3404 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5496...  Training loss: 4.8294...  0.3393 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5497...  Training loss: 4.8395...  0.3413 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5498...  Training loss: 4.8931...  0.3396 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5499...  Training loss: 4.8694...  0.3405 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5500...  Training loss: 4.8951...  0.3418 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5501...  Training loss: 4.9947...  0.3422 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5502...  Training loss: 4.9165...  0.3405 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5503...  Training loss: 4.8874...  0.3416 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5504...  Training loss: 4.8571...  0.3427 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5505...  Training loss: 4.8557...  0.3410 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5506...  Training loss: 4.8561...  0.3438 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5507...  Training loss: 4.8456...  0.3417 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5508...  Training loss: 4.8298...  0.3404 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5509...  Training loss: 4.8775...  0.3403 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5510...  Training loss: 4.8397...  0.3431 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5511...  Training loss: 4.8347...  0.3425 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5512...  Training loss: 4.8010...  0.3421 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5513...  Training loss: 4.8781...  0.3431 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5514...  Training loss: 4.8412...  0.3418 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5515...  Training loss: 4.8294...  0.3437 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5516...  Training loss: 4.8356...  0.3402 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5517...  Training loss: 5.1370...  0.3398 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5518...  Training loss: 5.0185...  0.3429 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5519...  Training loss: 5.0541...  0.3455 sec/batch\n",
      "Epoch: 30/100...  Training Step: 5520...  Training loss: 4.9625...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5521...  Training loss: 5.1116...  0.3421 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5522...  Training loss: 5.0842...  0.3443 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5523...  Training loss: 4.9182...  0.3403 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5524...  Training loss: 4.9300...  0.3440 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5525...  Training loss: 4.9198...  0.3439 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5526...  Training loss: 4.8246...  0.3403 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5527...  Training loss: 5.0159...  0.3394 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5528...  Training loss: 4.9324...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5529...  Training loss: 4.9639...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5530...  Training loss: 4.8644...  0.3396 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5531...  Training loss: 4.8587...  0.3409 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5532...  Training loss: 4.8375...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5533...  Training loss: 4.8262...  0.3407 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5534...  Training loss: 4.9215...  0.3407 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5535...  Training loss: 4.9112...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5536...  Training loss: 4.8466...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5537...  Training loss: 4.8953...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5538...  Training loss: 4.8082...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5539...  Training loss: 4.9255...  0.3442 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5540...  Training loss: 4.8310...  0.3423 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5541...  Training loss: 4.8427...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5542...  Training loss: 4.7963...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5543...  Training loss: 4.8367...  0.3430 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5544...  Training loss: 4.8627...  0.3416 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5545...  Training loss: 4.8627...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5546...  Training loss: 4.8277...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5547...  Training loss: 4.8277...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5548...  Training loss: 4.8176...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5549...  Training loss: 4.8631...  0.3409 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5550...  Training loss: 4.8953...  0.3418 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5551...  Training loss: 4.8529...  0.3426 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5552...  Training loss: 4.8082...  0.3416 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5553...  Training loss: 4.5653...  0.3420 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5554...  Training loss: 4.6754...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5555...  Training loss: 4.8052...  0.3395 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5556...  Training loss: 4.9418...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5557...  Training loss: 4.8943...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5558...  Training loss: 4.9236...  0.3396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/100...  Training Step: 5559...  Training loss: 4.8563...  0.3407 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5560...  Training loss: 4.8709...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5561...  Training loss: 4.9545...  0.3416 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5562...  Training loss: 4.9005...  0.3421 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5563...  Training loss: 4.9763...  0.3444 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5564...  Training loss: 4.9352...  0.3399 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5565...  Training loss: 4.8772...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5566...  Training loss: 4.8330...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5567...  Training loss: 4.9090...  0.3396 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5568...  Training loss: 4.7941...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5569...  Training loss: 4.8517...  0.3418 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5570...  Training loss: 4.9903...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5571...  Training loss: 4.9184...  0.3436 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5572...  Training loss: 4.8636...  0.3421 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5573...  Training loss: 4.9231...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5574...  Training loss: 4.9576...  0.3454 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5575...  Training loss: 4.9600...  0.3450 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5576...  Training loss: 4.8753...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5577...  Training loss: 4.8065...  0.3400 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5578...  Training loss: 4.9050...  0.3435 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5579...  Training loss: 4.7562...  0.3399 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5580...  Training loss: 4.7771...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5581...  Training loss: 4.7988...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5582...  Training loss: 4.8477...  0.3431 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5583...  Training loss: 4.8789...  0.3438 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5584...  Training loss: 4.8778...  0.3421 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5585...  Training loss: 4.7739...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5586...  Training loss: 4.7889...  0.3446 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5587...  Training loss: 4.8200...  0.3388 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5588...  Training loss: 4.7223...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5589...  Training loss: 4.9088...  0.3420 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5590...  Training loss: 4.8377...  0.3400 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5591...  Training loss: 4.8554...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5592...  Training loss: 4.6922...  0.3410 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5593...  Training loss: 4.7122...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5594...  Training loss: 4.7738...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5595...  Training loss: 4.7481...  0.3407 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5596...  Training loss: 4.7791...  0.3416 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5597...  Training loss: 4.8360...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5598...  Training loss: 4.8373...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5599...  Training loss: 4.8772...  0.3394 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5600...  Training loss: 4.9641...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5601...  Training loss: 4.9483...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5602...  Training loss: 4.8933...  0.3391 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5603...  Training loss: 4.9114...  0.3440 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5604...  Training loss: 4.8803...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5605...  Training loss: 4.9193...  0.3392 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5606...  Training loss: 4.8490...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5607...  Training loss: 4.8119...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5608...  Training loss: 4.8710...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5609...  Training loss: 4.9783...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5610...  Training loss: 5.0131...  0.3397 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5611...  Training loss: 4.9375...  0.3400 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5612...  Training loss: 4.7285...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5613...  Training loss: 4.9126...  0.3417 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5614...  Training loss: 4.9334...  0.3438 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5615...  Training loss: 4.8752...  0.3397 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5616...  Training loss: 4.8576...  0.3402 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5617...  Training loss: 4.8106...  0.3410 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5618...  Training loss: 4.7792...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5619...  Training loss: 4.8161...  0.3412 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5620...  Training loss: 4.8079...  0.3397 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5621...  Training loss: 4.8336...  0.3440 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5622...  Training loss: 4.8037...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5623...  Training loss: 4.9450...  0.3397 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5624...  Training loss: 4.8286...  0.3405 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5625...  Training loss: 4.9079...  0.3399 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5626...  Training loss: 4.9186...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5627...  Training loss: 4.8855...  0.3391 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5628...  Training loss: 4.7468...  0.3417 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5629...  Training loss: 4.7567...  0.3392 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5630...  Training loss: 4.7743...  0.3409 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5631...  Training loss: 4.7692...  0.3435 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5632...  Training loss: 4.8350...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5633...  Training loss: 4.8229...  0.3446 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5634...  Training loss: 4.7975...  0.3402 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5635...  Training loss: 4.7671...  0.3424 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5636...  Training loss: 4.7640...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5637...  Training loss: 4.9085...  0.3412 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5638...  Training loss: 4.7703...  0.3409 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5639...  Training loss: 4.7854...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5640...  Training loss: 4.8279...  0.3399 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5641...  Training loss: 4.9987...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5642...  Training loss: 4.9209...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5643...  Training loss: 4.8405...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5644...  Training loss: 4.8249...  0.3407 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5645...  Training loss: 4.7601...  0.3419 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5646...  Training loss: 4.7914...  0.3403 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5647...  Training loss: 4.8306...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5648...  Training loss: 4.8614...  0.3434 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5649...  Training loss: 4.9726...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5650...  Training loss: 4.8450...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5651...  Training loss: 4.8436...  0.3434 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5652...  Training loss: 4.8645...  0.3436 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5653...  Training loss: 4.9077...  0.3409 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5654...  Training loss: 4.7184...  0.3418 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5655...  Training loss: 4.7967...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/100...  Training Step: 5656...  Training loss: 4.7279...  0.3436 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5657...  Training loss: 4.9134...  0.3422 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5658...  Training loss: 4.7827...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5659...  Training loss: 4.8644...  0.3434 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5660...  Training loss: 5.0795...  0.3410 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5661...  Training loss: 5.0685...  0.3446 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5662...  Training loss: 4.9837...  0.3426 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5663...  Training loss: 4.8111...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5664...  Training loss: 4.7838...  0.3424 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5665...  Training loss: 4.7999...  0.3436 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5666...  Training loss: 4.8379...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5667...  Training loss: 4.8994...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5668...  Training loss: 4.8749...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5669...  Training loss: 4.8888...  0.3416 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5670...  Training loss: 4.8157...  0.3415 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5671...  Training loss: 4.8668...  0.3395 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5672...  Training loss: 4.7432...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5673...  Training loss: 4.8733...  0.3442 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5674...  Training loss: 4.8069...  0.3425 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5675...  Training loss: 4.8869...  0.3412 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5676...  Training loss: 4.8266...  0.3412 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5677...  Training loss: 4.8980...  0.3401 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5678...  Training loss: 4.9194...  0.3425 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5679...  Training loss: 4.7857...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5680...  Training loss: 4.8194...  0.3399 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5681...  Training loss: 4.8367...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5682...  Training loss: 4.8568...  0.3406 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5683...  Training loss: 4.8455...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5684...  Training loss: 4.8678...  0.3410 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5685...  Training loss: 4.9679...  0.3435 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5686...  Training loss: 4.9004...  0.3411 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5687...  Training loss: 4.8596...  0.3434 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5688...  Training loss: 4.8373...  0.3392 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5689...  Training loss: 4.8639...  0.3432 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5690...  Training loss: 4.8334...  0.3402 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5691...  Training loss: 4.8109...  0.3414 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5692...  Training loss: 4.7934...  0.3389 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5693...  Training loss: 4.8679...  0.3404 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5694...  Training loss: 4.8043...  0.3383 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5695...  Training loss: 4.8206...  0.3388 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5696...  Training loss: 4.7772...  0.3412 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5697...  Training loss: 4.8252...  0.3449 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5698...  Training loss: 4.8053...  0.3427 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5699...  Training loss: 4.7782...  0.3428 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5700...  Training loss: 4.7909...  0.3413 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5701...  Training loss: 5.1101...  0.3441 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5702...  Training loss: 5.0163...  0.3442 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5703...  Training loss: 5.0223...  0.3431 sec/batch\n",
      "Epoch: 31/100...  Training Step: 5704...  Training loss: 4.9487...  0.3396 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5705...  Training loss: 5.0686...  0.3427 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5706...  Training loss: 5.0267...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5707...  Training loss: 4.8755...  0.3420 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5708...  Training loss: 4.8724...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5709...  Training loss: 4.8710...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5710...  Training loss: 4.7775...  0.3394 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5711...  Training loss: 4.9697...  0.3409 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5712...  Training loss: 4.8920...  0.3403 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5713...  Training loss: 4.9343...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5714...  Training loss: 4.8559...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5715...  Training loss: 4.8372...  0.3432 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5716...  Training loss: 4.8121...  0.3427 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5717...  Training loss: 4.8168...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5718...  Training loss: 4.8879...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5719...  Training loss: 4.8929...  0.3399 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5720...  Training loss: 4.8516...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5721...  Training loss: 4.8814...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5722...  Training loss: 4.7743...  0.3406 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5723...  Training loss: 4.8898...  0.3421 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5724...  Training loss: 4.8066...  0.3438 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5725...  Training loss: 4.8021...  0.3431 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5726...  Training loss: 4.7756...  0.3439 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5727...  Training loss: 4.7931...  0.3414 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5728...  Training loss: 4.8333...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5729...  Training loss: 4.8264...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5730...  Training loss: 4.7874...  0.3395 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5731...  Training loss: 4.8090...  0.3409 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5732...  Training loss: 4.8068...  0.3444 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5733...  Training loss: 4.8266...  0.3433 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5734...  Training loss: 4.8612...  0.3421 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5735...  Training loss: 4.8189...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5736...  Training loss: 4.7717...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5737...  Training loss: 4.5333...  0.3428 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5738...  Training loss: 4.6508...  0.3393 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5739...  Training loss: 4.7846...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5740...  Training loss: 4.9227...  0.3404 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5741...  Training loss: 4.8541...  0.3408 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5742...  Training loss: 4.8946...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5743...  Training loss: 4.8339...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5744...  Training loss: 4.8446...  0.3425 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5745...  Training loss: 4.9333...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5746...  Training loss: 4.8973...  0.3422 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5747...  Training loss: 4.9386...  0.3391 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5748...  Training loss: 4.9378...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5749...  Training loss: 4.8557...  0.3434 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5750...  Training loss: 4.7941...  0.3397 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5751...  Training loss: 4.8928...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5752...  Training loss: 4.7537...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/100...  Training Step: 5753...  Training loss: 4.8357...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5754...  Training loss: 4.9576...  0.3403 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5755...  Training loss: 4.8884...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5756...  Training loss: 4.8317...  0.3394 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5757...  Training loss: 4.8990...  0.3397 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5758...  Training loss: 4.9279...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5759...  Training loss: 4.9187...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5760...  Training loss: 4.8516...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5761...  Training loss: 4.7857...  0.3393 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5762...  Training loss: 4.9000...  0.3453 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5763...  Training loss: 4.7451...  0.3397 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5764...  Training loss: 4.7682...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5765...  Training loss: 4.7874...  0.3395 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5766...  Training loss: 4.8461...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5767...  Training loss: 4.8712...  0.3404 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5768...  Training loss: 4.8821...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5769...  Training loss: 4.7791...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5770...  Training loss: 4.7820...  0.3436 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5771...  Training loss: 4.8131...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5772...  Training loss: 4.7121...  0.3418 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5773...  Training loss: 4.8855...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5774...  Training loss: 4.8319...  0.3421 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5775...  Training loss: 4.8396...  0.3428 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5776...  Training loss: 4.6726...  0.3406 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5777...  Training loss: 4.7194...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5778...  Training loss: 4.7797...  0.3414 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5779...  Training loss: 4.7413...  0.3433 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5780...  Training loss: 4.7701...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5781...  Training loss: 4.8236...  0.3421 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5782...  Training loss: 4.8105...  0.3443 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5783...  Training loss: 4.8681...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5784...  Training loss: 4.9526...  0.3448 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5785...  Training loss: 4.9519...  0.3431 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5786...  Training loss: 4.8743...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5787...  Training loss: 4.8754...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5788...  Training loss: 4.8580...  0.3392 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5789...  Training loss: 4.9288...  0.3427 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5790...  Training loss: 4.8573...  0.3446 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5791...  Training loss: 4.8135...  0.3443 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5792...  Training loss: 4.8759...  0.3423 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5793...  Training loss: 4.9582...  0.3442 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5794...  Training loss: 4.9686...  0.3414 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5795...  Training loss: 4.9496...  0.3387 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5796...  Training loss: 4.7267...  0.3394 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5797...  Training loss: 4.9124...  0.3439 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5798...  Training loss: 4.9106...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5799...  Training loss: 4.8711...  0.3422 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5800...  Training loss: 4.8459...  0.3430 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5801...  Training loss: 4.7887...  0.3404 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5802...  Training loss: 4.7694...  0.3401 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5803...  Training loss: 4.7947...  0.3400 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5804...  Training loss: 4.7783...  0.3404 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5805...  Training loss: 4.7886...  0.3431 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5806...  Training loss: 4.7850...  0.3444 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5807...  Training loss: 4.9087...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5808...  Training loss: 4.7978...  0.3427 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5809...  Training loss: 4.8805...  0.3435 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5810...  Training loss: 4.9219...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5811...  Training loss: 4.8800...  0.3409 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5812...  Training loss: 4.7141...  0.3404 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5813...  Training loss: 4.7193...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5814...  Training loss: 4.7441...  0.3392 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5815...  Training loss: 4.7452...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5816...  Training loss: 4.8162...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5817...  Training loss: 4.7993...  0.3420 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5818...  Training loss: 4.7605...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5819...  Training loss: 4.7328...  0.3401 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5820...  Training loss: 4.7470...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5821...  Training loss: 4.8643...  0.3439 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5822...  Training loss: 4.7510...  0.3438 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5823...  Training loss: 4.7570...  0.3429 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5824...  Training loss: 4.7991...  0.3427 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5825...  Training loss: 4.9703...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5826...  Training loss: 4.8845...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5827...  Training loss: 4.8324...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5828...  Training loss: 4.8070...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5829...  Training loss: 4.7443...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5830...  Training loss: 4.7691...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5831...  Training loss: 4.8011...  0.3395 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5832...  Training loss: 4.8138...  0.3450 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5833...  Training loss: 4.9213...  0.3423 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5834...  Training loss: 4.8200...  0.3453 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5835...  Training loss: 4.8275...  0.3398 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5836...  Training loss: 4.8461...  0.3429 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5837...  Training loss: 4.8738...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5838...  Training loss: 4.6978...  0.3434 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5839...  Training loss: 4.7724...  0.3414 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5840...  Training loss: 4.7174...  0.3442 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5841...  Training loss: 4.8912...  0.3426 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5842...  Training loss: 4.7540...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5843...  Training loss: 4.8290...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5844...  Training loss: 5.0001...  0.3403 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5845...  Training loss: 5.0323...  0.3437 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5846...  Training loss: 4.9380...  0.3395 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5847...  Training loss: 4.7692...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5848...  Training loss: 4.7384...  0.3400 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5849...  Training loss: 4.7720...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/100...  Training Step: 5850...  Training loss: 4.8089...  0.3435 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5851...  Training loss: 4.8872...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5852...  Training loss: 4.8435...  0.3387 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5853...  Training loss: 4.8644...  0.3448 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5854...  Training loss: 4.7943...  0.3456 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5855...  Training loss: 4.8431...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5856...  Training loss: 4.7107...  0.3425 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5857...  Training loss: 4.8428...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5858...  Training loss: 4.7995...  0.3409 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5859...  Training loss: 4.8725...  0.3403 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5860...  Training loss: 4.8440...  0.3435 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5861...  Training loss: 4.9005...  0.3400 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5862...  Training loss: 4.9105...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5863...  Training loss: 4.7719...  0.3402 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5864...  Training loss: 4.7925...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5865...  Training loss: 4.8046...  0.3437 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5866...  Training loss: 4.8664...  0.3408 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5867...  Training loss: 4.8469...  0.3406 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5868...  Training loss: 4.8497...  0.3439 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5869...  Training loss: 4.9646...  0.3420 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5870...  Training loss: 4.8885...  0.3416 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5871...  Training loss: 4.8264...  0.3411 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5872...  Training loss: 4.8331...  0.3400 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5873...  Training loss: 4.8522...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5874...  Training loss: 4.8156...  0.3410 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5875...  Training loss: 4.7938...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5876...  Training loss: 4.7966...  0.3424 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5877...  Training loss: 4.8417...  0.3423 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5878...  Training loss: 4.7942...  0.3431 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5879...  Training loss: 4.7970...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5880...  Training loss: 4.7595...  0.3418 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5881...  Training loss: 4.8032...  0.3418 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5882...  Training loss: 4.7882...  0.3401 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5883...  Training loss: 4.7706...  0.3412 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5884...  Training loss: 4.7741...  0.3414 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5885...  Training loss: 5.1010...  0.3407 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5886...  Training loss: 4.9769...  0.3413 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5887...  Training loss: 5.0116...  0.3405 sec/batch\n",
      "Epoch: 32/100...  Training Step: 5888...  Training loss: 4.9352...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5889...  Training loss: 5.0393...  0.3406 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5890...  Training loss: 5.0178...  0.3415 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5891...  Training loss: 4.8614...  0.3391 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5892...  Training loss: 4.8594...  0.3439 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5893...  Training loss: 4.8369...  0.3387 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5894...  Training loss: 4.7398...  0.3397 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5895...  Training loss: 4.9198...  0.3417 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5896...  Training loss: 4.8724...  0.3443 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5897...  Training loss: 4.8751...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5898...  Training loss: 4.8325...  0.3412 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5899...  Training loss: 4.8102...  0.3416 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5900...  Training loss: 4.7681...  0.3407 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5901...  Training loss: 4.7724...  0.3419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5902...  Training loss: 4.8605...  0.3406 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5903...  Training loss: 4.8609...  0.3406 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5904...  Training loss: 4.8210...  0.3405 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5905...  Training loss: 4.8422...  0.3403 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5906...  Training loss: 4.7681...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5907...  Training loss: 4.8684...  0.3421 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5908...  Training loss: 4.8092...  0.3404 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5909...  Training loss: 4.7862...  0.3426 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5910...  Training loss: 4.7456...  0.3437 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5911...  Training loss: 4.7494...  0.3421 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5912...  Training loss: 4.7802...  0.3408 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5913...  Training loss: 4.8276...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5914...  Training loss: 4.7640...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5915...  Training loss: 4.7769...  0.3396 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5916...  Training loss: 4.7698...  0.3419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5917...  Training loss: 4.7970...  0.3419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5918...  Training loss: 4.8530...  0.3391 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5919...  Training loss: 4.8215...  0.3414 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5920...  Training loss: 4.7737...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5921...  Training loss: 4.5361...  0.3441 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5922...  Training loss: 4.6265...  0.3416 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5923...  Training loss: 4.7705...  0.3438 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5924...  Training loss: 4.9161...  0.3442 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5925...  Training loss: 4.8247...  0.3459 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5926...  Training loss: 4.8704...  0.3462 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5927...  Training loss: 4.8002...  0.3412 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5928...  Training loss: 4.8245...  0.3440 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5929...  Training loss: 4.8955...  0.3430 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5930...  Training loss: 4.8636...  0.3424 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5931...  Training loss: 4.9218...  0.3450 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5932...  Training loss: 4.8799...  0.3454 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5933...  Training loss: 4.8124...  0.3430 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5934...  Training loss: 4.7838...  0.3446 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5935...  Training loss: 4.8614...  0.3426 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5936...  Training loss: 4.7304...  0.3434 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5937...  Training loss: 4.8162...  0.3402 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5938...  Training loss: 4.9166...  0.3429 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5939...  Training loss: 4.8743...  0.3445 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5940...  Training loss: 4.8128...  0.3439 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5941...  Training loss: 4.8732...  0.3460 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5942...  Training loss: 4.8827...  0.3431 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5943...  Training loss: 4.8739...  0.3435 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5944...  Training loss: 4.8200...  0.3424 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5945...  Training loss: 4.7609...  0.3450 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5946...  Training loss: 4.8508...  0.3449 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/100...  Training Step: 5947...  Training loss: 4.7044...  0.3454 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5948...  Training loss: 4.7308...  0.3455 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5949...  Training loss: 4.7460...  0.3445 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5950...  Training loss: 4.8046...  0.3458 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5951...  Training loss: 4.8501...  0.3422 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5952...  Training loss: 4.8643...  0.3422 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5953...  Training loss: 4.7532...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5954...  Training loss: 4.7606...  0.3444 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5955...  Training loss: 4.8002...  0.3411 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5956...  Training loss: 4.7172...  0.3437 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5957...  Training loss: 4.8767...  0.3443 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5958...  Training loss: 4.8265...  0.3451 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5959...  Training loss: 4.8085...  0.3442 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5960...  Training loss: 4.6835...  0.3449 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5961...  Training loss: 4.7151...  0.3420 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5962...  Training loss: 4.7378...  0.3413 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5963...  Training loss: 4.7290...  0.3414 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5964...  Training loss: 4.7241...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5965...  Training loss: 4.7987...  0.3432 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5966...  Training loss: 4.7874...  0.3433 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5967...  Training loss: 4.8408...  0.3435 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5968...  Training loss: 4.9088...  0.3451 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5969...  Training loss: 4.9110...  0.3419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5970...  Training loss: 4.8597...  0.3422 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5971...  Training loss: 4.8640...  0.3457 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5972...  Training loss: 4.8271...  0.3432 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5973...  Training loss: 4.8993...  0.3449 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5974...  Training loss: 4.8399...  0.3426 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5975...  Training loss: 4.7835...  0.3427 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5976...  Training loss: 4.8421...  0.3435 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5977...  Training loss: 4.9075...  0.3441 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5978...  Training loss: 4.9204...  0.3403 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5979...  Training loss: 4.8812...  0.3418 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5980...  Training loss: 4.7067...  0.3435 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5981...  Training loss: 4.8848...  0.3411 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5982...  Training loss: 4.8821...  0.3432 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5983...  Training loss: 4.8609...  0.3420 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5984...  Training loss: 4.8225...  0.3414 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5985...  Training loss: 4.7781...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5986...  Training loss: 4.7388...  0.3403 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5987...  Training loss: 4.7856...  0.3395 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5988...  Training loss: 4.7416...  0.3412 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5989...  Training loss: 4.7795...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5990...  Training loss: 4.7608...  0.3426 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5991...  Training loss: 4.8973...  0.3432 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5992...  Training loss: 4.7869...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5993...  Training loss: 4.8520...  0.3396 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5994...  Training loss: 4.8806...  0.3411 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5995...  Training loss: 4.8286...  0.3418 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5996...  Training loss: 4.7185...  0.3390 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5997...  Training loss: 4.7003...  0.3400 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5998...  Training loss: 4.7294...  0.3391 sec/batch\n",
      "Epoch: 33/100...  Training Step: 5999...  Training loss: 4.7280...  0.3421 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6000...  Training loss: 4.7978...  0.3416 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6001...  Training loss: 4.7686...  0.3935 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6002...  Training loss: 4.7667...  0.3458 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6003...  Training loss: 4.7254...  0.3383 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6004...  Training loss: 4.7251...  0.3382 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6005...  Training loss: 4.8539...  0.3374 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6006...  Training loss: 4.7281...  0.3395 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6007...  Training loss: 4.7298...  0.3394 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6008...  Training loss: 4.7768...  0.3391 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6009...  Training loss: 4.9286...  0.3392 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6010...  Training loss: 4.8677...  0.3398 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6011...  Training loss: 4.7900...  0.3424 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6012...  Training loss: 4.7706...  0.3389 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6013...  Training loss: 4.7221...  0.3418 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6014...  Training loss: 4.7477...  0.3421 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6015...  Training loss: 4.7759...  0.3382 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6016...  Training loss: 4.7815...  0.3419 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6017...  Training loss: 4.8798...  0.3387 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6018...  Training loss: 4.7943...  0.3386 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6019...  Training loss: 4.8005...  0.3377 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6020...  Training loss: 4.8052...  0.3404 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6021...  Training loss: 4.8441...  0.3414 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6022...  Training loss: 4.6726...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6023...  Training loss: 4.7603...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6024...  Training loss: 4.6752...  0.3395 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6025...  Training loss: 4.8401...  0.3402 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6026...  Training loss: 4.7199...  0.3388 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6027...  Training loss: 4.8176...  0.3396 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6028...  Training loss: 4.9618...  0.3396 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6029...  Training loss: 5.0011...  0.3417 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6030...  Training loss: 4.9162...  0.3411 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6031...  Training loss: 4.7374...  0.3394 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6032...  Training loss: 4.7155...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6033...  Training loss: 4.7248...  0.3398 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6034...  Training loss: 4.7869...  0.3403 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6035...  Training loss: 4.8561...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6036...  Training loss: 4.8282...  0.3406 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6037...  Training loss: 4.8211...  0.3409 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6038...  Training loss: 4.7641...  0.3391 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6039...  Training loss: 4.8133...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6040...  Training loss: 4.7043...  0.3408 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6041...  Training loss: 4.8116...  0.3401 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6042...  Training loss: 4.7813...  0.3398 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6043...  Training loss: 4.8318...  0.3385 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/100...  Training Step: 6044...  Training loss: 4.8001...  0.3407 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6045...  Training loss: 4.8438...  0.3376 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6046...  Training loss: 4.8786...  0.3378 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6047...  Training loss: 4.7364...  0.3411 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6048...  Training loss: 4.7676...  0.3372 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6049...  Training loss: 4.7875...  0.3386 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6050...  Training loss: 4.8277...  0.3398 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6051...  Training loss: 4.8146...  0.3392 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6052...  Training loss: 4.8147...  0.3417 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6053...  Training loss: 4.9311...  0.3388 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6054...  Training loss: 4.8424...  0.3415 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6055...  Training loss: 4.8012...  0.3393 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6056...  Training loss: 4.7942...  0.3400 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6057...  Training loss: 4.8080...  0.3406 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6058...  Training loss: 4.7794...  0.3428 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6059...  Training loss: 4.7778...  0.3395 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6060...  Training loss: 4.7554...  0.3376 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6061...  Training loss: 4.8153...  0.3407 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6062...  Training loss: 4.7696...  0.3383 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6063...  Training loss: 4.7668...  0.3387 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6064...  Training loss: 4.7519...  0.3415 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6065...  Training loss: 4.8000...  0.3386 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6066...  Training loss: 4.7716...  0.3410 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6067...  Training loss: 4.7309...  0.3389 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6068...  Training loss: 4.7431...  0.3376 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6069...  Training loss: 5.0505...  0.3424 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6070...  Training loss: 4.9307...  0.3402 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6071...  Training loss: 4.9541...  0.3421 sec/batch\n",
      "Epoch: 33/100...  Training Step: 6072...  Training loss: 4.8942...  0.3412 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6073...  Training loss: 5.0335...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6074...  Training loss: 4.9890...  0.3410 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6075...  Training loss: 4.8332...  0.3422 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6076...  Training loss: 4.8463...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6077...  Training loss: 4.8117...  0.3384 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6078...  Training loss: 4.7337...  0.3396 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6079...  Training loss: 4.9015...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6080...  Training loss: 4.8427...  0.3381 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6081...  Training loss: 4.8434...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6082...  Training loss: 4.8096...  0.3389 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6083...  Training loss: 4.7775...  0.3395 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6084...  Training loss: 4.7425...  0.3385 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6085...  Training loss: 4.7362...  0.3386 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6086...  Training loss: 4.8410...  0.3396 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6087...  Training loss: 4.8333...  0.3398 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6088...  Training loss: 4.7883...  0.3424 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6089...  Training loss: 4.8506...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6090...  Training loss: 4.7248...  0.3413 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6091...  Training loss: 4.8368...  0.3408 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6092...  Training loss: 4.7570...  0.3405 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6093...  Training loss: 4.7321...  0.3426 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6094...  Training loss: 4.7025...  0.3392 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6095...  Training loss: 4.7439...  0.3372 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6096...  Training loss: 4.7635...  0.3436 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6097...  Training loss: 4.7931...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6098...  Training loss: 4.7389...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6099...  Training loss: 4.7496...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6100...  Training loss: 4.7470...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6101...  Training loss: 4.7941...  0.3419 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6102...  Training loss: 4.8201...  0.3389 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6103...  Training loss: 4.7811...  0.3380 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6104...  Training loss: 4.7236...  0.3398 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6105...  Training loss: 4.5038...  0.3390 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6106...  Training loss: 4.6213...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6107...  Training loss: 4.7516...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6108...  Training loss: 4.8976...  0.3409 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6109...  Training loss: 4.8257...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6110...  Training loss: 4.8485...  0.3421 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6111...  Training loss: 4.7823...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6112...  Training loss: 4.8011...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6113...  Training loss: 4.8869...  0.3425 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6114...  Training loss: 4.8535...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6115...  Training loss: 4.9136...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6116...  Training loss: 4.8858...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6117...  Training loss: 4.8213...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6118...  Training loss: 4.7721...  0.3412 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6119...  Training loss: 4.8492...  0.3428 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6120...  Training loss: 4.6989...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6121...  Training loss: 4.7735...  0.3434 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6122...  Training loss: 4.8956...  0.3389 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6123...  Training loss: 4.8475...  0.3393 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6124...  Training loss: 4.8083...  0.3390 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6125...  Training loss: 4.8309...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6126...  Training loss: 4.8878...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6127...  Training loss: 4.8636...  0.3385 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6128...  Training loss: 4.8069...  0.3409 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6129...  Training loss: 4.7313...  0.3403 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6130...  Training loss: 4.8339...  0.3407 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6131...  Training loss: 4.7074...  0.3387 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6132...  Training loss: 4.7273...  0.3396 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6133...  Training loss: 4.7365...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6134...  Training loss: 4.7923...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6135...  Training loss: 4.8002...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6136...  Training loss: 4.8251...  0.3374 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6137...  Training loss: 4.7182...  0.3409 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6138...  Training loss: 4.7306...  0.3401 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6139...  Training loss: 4.7637...  0.3421 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6140...  Training loss: 4.6820...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100...  Training Step: 6141...  Training loss: 4.8529...  0.3366 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6142...  Training loss: 4.7789...  0.3422 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6143...  Training loss: 4.7841...  0.3423 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6144...  Training loss: 4.6303...  0.3419 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6145...  Training loss: 4.6648...  0.3421 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6146...  Training loss: 4.7030...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6147...  Training loss: 4.6866...  0.3408 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6148...  Training loss: 4.7092...  0.3416 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6149...  Training loss: 4.7593...  0.3372 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6150...  Training loss: 4.7508...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6151...  Training loss: 4.8285...  0.3383 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6152...  Training loss: 4.9163...  0.3385 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6153...  Training loss: 4.9193...  0.3392 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6154...  Training loss: 4.8575...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6155...  Training loss: 4.8354...  0.3401 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6156...  Training loss: 4.8242...  0.3418 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6157...  Training loss: 4.8731...  0.3411 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6158...  Training loss: 4.8262...  0.3402 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6159...  Training loss: 4.7613...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6160...  Training loss: 4.8217...  0.3426 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6161...  Training loss: 4.8557...  0.3420 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6162...  Training loss: 4.8804...  0.3428 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6163...  Training loss: 4.8517...  0.3423 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6164...  Training loss: 4.6958...  0.3434 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6165...  Training loss: 4.8234...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6166...  Training loss: 4.8678...  0.3376 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6167...  Training loss: 4.8353...  0.3383 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6168...  Training loss: 4.8209...  0.3377 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6169...  Training loss: 4.7592...  0.3434 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6170...  Training loss: 4.7278...  0.3381 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6171...  Training loss: 4.7658...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6172...  Training loss: 4.7279...  0.3409 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6173...  Training loss: 4.7607...  0.3409 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6174...  Training loss: 4.7179...  0.3400 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6175...  Training loss: 4.8599...  0.3403 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6176...  Training loss: 4.7462...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6177...  Training loss: 4.8074...  0.3384 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6178...  Training loss: 4.8473...  0.3430 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6179...  Training loss: 4.8354...  0.3381 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6180...  Training loss: 4.6832...  0.3392 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6181...  Training loss: 4.6886...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6182...  Training loss: 4.7317...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6183...  Training loss: 4.7144...  0.3425 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6184...  Training loss: 4.7786...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6185...  Training loss: 4.7622...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6186...  Training loss: 4.7250...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6187...  Training loss: 4.7094...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6188...  Training loss: 4.7032...  0.3424 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6189...  Training loss: 4.8322...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6190...  Training loss: 4.7029...  0.3403 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6191...  Training loss: 4.7390...  0.3389 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6192...  Training loss: 4.7620...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6193...  Training loss: 4.9176...  0.3413 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6194...  Training loss: 4.8517...  0.3415 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6195...  Training loss: 4.8287...  0.3399 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6196...  Training loss: 4.7444...  0.3390 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6197...  Training loss: 4.7169...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6198...  Training loss: 4.7483...  0.3401 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6199...  Training loss: 4.7661...  0.3417 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6200...  Training loss: 4.7992...  0.3412 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6201...  Training loss: 4.8945...  0.3437 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6202...  Training loss: 4.7796...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6203...  Training loss: 4.7858...  0.3387 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6204...  Training loss: 4.7738...  0.3401 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6205...  Training loss: 4.8219...  0.3398 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6206...  Training loss: 4.6530...  0.3427 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6207...  Training loss: 4.7325...  0.3382 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6208...  Training loss: 4.6788...  0.3431 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6209...  Training loss: 4.8355...  0.3408 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6210...  Training loss: 4.7069...  0.3378 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6211...  Training loss: 4.7971...  0.3386 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6212...  Training loss: 4.9549...  0.3434 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6213...  Training loss: 4.9739...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6214...  Training loss: 4.8818...  0.3407 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6215...  Training loss: 4.7406...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6216...  Training loss: 4.6866...  0.3386 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6217...  Training loss: 4.7348...  0.3397 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6218...  Training loss: 4.7628...  0.3387 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6219...  Training loss: 4.8327...  0.3436 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6220...  Training loss: 4.8030...  0.3401 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6221...  Training loss: 4.8212...  0.3426 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6222...  Training loss: 4.7472...  0.3408 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6223...  Training loss: 4.7950...  0.3414 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6224...  Training loss: 4.6736...  0.3420 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6225...  Training loss: 4.8106...  0.3434 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6226...  Training loss: 4.7762...  0.3393 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6227...  Training loss: 4.8173...  0.3418 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6228...  Training loss: 4.7855...  0.3416 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6229...  Training loss: 4.8340...  0.3385 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6230...  Training loss: 4.8805...  0.3388 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6231...  Training loss: 4.7203...  0.3378 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6232...  Training loss: 4.7765...  0.3403 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6233...  Training loss: 4.7694...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6234...  Training loss: 4.7954...  0.3406 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6235...  Training loss: 4.7694...  0.3371 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6236...  Training loss: 4.7751...  0.3381 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6237...  Training loss: 4.9020...  0.3384 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/100...  Training Step: 6238...  Training loss: 4.8280...  0.3419 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6239...  Training loss: 4.7561...  0.3414 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6240...  Training loss: 4.7777...  0.3377 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6241...  Training loss: 4.7860...  0.3384 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6242...  Training loss: 4.7556...  0.3392 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6243...  Training loss: 4.7461...  0.3404 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6244...  Training loss: 4.7409...  0.3422 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6245...  Training loss: 4.7744...  0.3400 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6246...  Training loss: 4.7478...  0.3391 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6247...  Training loss: 4.7249...  0.3380 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6248...  Training loss: 4.7000...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6249...  Training loss: 4.7829...  0.3386 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6250...  Training loss: 4.7402...  0.3424 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6251...  Training loss: 4.7071...  0.3413 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6252...  Training loss: 4.7143...  0.3396 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6253...  Training loss: 4.9997...  0.3405 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6254...  Training loss: 4.8903...  0.3394 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6255...  Training loss: 4.9077...  0.3388 sec/batch\n",
      "Epoch: 34/100...  Training Step: 6256...  Training loss: 4.8619...  0.3381 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6257...  Training loss: 4.9841...  0.3386 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6258...  Training loss: 4.9284...  0.3394 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6259...  Training loss: 4.7969...  0.3422 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6260...  Training loss: 4.8106...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6261...  Training loss: 4.8010...  0.3399 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6262...  Training loss: 4.7279...  0.3413 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6263...  Training loss: 4.8950...  0.3398 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6264...  Training loss: 4.8143...  0.3410 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6265...  Training loss: 4.8058...  0.3421 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6266...  Training loss: 4.7930...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6267...  Training loss: 4.7491...  0.3390 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6268...  Training loss: 4.7170...  0.3386 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6269...  Training loss: 4.7166...  0.3396 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6270...  Training loss: 4.7755...  0.3408 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6271...  Training loss: 4.7845...  0.3385 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6272...  Training loss: 4.7352...  0.3416 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6273...  Training loss: 4.8242...  0.3396 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6274...  Training loss: 4.7010...  0.3379 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6275...  Training loss: 4.7954...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6276...  Training loss: 4.7402...  0.3413 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6277...  Training loss: 4.7183...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6278...  Training loss: 4.6839...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6279...  Training loss: 4.6907...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6280...  Training loss: 4.7311...  0.3418 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6281...  Training loss: 4.7783...  0.3412 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6282...  Training loss: 4.7271...  0.3417 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6283...  Training loss: 4.7344...  0.3414 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6284...  Training loss: 4.7250...  0.3427 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6285...  Training loss: 4.7569...  0.3433 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6286...  Training loss: 4.8091...  0.3399 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6287...  Training loss: 4.7723...  0.3420 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6288...  Training loss: 4.7035...  0.3429 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6289...  Training loss: 4.4901...  0.3374 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6290...  Training loss: 4.5822...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6291...  Training loss: 4.7248...  0.3381 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6292...  Training loss: 4.8669...  0.3411 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6293...  Training loss: 4.8073...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6294...  Training loss: 4.8410...  0.3395 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6295...  Training loss: 4.7539...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6296...  Training loss: 4.7707...  0.3407 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6297...  Training loss: 4.8463...  0.3426 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6298...  Training loss: 4.8225...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6299...  Training loss: 4.8837...  0.3437 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6300...  Training loss: 4.8511...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6301...  Training loss: 4.7964...  0.3428 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6302...  Training loss: 4.7579...  0.3403 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6303...  Training loss: 4.8223...  0.3381 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6304...  Training loss: 4.6983...  0.3408 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6305...  Training loss: 4.7664...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6306...  Training loss: 4.8913...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6307...  Training loss: 4.8240...  0.3408 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6308...  Training loss: 4.7690...  0.3394 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6309...  Training loss: 4.8152...  0.3428 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6310...  Training loss: 4.8392...  0.3431 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6311...  Training loss: 4.8232...  0.3427 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6312...  Training loss: 4.7816...  0.3456 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6313...  Training loss: 4.7121...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6314...  Training loss: 4.8133...  0.3402 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6315...  Training loss: 4.6965...  0.3400 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6316...  Training loss: 4.6999...  0.3408 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6317...  Training loss: 4.7303...  0.3411 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6318...  Training loss: 4.7632...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6319...  Training loss: 4.7997...  0.3389 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6320...  Training loss: 4.7896...  0.3415 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6321...  Training loss: 4.7077...  0.3412 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6322...  Training loss: 4.6891...  0.3379 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6323...  Training loss: 4.7234...  0.3418 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6324...  Training loss: 4.6560...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6325...  Training loss: 4.8044...  0.3369 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6326...  Training loss: 4.7564...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6327...  Training loss: 4.7562...  0.3388 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6328...  Training loss: 4.6107...  0.3402 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6329...  Training loss: 4.6418...  0.3397 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6330...  Training loss: 4.6895...  0.3384 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6331...  Training loss: 4.6529...  0.3400 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6332...  Training loss: 4.6624...  0.3395 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6333...  Training loss: 4.7351...  0.3399 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6334...  Training loss: 4.7299...  0.3388 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/100...  Training Step: 6335...  Training loss: 4.7877...  0.3376 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6336...  Training loss: 4.8499...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6337...  Training loss: 4.8641...  0.3379 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6338...  Training loss: 4.8359...  0.3429 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6339...  Training loss: 4.8132...  0.3416 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6340...  Training loss: 4.7802...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6341...  Training loss: 4.8183...  0.3405 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6342...  Training loss: 4.7811...  0.3382 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6343...  Training loss: 4.7537...  0.3430 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6344...  Training loss: 4.7742...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6345...  Training loss: 4.8226...  0.3422 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6346...  Training loss: 4.8554...  0.3395 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6347...  Training loss: 4.8115...  0.3434 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6348...  Training loss: 4.6394...  0.3422 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6349...  Training loss: 4.7982...  0.3416 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6350...  Training loss: 4.8184...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6351...  Training loss: 4.8233...  0.3382 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6352...  Training loss: 4.7767...  0.3389 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6353...  Training loss: 4.7268...  0.3396 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6354...  Training loss: 4.7141...  0.3399 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6355...  Training loss: 4.7310...  0.3406 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6356...  Training loss: 4.7107...  0.3412 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6357...  Training loss: 4.7385...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6358...  Training loss: 4.7009...  0.3386 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6359...  Training loss: 4.8418...  0.3397 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6360...  Training loss: 4.7225...  0.3404 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6361...  Training loss: 4.7742...  0.3413 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6362...  Training loss: 4.7950...  0.3376 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6363...  Training loss: 4.7742...  0.3391 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6364...  Training loss: 4.6461...  0.3397 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6365...  Training loss: 4.6558...  0.3420 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6366...  Training loss: 4.6902...  0.3399 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6367...  Training loss: 4.6884...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6368...  Training loss: 4.7419...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6369...  Training loss: 4.7187...  0.3384 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6370...  Training loss: 4.6814...  0.3427 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6371...  Training loss: 4.6815...  0.3392 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6372...  Training loss: 4.6718...  0.3440 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6373...  Training loss: 4.8052...  0.3436 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6374...  Training loss: 4.6948...  0.3429 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6375...  Training loss: 4.7135...  0.3451 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6376...  Training loss: 4.7147...  0.3447 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6377...  Training loss: 4.9008...  0.3426 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6378...  Training loss: 4.8218...  0.3437 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6379...  Training loss: 4.7699...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6380...  Training loss: 4.7246...  0.3421 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6381...  Training loss: 4.6967...  0.3411 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6382...  Training loss: 4.7138...  0.3450 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6383...  Training loss: 4.7304...  0.3408 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6384...  Training loss: 4.7539...  0.3403 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6385...  Training loss: 4.8278...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6386...  Training loss: 4.7693...  0.3413 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6387...  Training loss: 4.7636...  0.3430 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6388...  Training loss: 4.7565...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6389...  Training loss: 4.7820...  0.3400 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6390...  Training loss: 4.6187...  0.3416 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6391...  Training loss: 4.7066...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6392...  Training loss: 4.6437...  0.3428 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6393...  Training loss: 4.8009...  0.3388 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6394...  Training loss: 4.6723...  0.3404 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6395...  Training loss: 4.7502...  0.3394 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6396...  Training loss: 4.8682...  0.3436 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6397...  Training loss: 4.9150...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6398...  Training loss: 4.8580...  0.3421 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6399...  Training loss: 4.6966...  0.3393 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6400...  Training loss: 4.6661...  0.3402 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6401...  Training loss: 4.7081...  0.3425 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6402...  Training loss: 4.7228...  0.3394 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6403...  Training loss: 4.7883...  0.3436 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6404...  Training loss: 4.7709...  0.3452 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6405...  Training loss: 4.7939...  0.3418 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6406...  Training loss: 4.7178...  0.3388 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6407...  Training loss: 4.7708...  0.3433 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6408...  Training loss: 4.6791...  0.3426 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6409...  Training loss: 4.7719...  0.3401 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6410...  Training loss: 4.7439...  0.3427 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6411...  Training loss: 4.8156...  0.3403 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6412...  Training loss: 4.7698...  0.3398 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6413...  Training loss: 4.8097...  0.3417 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6414...  Training loss: 4.8270...  0.3421 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6415...  Training loss: 4.7035...  0.3424 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6416...  Training loss: 4.7345...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6417...  Training loss: 4.7392...  0.3429 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6418...  Training loss: 4.7851...  0.3442 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6419...  Training loss: 4.7578...  0.3410 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6420...  Training loss: 4.7723...  0.3395 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6421...  Training loss: 4.8615...  0.3446 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6422...  Training loss: 4.7907...  0.3435 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6423...  Training loss: 4.7527...  0.3417 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6424...  Training loss: 4.7329...  0.3436 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6425...  Training loss: 4.7528...  0.3439 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6426...  Training loss: 4.7372...  0.3421 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6427...  Training loss: 4.7275...  0.3397 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6428...  Training loss: 4.7135...  0.3432 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6429...  Training loss: 4.7708...  0.3441 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6430...  Training loss: 4.7216...  0.3433 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6431...  Training loss: 4.6883...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/100...  Training Step: 6432...  Training loss: 4.6781...  0.3418 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6433...  Training loss: 4.7329...  0.3433 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6434...  Training loss: 4.7198...  0.3433 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6435...  Training loss: 4.7015...  0.3405 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6436...  Training loss: 4.6747...  0.3407 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6437...  Training loss: 4.9278...  0.3390 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6438...  Training loss: 4.8556...  0.3431 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6439...  Training loss: 4.8767...  0.3394 sec/batch\n",
      "Epoch: 35/100...  Training Step: 6440...  Training loss: 4.8307...  0.3432 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6441...  Training loss: 4.9328...  0.3427 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6442...  Training loss: 4.8703...  0.3425 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6443...  Training loss: 4.7453...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6444...  Training loss: 4.8112...  0.3404 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6445...  Training loss: 4.7638...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6446...  Training loss: 4.6810...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6447...  Training loss: 4.8663...  0.3409 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6448...  Training loss: 4.8040...  0.3434 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6449...  Training loss: 4.7921...  0.3438 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6450...  Training loss: 4.7826...  0.3417 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6451...  Training loss: 4.7441...  0.3420 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6452...  Training loss: 4.7192...  0.3424 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6453...  Training loss: 4.6848...  0.3405 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6454...  Training loss: 4.7517...  0.3400 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6455...  Training loss: 4.7497...  0.3408 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6456...  Training loss: 4.6897...  0.3404 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6457...  Training loss: 4.7682...  0.3400 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6458...  Training loss: 4.6901...  0.3410 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6459...  Training loss: 4.7525...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6460...  Training loss: 4.6715...  0.3406 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6461...  Training loss: 4.6708...  0.3404 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6462...  Training loss: 4.6492...  0.3427 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6463...  Training loss: 4.6540...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6464...  Training loss: 4.6930...  0.3421 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6465...  Training loss: 4.7453...  0.3377 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6466...  Training loss: 4.6948...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6467...  Training loss: 4.6894...  0.3409 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6468...  Training loss: 4.7196...  0.3376 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6469...  Training loss: 4.7423...  0.3394 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6470...  Training loss: 4.7946...  0.3403 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6471...  Training loss: 4.7460...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6472...  Training loss: 4.6984...  0.3372 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6473...  Training loss: 4.4641...  0.3391 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6474...  Training loss: 4.5629...  0.3431 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6475...  Training loss: 4.6902...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6476...  Training loss: 4.8276...  0.3424 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6477...  Training loss: 4.7821...  0.3397 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6478...  Training loss: 4.7979...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6479...  Training loss: 4.7509...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6480...  Training loss: 4.7909...  0.3416 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6481...  Training loss: 4.8421...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6482...  Training loss: 4.8108...  0.3380 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6483...  Training loss: 4.8661...  0.3424 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6484...  Training loss: 4.8325...  0.3425 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6485...  Training loss: 4.7709...  0.3420 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6486...  Training loss: 4.7155...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6487...  Training loss: 4.8042...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6488...  Training loss: 4.6840...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6489...  Training loss: 4.7490...  0.3397 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6490...  Training loss: 4.8583...  0.3413 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6491...  Training loss: 4.8173...  0.3418 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6492...  Training loss: 4.7633...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6493...  Training loss: 4.8058...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6494...  Training loss: 4.8262...  0.3394 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6495...  Training loss: 4.8062...  0.3383 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6496...  Training loss: 4.7748...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6497...  Training loss: 4.6921...  0.3417 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6498...  Training loss: 4.7572...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6499...  Training loss: 4.6429...  0.3428 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6500...  Training loss: 4.6399...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6501...  Training loss: 4.6706...  0.3412 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6502...  Training loss: 4.7259...  0.3396 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6503...  Training loss: 4.7648...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6504...  Training loss: 4.7691...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6505...  Training loss: 4.6830...  0.3395 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6506...  Training loss: 4.6814...  0.3431 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6507...  Training loss: 4.7165...  0.3416 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6508...  Training loss: 4.6391...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6509...  Training loss: 4.7928...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6510...  Training loss: 4.7392...  0.3416 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6511...  Training loss: 4.7203...  0.3407 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6512...  Training loss: 4.5920...  0.3384 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6513...  Training loss: 4.6370...  0.3407 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6514...  Training loss: 4.6841...  0.3371 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6515...  Training loss: 4.6315...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6516...  Training loss: 4.6495...  0.3403 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6517...  Training loss: 4.7358...  0.3394 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6518...  Training loss: 4.6920...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6519...  Training loss: 4.7565...  0.3388 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6520...  Training loss: 4.8153...  0.3404 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6521...  Training loss: 4.8657...  0.3406 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6522...  Training loss: 4.7903...  0.3385 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6523...  Training loss: 4.7691...  0.3426 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6524...  Training loss: 4.7587...  0.3396 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6525...  Training loss: 4.7888...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6526...  Training loss: 4.7573...  0.3375 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6527...  Training loss: 4.7125...  0.3425 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6528...  Training loss: 4.7500...  0.3435 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/100...  Training Step: 6529...  Training loss: 4.7935...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6530...  Training loss: 4.8428...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6531...  Training loss: 4.7663...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6532...  Training loss: 4.6235...  0.3406 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6533...  Training loss: 4.7659...  0.3377 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6534...  Training loss: 4.7738...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6535...  Training loss: 4.7429...  0.3416 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6536...  Training loss: 4.7256...  0.3400 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6537...  Training loss: 4.7096...  0.3395 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6538...  Training loss: 4.6792...  0.3401 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6539...  Training loss: 4.7152...  0.3378 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6540...  Training loss: 4.6790...  0.3424 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6541...  Training loss: 4.7029...  0.3420 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6542...  Training loss: 4.6699...  0.3401 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6543...  Training loss: 4.8028...  0.3434 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6544...  Training loss: 4.6734...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6545...  Training loss: 4.7683...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6546...  Training loss: 4.7698...  0.3410 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6547...  Training loss: 4.7336...  0.3437 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6548...  Training loss: 4.6105...  0.3415 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6549...  Training loss: 4.6118...  0.3407 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6550...  Training loss: 4.6597...  0.3395 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6551...  Training loss: 4.6521...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6552...  Training loss: 4.7186...  0.3392 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6553...  Training loss: 4.6876...  0.3392 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6554...  Training loss: 4.6627...  0.3392 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6555...  Training loss: 4.6628...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6556...  Training loss: 4.6405...  0.3401 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6557...  Training loss: 4.7817...  0.3394 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6558...  Training loss: 4.6692...  0.3412 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6559...  Training loss: 4.6802...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6560...  Training loss: 4.7155...  0.3420 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6561...  Training loss: 4.8865...  0.3397 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6562...  Training loss: 4.8319...  0.3385 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6563...  Training loss: 4.7614...  0.3398 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6564...  Training loss: 4.7197...  0.3421 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6565...  Training loss: 4.6872...  0.3430 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6566...  Training loss: 4.7076...  0.3386 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6567...  Training loss: 4.7155...  0.3428 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6568...  Training loss: 4.7422...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6569...  Training loss: 4.8115...  0.3400 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6570...  Training loss: 4.7569...  0.3397 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6571...  Training loss: 4.7396...  0.3380 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6572...  Training loss: 4.7244...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6573...  Training loss: 4.7767...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6574...  Training loss: 4.6197...  0.3385 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6575...  Training loss: 4.6818...  0.3405 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6576...  Training loss: 4.6103...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6577...  Training loss: 4.7556...  0.3410 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6578...  Training loss: 4.6482...  0.3403 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6579...  Training loss: 4.7146...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6580...  Training loss: 4.8533...  0.3407 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6581...  Training loss: 4.8863...  0.3405 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6582...  Training loss: 4.8285...  0.3409 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6583...  Training loss: 4.6695...  0.3423 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6584...  Training loss: 4.6445...  0.3434 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6585...  Training loss: 4.6595...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6586...  Training loss: 4.6855...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6587...  Training loss: 4.7664...  0.3417 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6588...  Training loss: 4.7524...  0.3405 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6589...  Training loss: 4.7646...  0.3385 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6590...  Training loss: 4.7028...  0.3392 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6591...  Training loss: 4.7511...  0.3408 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6592...  Training loss: 4.6470...  0.3432 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6593...  Training loss: 4.7631...  0.3437 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6594...  Training loss: 4.7345...  0.3385 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6595...  Training loss: 4.7838...  0.3405 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6596...  Training loss: 4.7313...  0.3389 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6597...  Training loss: 4.7939...  0.3401 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6598...  Training loss: 4.8405...  0.3423 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6599...  Training loss: 4.6765...  0.3415 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6600...  Training loss: 4.7043...  0.3409 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6601...  Training loss: 4.7102...  0.3402 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6602...  Training loss: 4.7660...  0.3388 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6603...  Training loss: 4.7369...  0.3395 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6604...  Training loss: 4.7224...  0.3425 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6605...  Training loss: 4.8287...  0.3387 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6606...  Training loss: 4.7576...  0.3414 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6607...  Training loss: 4.7099...  0.3407 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6608...  Training loss: 4.6954...  0.3418 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6609...  Training loss: 4.7443...  0.3425 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6610...  Training loss: 4.7203...  0.3415 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6611...  Training loss: 4.7036...  0.3430 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6612...  Training loss: 4.6844...  0.3415 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6613...  Training loss: 4.7094...  0.3426 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6614...  Training loss: 4.6777...  0.3392 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6615...  Training loss: 4.6845...  0.3397 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6616...  Training loss: 4.6785...  0.3427 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6617...  Training loss: 4.7078...  0.3411 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6618...  Training loss: 4.6920...  0.3393 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6619...  Training loss: 4.6642...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6620...  Training loss: 4.6391...  0.3390 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6621...  Training loss: 4.8981...  0.3399 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6622...  Training loss: 4.8093...  0.3409 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6623...  Training loss: 4.8186...  0.3410 sec/batch\n",
      "Epoch: 36/100...  Training Step: 6624...  Training loss: 4.8042...  0.3382 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6625...  Training loss: 4.8843...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/100...  Training Step: 6626...  Training loss: 4.8539...  0.3419 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6627...  Training loss: 4.7112...  0.3421 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6628...  Training loss: 4.7582...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6629...  Training loss: 4.7330...  0.3409 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6630...  Training loss: 4.6330...  0.3421 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6631...  Training loss: 4.8480...  0.3375 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6632...  Training loss: 4.7784...  0.3381 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6633...  Training loss: 4.7697...  0.3424 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6634...  Training loss: 4.7400...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6635...  Training loss: 4.7152...  0.3407 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6636...  Training loss: 4.7038...  0.3429 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6637...  Training loss: 4.6858...  0.3404 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6638...  Training loss: 4.7266...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6639...  Training loss: 4.7330...  0.3411 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6640...  Training loss: 4.6795...  0.3396 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6641...  Training loss: 4.7529...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6642...  Training loss: 4.6557...  0.3394 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6643...  Training loss: 4.7244...  0.3398 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6644...  Training loss: 4.6781...  0.3403 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6645...  Training loss: 4.6611...  0.3398 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6646...  Training loss: 4.6257...  0.3414 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6647...  Training loss: 4.6435...  0.3406 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6648...  Training loss: 4.6812...  0.3427 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6649...  Training loss: 4.7516...  0.3423 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6650...  Training loss: 4.6907...  0.3435 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6651...  Training loss: 4.7034...  0.3398 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6652...  Training loss: 4.6785...  0.3409 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6653...  Training loss: 4.7019...  0.3389 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6654...  Training loss: 4.7699...  0.3394 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6655...  Training loss: 4.7397...  0.3409 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6656...  Training loss: 4.6941...  0.3384 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6657...  Training loss: 4.4743...  0.3407 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6658...  Training loss: 4.5686...  0.3392 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6659...  Training loss: 4.6917...  0.3404 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6660...  Training loss: 4.8067...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6661...  Training loss: 4.7534...  0.3417 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6662...  Training loss: 4.7764...  0.3386 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6663...  Training loss: 4.7035...  0.3418 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6664...  Training loss: 4.7351...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6665...  Training loss: 4.7953...  0.3383 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6666...  Training loss: 4.7959...  0.3396 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6667...  Training loss: 4.8393...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6668...  Training loss: 4.8161...  0.3385 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6669...  Training loss: 4.7501...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6670...  Training loss: 4.7152...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6671...  Training loss: 4.7900...  0.3382 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6672...  Training loss: 4.6568...  0.3378 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6673...  Training loss: 4.7330...  0.3392 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6674...  Training loss: 4.8347...  0.3426 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6675...  Training loss: 4.8030...  0.3385 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6676...  Training loss: 4.7713...  0.3417 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6677...  Training loss: 4.7872...  0.3385 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6678...  Training loss: 4.8465...  0.3396 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6679...  Training loss: 4.8269...  0.3412 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6680...  Training loss: 4.7755...  0.3387 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6681...  Training loss: 4.6885...  0.3395 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6682...  Training loss: 4.7434...  0.3385 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6683...  Training loss: 4.6407...  0.3403 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6684...  Training loss: 4.6556...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6685...  Training loss: 4.6770...  0.3387 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6686...  Training loss: 4.7118...  0.3397 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6687...  Training loss: 4.7453...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6688...  Training loss: 4.7469...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6689...  Training loss: 4.6517...  0.3426 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6690...  Training loss: 4.6655...  0.3417 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6691...  Training loss: 4.6955...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6692...  Training loss: 4.6112...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6693...  Training loss: 4.7795...  0.3407 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6694...  Training loss: 4.7189...  0.3383 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6695...  Training loss: 4.6980...  0.3394 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6696...  Training loss: 4.5737...  0.3404 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6697...  Training loss: 4.6262...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6698...  Training loss: 4.6418...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6699...  Training loss: 4.6228...  0.3407 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6700...  Training loss: 4.6619...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6701...  Training loss: 4.6942...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6702...  Training loss: 4.6771...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6703...  Training loss: 4.7352...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6704...  Training loss: 4.8016...  0.3409 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6705...  Training loss: 4.8291...  0.3421 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6706...  Training loss: 4.7944...  0.3423 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6707...  Training loss: 4.7579...  0.3401 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6708...  Training loss: 4.7424...  0.3419 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6709...  Training loss: 4.7941...  0.3428 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6710...  Training loss: 4.7337...  0.3408 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6711...  Training loss: 4.6998...  0.3426 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6712...  Training loss: 4.7372...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6713...  Training loss: 4.7638...  0.3393 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6714...  Training loss: 4.8180...  0.3395 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6715...  Training loss: 4.7382...  0.3396 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6716...  Training loss: 4.5693...  0.3432 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6717...  Training loss: 4.7301...  0.3398 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6718...  Training loss: 4.7380...  0.3405 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6719...  Training loss: 4.7125...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6720...  Training loss: 4.7041...  0.3397 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6721...  Training loss: 4.6655...  0.3432 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6722...  Training loss: 4.6359...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/100...  Training Step: 6723...  Training loss: 4.6440...  0.3393 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6724...  Training loss: 4.6439...  0.3401 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6725...  Training loss: 4.6709...  0.3401 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6726...  Training loss: 4.6392...  0.3397 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6727...  Training loss: 4.7703...  0.3389 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6728...  Training loss: 4.6486...  0.3426 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6729...  Training loss: 4.7404...  0.3395 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6730...  Training loss: 4.7654...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6731...  Training loss: 4.7160...  0.3422 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6732...  Training loss: 4.6094...  0.3414 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6733...  Training loss: 4.6126...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6734...  Training loss: 4.6234...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6735...  Training loss: 4.6307...  0.3411 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6736...  Training loss: 4.6754...  0.3401 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6737...  Training loss: 4.6452...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6738...  Training loss: 4.6176...  0.3409 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6739...  Training loss: 4.6453...  0.3412 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6740...  Training loss: 4.6181...  0.3379 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6741...  Training loss: 4.7473...  0.3410 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6742...  Training loss: 4.6419...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6743...  Training loss: 4.6460...  0.3444 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6744...  Training loss: 4.6907...  0.3422 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6745...  Training loss: 4.8671...  0.3414 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6746...  Training loss: 4.7906...  0.3418 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6747...  Training loss: 4.7177...  0.3417 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6748...  Training loss: 4.6844...  0.3389 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6749...  Training loss: 4.6540...  0.3430 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6750...  Training loss: 4.7028...  0.3433 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6751...  Training loss: 4.6987...  0.3377 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6752...  Training loss: 4.7063...  0.3390 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6753...  Training loss: 4.8172...  0.3390 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6754...  Training loss: 4.7395...  0.3382 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6755...  Training loss: 4.7461...  0.3401 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6756...  Training loss: 4.7196...  0.3395 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6757...  Training loss: 4.7572...  0.3425 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6758...  Training loss: 4.5991...  0.3418 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6759...  Training loss: 4.6645...  0.3411 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6760...  Training loss: 4.6057...  0.3390 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6761...  Training loss: 4.7274...  0.3406 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6762...  Training loss: 4.6353...  0.3371 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6763...  Training loss: 4.7202...  0.3397 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6764...  Training loss: 4.8079...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6765...  Training loss: 4.8348...  0.3393 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6766...  Training loss: 4.7801...  0.3408 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6767...  Training loss: 4.6251...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6768...  Training loss: 4.6051...  0.3416 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6769...  Training loss: 4.6322...  0.3424 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6770...  Training loss: 4.6639...  0.3387 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6771...  Training loss: 4.7294...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6772...  Training loss: 4.7038...  0.3396 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6773...  Training loss: 4.7354...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6774...  Training loss: 4.6732...  0.3374 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6775...  Training loss: 4.6994...  0.3376 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6776...  Training loss: 4.6089...  0.3406 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6777...  Training loss: 4.7153...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6778...  Training loss: 4.6702...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6779...  Training loss: 4.7446...  0.3398 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6780...  Training loss: 4.7174...  0.3406 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6781...  Training loss: 4.7524...  0.3386 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6782...  Training loss: 4.7869...  0.3404 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6783...  Training loss: 4.6379...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6784...  Training loss: 4.6522...  0.3403 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6785...  Training loss: 4.6949...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6786...  Training loss: 4.7431...  0.3385 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6787...  Training loss: 4.7115...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6788...  Training loss: 4.6866...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6789...  Training loss: 4.8026...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6790...  Training loss: 4.7292...  0.3403 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6791...  Training loss: 4.6939...  0.3393 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6792...  Training loss: 4.6588...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6793...  Training loss: 4.6937...  0.3397 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6794...  Training loss: 4.6821...  0.3390 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6795...  Training loss: 4.6667...  0.3399 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6796...  Training loss: 4.6533...  0.3375 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6797...  Training loss: 4.6831...  0.3391 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6798...  Training loss: 4.6476...  0.3402 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6799...  Training loss: 4.6302...  0.3400 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6800...  Training loss: 4.6352...  0.3388 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6801...  Training loss: 4.6825...  0.3378 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6802...  Training loss: 4.6538...  0.3377 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6803...  Training loss: 4.6406...  0.3403 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6804...  Training loss: 4.6093...  0.3408 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6805...  Training loss: 4.8443...  0.3429 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6806...  Training loss: 4.7668...  0.3419 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6807...  Training loss: 4.7964...  0.3408 sec/batch\n",
      "Epoch: 37/100...  Training Step: 6808...  Training loss: 4.7442...  0.3391 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6809...  Training loss: 4.8560...  0.3411 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6810...  Training loss: 4.7623...  0.3407 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6811...  Training loss: 4.6675...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6812...  Training loss: 4.7261...  0.3378 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6813...  Training loss: 4.6978...  0.3377 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6814...  Training loss: 4.6015...  0.3387 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6815...  Training loss: 4.8056...  0.3405 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6816...  Training loss: 4.7303...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6817...  Training loss: 4.7415...  0.3421 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6818...  Training loss: 4.7136...  0.3416 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6819...  Training loss: 4.6838...  0.3383 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/100...  Training Step: 6820...  Training loss: 4.6779...  0.3399 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6821...  Training loss: 4.6578...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6822...  Training loss: 4.7203...  0.3384 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6823...  Training loss: 4.7280...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6824...  Training loss: 4.6677...  0.3384 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6825...  Training loss: 4.7385...  0.3390 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6826...  Training loss: 4.6535...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6827...  Training loss: 4.7303...  0.3411 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6828...  Training loss: 4.6526...  0.3401 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6829...  Training loss: 4.6088...  0.3415 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6830...  Training loss: 4.5811...  0.3421 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6831...  Training loss: 4.5754...  0.3386 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6832...  Training loss: 4.6099...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6833...  Training loss: 4.6725...  0.3404 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6834...  Training loss: 4.6412...  0.3405 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6835...  Training loss: 4.6664...  0.3401 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6836...  Training loss: 4.6861...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6837...  Training loss: 4.7234...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6838...  Training loss: 4.7766...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6839...  Training loss: 4.6972...  0.3429 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6840...  Training loss: 4.6694...  0.3411 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6841...  Training loss: 4.4651...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6842...  Training loss: 4.5586...  0.3414 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6843...  Training loss: 4.6675...  0.3423 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6844...  Training loss: 4.8021...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6845...  Training loss: 4.7583...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6846...  Training loss: 4.7713...  0.3391 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6847...  Training loss: 4.7054...  0.3409 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6848...  Training loss: 4.7222...  0.3397 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6849...  Training loss: 4.7652...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6850...  Training loss: 4.7567...  0.3423 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6851...  Training loss: 4.8118...  0.3405 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6852...  Training loss: 4.7748...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6853...  Training loss: 4.6977...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6854...  Training loss: 4.6654...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6855...  Training loss: 4.7569...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6856...  Training loss: 4.6256...  0.3404 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6857...  Training loss: 4.7081...  0.3406 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6858...  Training loss: 4.7806...  0.3382 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6859...  Training loss: 4.7526...  0.3384 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6860...  Training loss: 4.7242...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6861...  Training loss: 4.7643...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6862...  Training loss: 4.8076...  0.3414 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6863...  Training loss: 4.7857...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6864...  Training loss: 4.7336...  0.3403 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6865...  Training loss: 4.6687...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6866...  Training loss: 4.7570...  0.3403 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6867...  Training loss: 4.6270...  0.3379 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6868...  Training loss: 4.6422...  0.3411 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6869...  Training loss: 4.6696...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6870...  Training loss: 4.7093...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6871...  Training loss: 4.7136...  0.3410 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6872...  Training loss: 4.7154...  0.3385 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6873...  Training loss: 4.6403...  0.3414 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6874...  Training loss: 4.6340...  0.3429 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6875...  Training loss: 4.6599...  0.3403 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6876...  Training loss: 4.5901...  0.3416 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6877...  Training loss: 4.7380...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6878...  Training loss: 4.6802...  0.3381 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6879...  Training loss: 4.7013...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6880...  Training loss: 4.5491...  0.3380 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6881...  Training loss: 4.5814...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6882...  Training loss: 4.6278...  0.3412 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6883...  Training loss: 4.6142...  0.3415 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6884...  Training loss: 4.6572...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6885...  Training loss: 4.6936...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6886...  Training loss: 4.6515...  0.3386 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6887...  Training loss: 4.6833...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6888...  Training loss: 4.7608...  0.3379 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6889...  Training loss: 4.7983...  0.3387 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6890...  Training loss: 4.7391...  0.3437 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6891...  Training loss: 4.7240...  0.3381 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6892...  Training loss: 4.7184...  0.3448 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6893...  Training loss: 4.7657...  0.3422 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6894...  Training loss: 4.7308...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6895...  Training loss: 4.7077...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6896...  Training loss: 4.7073...  0.3419 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6897...  Training loss: 4.7771...  0.3404 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6898...  Training loss: 4.8048...  0.3393 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6899...  Training loss: 4.7343...  0.3374 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6900...  Training loss: 4.5762...  0.3391 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6901...  Training loss: 4.6863...  0.3373 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6902...  Training loss: 4.7149...  0.3415 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6903...  Training loss: 4.6830...  0.3386 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6904...  Training loss: 4.6926...  0.3409 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6905...  Training loss: 4.6357...  0.3373 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6906...  Training loss: 4.6045...  0.3393 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6907...  Training loss: 4.6402...  0.3404 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6908...  Training loss: 4.6124...  0.3416 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6909...  Training loss: 4.6399...  0.3419 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6910...  Training loss: 4.6266...  0.3441 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6911...  Training loss: 4.7581...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6912...  Training loss: 4.6634...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6913...  Training loss: 4.7359...  0.3405 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6914...  Training loss: 4.7461...  0.3393 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6915...  Training loss: 4.6956...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6916...  Training loss: 4.5978...  0.3397 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/100...  Training Step: 6917...  Training loss: 4.5827...  0.3387 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6918...  Training loss: 4.6164...  0.3390 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6919...  Training loss: 4.6431...  0.3422 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6920...  Training loss: 4.6771...  0.3397 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6921...  Training loss: 4.6669...  0.3397 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6922...  Training loss: 4.6161...  0.3407 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6923...  Training loss: 4.6205...  0.3378 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6924...  Training loss: 4.6050...  0.3414 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6925...  Training loss: 4.7127...  0.3379 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6926...  Training loss: 4.6222...  0.3413 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6927...  Training loss: 4.6510...  0.3403 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6928...  Training loss: 4.6800...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6929...  Training loss: 4.8122...  0.3433 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6930...  Training loss: 4.7601...  0.3394 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6931...  Training loss: 4.6745...  0.3407 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6932...  Training loss: 4.6615...  0.3435 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6933...  Training loss: 4.6312...  0.3429 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6934...  Training loss: 4.6804...  0.3376 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6935...  Training loss: 4.6991...  0.3399 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6936...  Training loss: 4.7126...  0.3426 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6937...  Training loss: 4.8124...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6938...  Training loss: 4.7109...  0.3390 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6939...  Training loss: 4.7091...  0.3384 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6940...  Training loss: 4.6902...  0.3385 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6941...  Training loss: 4.7365...  0.3410 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6942...  Training loss: 4.5933...  0.3399 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6943...  Training loss: 4.6610...  0.3397 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6944...  Training loss: 4.5862...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6945...  Training loss: 4.7093...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6946...  Training loss: 4.6188...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6947...  Training loss: 4.6947...  0.3390 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6948...  Training loss: 4.7648...  0.3419 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6949...  Training loss: 4.7973...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6950...  Training loss: 4.7618...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6951...  Training loss: 4.6087...  0.3407 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6952...  Training loss: 4.5780...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6953...  Training loss: 4.6077...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6954...  Training loss: 4.6283...  0.3423 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6955...  Training loss: 4.6852...  0.3394 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6956...  Training loss: 4.6635...  0.3409 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6957...  Training loss: 4.6898...  0.3373 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6958...  Training loss: 4.6384...  0.3424 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6959...  Training loss: 4.6614...  0.3400 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6960...  Training loss: 4.5911...  0.3416 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6961...  Training loss: 4.6818...  0.3392 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6962...  Training loss: 4.6651...  0.3388 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6963...  Training loss: 4.7232...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6964...  Training loss: 4.6984...  0.3380 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6965...  Training loss: 4.7246...  0.3395 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6966...  Training loss: 4.7789...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6967...  Training loss: 4.6153...  0.3409 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6968...  Training loss: 4.6401...  0.3438 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6969...  Training loss: 4.6682...  0.3381 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6970...  Training loss: 4.7085...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6971...  Training loss: 4.6810...  0.3409 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6972...  Training loss: 4.6700...  0.3388 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6973...  Training loss: 4.7825...  0.3378 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6974...  Training loss: 4.6927...  0.3398 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6975...  Training loss: 4.6654...  0.3405 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6976...  Training loss: 4.6740...  0.3429 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6977...  Training loss: 4.6830...  0.3393 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6978...  Training loss: 4.6611...  0.3389 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6979...  Training loss: 4.6492...  0.3380 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6980...  Training loss: 4.6374...  0.3410 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6981...  Training loss: 4.6980...  0.3433 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6982...  Training loss: 4.6509...  0.3408 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6983...  Training loss: 4.6342...  0.3433 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6984...  Training loss: 4.6422...  0.3396 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6985...  Training loss: 4.6631...  0.3412 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6986...  Training loss: 4.6627...  0.3431 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6987...  Training loss: 4.6388...  0.3426 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6988...  Training loss: 4.6051...  0.3402 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6989...  Training loss: 4.8165...  0.3401 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6990...  Training loss: 4.7315...  0.3397 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6991...  Training loss: 4.7436...  0.3419 sec/batch\n",
      "Epoch: 38/100...  Training Step: 6992...  Training loss: 4.7191...  0.3385 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6993...  Training loss: 4.8002...  0.3424 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6994...  Training loss: 4.7049...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6995...  Training loss: 4.6232...  0.3412 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6996...  Training loss: 4.6734...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6997...  Training loss: 4.6557...  0.3396 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6998...  Training loss: 4.5939...  0.3377 sec/batch\n",
      "Epoch: 39/100...  Training Step: 6999...  Training loss: 4.7808...  0.3384 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7000...  Training loss: 4.7341...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7001...  Training loss: 4.7435...  0.3788 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7002...  Training loss: 4.7040...  0.3466 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7003...  Training loss: 4.6815...  0.3408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7004...  Training loss: 4.6457...  0.3424 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7005...  Training loss: 4.6450...  0.3400 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7006...  Training loss: 4.6903...  0.3418 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7007...  Training loss: 4.7028...  0.3400 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7008...  Training loss: 4.6641...  0.3398 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7009...  Training loss: 4.7306...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7010...  Training loss: 4.6549...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7011...  Training loss: 4.7063...  0.3434 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7012...  Training loss: 4.6660...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7013...  Training loss: 4.6129...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/100...  Training Step: 7014...  Training loss: 4.5726...  0.3395 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7015...  Training loss: 4.5731...  0.3426 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7016...  Training loss: 4.5892...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7017...  Training loss: 4.6533...  0.3414 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7018...  Training loss: 4.6118...  0.3436 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7019...  Training loss: 4.6386...  0.3442 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7020...  Training loss: 4.6404...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7021...  Training loss: 4.6770...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7022...  Training loss: 4.7295...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7023...  Training loss: 4.6730...  0.3405 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7024...  Training loss: 4.6428...  0.3402 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7025...  Training loss: 4.4218...  0.3436 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7026...  Training loss: 4.5199...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7027...  Training loss: 4.6363...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7028...  Training loss: 4.7525...  0.3406 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7029...  Training loss: 4.7147...  0.3402 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7030...  Training loss: 4.7223...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7031...  Training loss: 4.6600...  0.3403 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7032...  Training loss: 4.6776...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7033...  Training loss: 4.7473...  0.3434 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7034...  Training loss: 4.7242...  0.3385 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7035...  Training loss: 4.7856...  0.3442 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7036...  Training loss: 4.7397...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7037...  Training loss: 4.6805...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7038...  Training loss: 4.6328...  0.3425 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7039...  Training loss: 4.7181...  0.3425 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7040...  Training loss: 4.5989...  0.3386 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7041...  Training loss: 4.6852...  0.3399 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7042...  Training loss: 4.7328...  0.3400 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7043...  Training loss: 4.6938...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7044...  Training loss: 4.7020...  0.3419 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7045...  Training loss: 4.7348...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7046...  Training loss: 4.7513...  0.3389 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7047...  Training loss: 4.7294...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7048...  Training loss: 4.6986...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7049...  Training loss: 4.6431...  0.3391 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7050...  Training loss: 4.7029...  0.3403 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7051...  Training loss: 4.5796...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7052...  Training loss: 4.6099...  0.3446 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7053...  Training loss: 4.6100...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7054...  Training loss: 4.6727...  0.3420 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7055...  Training loss: 4.6846...  0.3414 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7056...  Training loss: 4.7138...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7057...  Training loss: 4.6294...  0.3406 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7058...  Training loss: 4.6260...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7059...  Training loss: 4.6403...  0.3432 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7060...  Training loss: 4.5568...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7061...  Training loss: 4.6898...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7062...  Training loss: 4.6406...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7063...  Training loss: 4.6654...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7064...  Training loss: 4.5127...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7065...  Training loss: 4.5584...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7066...  Training loss: 4.5913...  0.3395 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7067...  Training loss: 4.5657...  0.3400 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7068...  Training loss: 4.5954...  0.3391 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7069...  Training loss: 4.6531...  0.3406 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7070...  Training loss: 4.6087...  0.3397 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7071...  Training loss: 4.6484...  0.3422 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7072...  Training loss: 4.7346...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7073...  Training loss: 4.7612...  0.3405 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7074...  Training loss: 4.6970...  0.3426 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7075...  Training loss: 4.6694...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7076...  Training loss: 4.6728...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7077...  Training loss: 4.7064...  0.3390 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7078...  Training loss: 4.6694...  0.3414 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7079...  Training loss: 4.6332...  0.3393 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7080...  Training loss: 4.6556...  0.3447 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7081...  Training loss: 4.7125...  0.3405 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7082...  Training loss: 4.7849...  0.3396 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7083...  Training loss: 4.7079...  0.3400 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7084...  Training loss: 4.5401...  0.3414 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7085...  Training loss: 4.6858...  0.3445 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7086...  Training loss: 4.6934...  0.3425 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7087...  Training loss: 4.6635...  0.3426 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7088...  Training loss: 4.6469...  0.3392 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7089...  Training loss: 4.6004...  0.3406 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7090...  Training loss: 4.5607...  0.3398 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7091...  Training loss: 4.5941...  0.3412 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7092...  Training loss: 4.5923...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7093...  Training loss: 4.6283...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7094...  Training loss: 4.5938...  0.3429 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7095...  Training loss: 4.6895...  0.3431 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7096...  Training loss: 4.5900...  0.3434 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7097...  Training loss: 4.7174...  0.3403 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7098...  Training loss: 4.6992...  0.3432 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7099...  Training loss: 4.6854...  0.3431 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7100...  Training loss: 4.6077...  0.3420 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7101...  Training loss: 4.5833...  0.3412 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7102...  Training loss: 4.5955...  0.3402 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7103...  Training loss: 4.6251...  0.3441 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7104...  Training loss: 4.6585...  0.3434 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7105...  Training loss: 4.6635...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7106...  Training loss: 4.5930...  0.3408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7107...  Training loss: 4.6181...  0.3407 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7108...  Training loss: 4.5937...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7109...  Training loss: 4.7122...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7110...  Training loss: 4.6157...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/100...  Training Step: 7111...  Training loss: 4.6291...  0.3425 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7112...  Training loss: 4.6615...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7113...  Training loss: 4.8496...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7114...  Training loss: 4.7709...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7115...  Training loss: 4.7174...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7116...  Training loss: 4.6690...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7117...  Training loss: 4.6168...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7118...  Training loss: 4.6359...  0.3404 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7119...  Training loss: 4.6391...  0.3424 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7120...  Training loss: 4.6603...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7121...  Training loss: 4.7560...  0.3422 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7122...  Training loss: 4.6743...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7123...  Training loss: 4.7205...  0.3415 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7124...  Training loss: 4.6940...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7125...  Training loss: 4.7384...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7126...  Training loss: 4.5900...  0.3406 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7127...  Training loss: 4.6440...  0.3390 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7128...  Training loss: 4.5886...  0.3390 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7129...  Training loss: 4.6845...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7130...  Training loss: 4.6043...  0.3418 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7131...  Training loss: 4.7005...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7132...  Training loss: 4.7561...  0.3434 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7133...  Training loss: 4.7973...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7134...  Training loss: 4.7450...  0.3444 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7135...  Training loss: 4.5973...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7136...  Training loss: 4.5550...  0.3399 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7137...  Training loss: 4.6087...  0.3411 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7138...  Training loss: 4.6255...  0.3417 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7139...  Training loss: 4.6737...  0.3433 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7140...  Training loss: 4.6647...  0.3405 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7141...  Training loss: 4.6527...  0.3407 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7142...  Training loss: 4.5983...  0.3451 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7143...  Training loss: 4.6507...  0.3438 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7144...  Training loss: 4.5449...  0.3439 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7145...  Training loss: 4.6497...  0.3436 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7146...  Training loss: 4.6411...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7147...  Training loss: 4.6758...  0.3422 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7148...  Training loss: 4.6550...  0.3416 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7149...  Training loss: 4.7121...  0.3396 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7150...  Training loss: 4.7322...  0.3422 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7151...  Training loss: 4.6068...  0.3410 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7152...  Training loss: 4.6276...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7153...  Training loss: 4.6408...  0.3412 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7154...  Training loss: 4.6822...  0.3401 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7155...  Training loss: 4.6457...  0.3397 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7156...  Training loss: 4.6456...  0.3432 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7157...  Training loss: 4.7760...  0.3408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7158...  Training loss: 4.6936...  0.3402 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7159...  Training loss: 4.6460...  0.3398 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7160...  Training loss: 4.6246...  0.3419 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7161...  Training loss: 4.6402...  0.3405 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7162...  Training loss: 4.6496...  0.3398 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7163...  Training loss: 4.6456...  0.3414 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7164...  Training loss: 4.6202...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7165...  Training loss: 4.6400...  0.3402 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7166...  Training loss: 4.6459...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7167...  Training loss: 4.6084...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7168...  Training loss: 4.6150...  0.3408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7169...  Training loss: 4.6657...  0.3408 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7170...  Training loss: 4.6563...  0.3419 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7171...  Training loss: 4.6365...  0.3423 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7172...  Training loss: 4.5923...  0.3407 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7173...  Training loss: 4.7825...  0.3439 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7174...  Training loss: 4.6954...  0.3409 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7175...  Training loss: 4.7216...  0.3413 sec/batch\n",
      "Epoch: 39/100...  Training Step: 7176...  Training loss: 4.6937...  0.3396 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7177...  Training loss: 4.7771...  0.3446 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7178...  Training loss: 4.6388...  0.3411 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7179...  Training loss: 4.5901...  0.3403 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7180...  Training loss: 4.6598...  0.3437 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7181...  Training loss: 4.6412...  0.3422 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7182...  Training loss: 4.5564...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7183...  Training loss: 4.7311...  0.3407 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7184...  Training loss: 4.6785...  0.3390 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7185...  Training loss: 4.7044...  0.3417 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7186...  Training loss: 4.6908...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7187...  Training loss: 4.6664...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7188...  Training loss: 4.6366...  0.3417 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7189...  Training loss: 4.6435...  0.3424 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7190...  Training loss: 4.7008...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7191...  Training loss: 4.6941...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7192...  Training loss: 4.6404...  0.3398 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7193...  Training loss: 4.6853...  0.3414 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7194...  Training loss: 4.6227...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7195...  Training loss: 4.6980...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7196...  Training loss: 4.6482...  0.3432 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7197...  Training loss: 4.5893...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7198...  Training loss: 4.5450...  0.3429 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7199...  Training loss: 4.5671...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7200...  Training loss: 4.5812...  0.3412 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7201...  Training loss: 4.6201...  0.3412 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7202...  Training loss: 4.5812...  0.3400 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7203...  Training loss: 4.6352...  0.3397 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7204...  Training loss: 4.6289...  0.3406 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7205...  Training loss: 4.6516...  0.3431 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7206...  Training loss: 4.6992...  0.3444 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7207...  Training loss: 4.6604...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/100...  Training Step: 7208...  Training loss: 4.6371...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7209...  Training loss: 4.4111...  0.3411 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7210...  Training loss: 4.4952...  0.3395 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7211...  Training loss: 4.6179...  0.3397 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7212...  Training loss: 4.7270...  0.3415 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7213...  Training loss: 4.7055...  0.3407 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7214...  Training loss: 4.7042...  0.3419 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7215...  Training loss: 4.6402...  0.3424 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7216...  Training loss: 4.6534...  0.3401 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7217...  Training loss: 4.7192...  0.3404 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7218...  Training loss: 4.7083...  0.3399 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7219...  Training loss: 4.7596...  0.3406 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7220...  Training loss: 4.7221...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7221...  Training loss: 4.6829...  0.3428 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7222...  Training loss: 4.6117...  0.3428 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7223...  Training loss: 4.6853...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7224...  Training loss: 4.5724...  0.3394 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7225...  Training loss: 4.6536...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7226...  Training loss: 4.7209...  0.3440 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7227...  Training loss: 4.6891...  0.3421 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7228...  Training loss: 4.6761...  0.3394 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7229...  Training loss: 4.7007...  0.3425 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7230...  Training loss: 4.7291...  0.3447 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7231...  Training loss: 4.7166...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7232...  Training loss: 4.6593...  0.3406 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7233...  Training loss: 4.6253...  0.3440 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7234...  Training loss: 4.6829...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7235...  Training loss: 4.5445...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7236...  Training loss: 4.5645...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7237...  Training loss: 4.6032...  0.3429 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7238...  Training loss: 4.6343...  0.3419 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7239...  Training loss: 4.6601...  0.3443 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7240...  Training loss: 4.6713...  0.3392 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7241...  Training loss: 4.5877...  0.3446 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7242...  Training loss: 4.6082...  0.3431 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7243...  Training loss: 4.6037...  0.3411 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7244...  Training loss: 4.5397...  0.3431 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7245...  Training loss: 4.6857...  0.3394 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7246...  Training loss: 4.6394...  0.3394 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7247...  Training loss: 4.6248...  0.3398 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7248...  Training loss: 4.4872...  0.3379 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7249...  Training loss: 4.5191...  0.3447 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7250...  Training loss: 4.5671...  0.3404 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7251...  Training loss: 4.5581...  0.3425 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7252...  Training loss: 4.5737...  0.3407 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7253...  Training loss: 4.6332...  0.3419 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7254...  Training loss: 4.6023...  0.3411 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7255...  Training loss: 4.6488...  0.3414 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7256...  Training loss: 4.6796...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7257...  Training loss: 4.7399...  0.3436 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7258...  Training loss: 4.6792...  0.3434 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7259...  Training loss: 4.6698...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7260...  Training loss: 4.6342...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7261...  Training loss: 4.6799...  0.3438 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7262...  Training loss: 4.6272...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7263...  Training loss: 4.5947...  0.3386 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7264...  Training loss: 4.6296...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7265...  Training loss: 4.6624...  0.3423 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7266...  Training loss: 4.7417...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7267...  Training loss: 4.6527...  0.3442 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7268...  Training loss: 4.5237...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7269...  Training loss: 4.6477...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7270...  Training loss: 4.6493...  0.3438 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7271...  Training loss: 4.6494...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7272...  Training loss: 4.6181...  0.3397 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7273...  Training loss: 4.5855...  0.3427 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7274...  Training loss: 4.5554...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7275...  Training loss: 4.5714...  0.3421 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7276...  Training loss: 4.5754...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7277...  Training loss: 4.6182...  0.3390 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7278...  Training loss: 4.5786...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7279...  Training loss: 4.6633...  0.3417 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7280...  Training loss: 4.5609...  0.3412 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7281...  Training loss: 4.6291...  0.3388 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7282...  Training loss: 4.6583...  0.3421 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7283...  Training loss: 4.6132...  0.3396 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7284...  Training loss: 4.5490...  0.3415 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7285...  Training loss: 4.5533...  0.3405 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7286...  Training loss: 4.5652...  0.3432 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7287...  Training loss: 4.5962...  0.3436 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7288...  Training loss: 4.6241...  0.3402 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7289...  Training loss: 4.6142...  0.3402 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7290...  Training loss: 4.5626...  0.3417 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7291...  Training loss: 4.5841...  0.3412 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7292...  Training loss: 4.5449...  0.3425 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7293...  Training loss: 4.6821...  0.3427 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7294...  Training loss: 4.5776...  0.3407 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7295...  Training loss: 4.6229...  0.3428 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7296...  Training loss: 4.6469...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7297...  Training loss: 4.7888...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7298...  Training loss: 4.7321...  0.3427 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7299...  Training loss: 4.6746...  0.3400 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7300...  Training loss: 4.6506...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7301...  Training loss: 4.6166...  0.3421 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7302...  Training loss: 4.6401...  0.3402 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7303...  Training loss: 4.6336...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7304...  Training loss: 4.6543...  0.3433 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/100...  Training Step: 7305...  Training loss: 4.7299...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7306...  Training loss: 4.6717...  0.3433 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7307...  Training loss: 4.6672...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7308...  Training loss: 4.6407...  0.3437 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7309...  Training loss: 4.6837...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7310...  Training loss: 4.5450...  0.3431 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7311...  Training loss: 4.6082...  0.3391 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7312...  Training loss: 4.5601...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7313...  Training loss: 4.6723...  0.3414 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7314...  Training loss: 4.5613...  0.3401 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7315...  Training loss: 4.6582...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7316...  Training loss: 4.7349...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7317...  Training loss: 4.7682...  0.3424 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7318...  Training loss: 4.7113...  0.3393 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7319...  Training loss: 4.5909...  0.3424 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7320...  Training loss: 4.5303...  0.3407 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7321...  Training loss: 4.5801...  0.3414 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7322...  Training loss: 4.6064...  0.3433 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7323...  Training loss: 4.6417...  0.3403 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7324...  Training loss: 4.6452...  0.3411 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7325...  Training loss: 4.6538...  0.3423 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7326...  Training loss: 4.5884...  0.3409 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7327...  Training loss: 4.6250...  0.3420 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7328...  Training loss: 4.5300...  0.3423 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7329...  Training loss: 4.6530...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7330...  Training loss: 4.6069...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7331...  Training loss: 4.6573...  0.3396 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7332...  Training loss: 4.6319...  0.3419 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7333...  Training loss: 4.6781...  0.3417 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7334...  Training loss: 4.7164...  0.3398 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7335...  Training loss: 4.5528...  0.3392 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7336...  Training loss: 4.5926...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7337...  Training loss: 4.6366...  0.3394 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7338...  Training loss: 4.6499...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7339...  Training loss: 4.6193...  0.3414 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7340...  Training loss: 4.6005...  0.3405 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7341...  Training loss: 4.7634...  0.3413 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7342...  Training loss: 4.6778...  0.3467 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7343...  Training loss: 4.6050...  0.3428 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7344...  Training loss: 4.6020...  0.3434 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7345...  Training loss: 4.6237...  0.3427 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7346...  Training loss: 4.6340...  0.3416 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7347...  Training loss: 4.6241...  0.3406 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7348...  Training loss: 4.5575...  0.3392 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7349...  Training loss: 4.6149...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7350...  Training loss: 4.5886...  0.3427 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7351...  Training loss: 4.5886...  0.3436 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7352...  Training loss: 4.5959...  0.3418 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7353...  Training loss: 4.6469...  0.3425 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7354...  Training loss: 4.6173...  0.3429 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7355...  Training loss: 4.6156...  0.3433 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7356...  Training loss: 4.5736...  0.3397 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7357...  Training loss: 4.7479...  0.3390 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7358...  Training loss: 4.6504...  0.3410 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7359...  Training loss: 4.6862...  0.3408 sec/batch\n",
      "Epoch: 40/100...  Training Step: 7360...  Training loss: 4.6603...  0.3391 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7361...  Training loss: 4.7219...  0.3434 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7362...  Training loss: 4.6275...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7363...  Training loss: 4.5748...  0.3425 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7364...  Training loss: 4.6067...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7365...  Training loss: 4.5678...  0.3423 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7366...  Training loss: 4.5200...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7367...  Training loss: 4.6756...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7368...  Training loss: 4.6554...  0.3407 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7369...  Training loss: 4.6259...  0.3399 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7370...  Training loss: 4.6471...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7371...  Training loss: 4.6205...  0.3415 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7372...  Training loss: 4.5854...  0.3428 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7373...  Training loss: 4.6040...  0.3376 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7374...  Training loss: 4.6458...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7375...  Training loss: 4.6897...  0.3401 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7376...  Training loss: 4.5906...  0.3403 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7377...  Training loss: 4.6801...  0.3391 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7378...  Training loss: 4.6000...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7379...  Training loss: 4.6724...  0.3392 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7380...  Training loss: 4.5906...  0.3396 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7381...  Training loss: 4.5734...  0.3414 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7382...  Training loss: 4.5372...  0.3414 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7383...  Training loss: 4.5317...  0.3394 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7384...  Training loss: 4.5629...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7385...  Training loss: 4.6061...  0.3396 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7386...  Training loss: 4.5862...  0.3409 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7387...  Training loss: 4.6057...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7388...  Training loss: 4.6089...  0.3380 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7389...  Training loss: 4.6469...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7390...  Training loss: 4.6884...  0.3438 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7391...  Training loss: 4.6413...  0.3415 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7392...  Training loss: 4.6232...  0.3406 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7393...  Training loss: 4.4012...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7394...  Training loss: 4.4912...  0.3401 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7395...  Training loss: 4.5901...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7396...  Training loss: 4.7228...  0.3421 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7397...  Training loss: 4.6885...  0.3427 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7398...  Training loss: 4.6946...  0.3407 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7399...  Training loss: 4.6209...  0.3429 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7400...  Training loss: 4.6430...  0.3415 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7401...  Training loss: 4.6889...  0.3419 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/100...  Training Step: 7402...  Training loss: 4.6896...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7403...  Training loss: 4.7442...  0.3427 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7404...  Training loss: 4.6981...  0.3419 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7405...  Training loss: 4.6373...  0.3406 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7406...  Training loss: 4.5888...  0.3424 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7407...  Training loss: 4.6600...  0.3401 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7408...  Training loss: 4.5636...  0.3441 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7409...  Training loss: 4.6267...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7410...  Training loss: 4.6947...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7411...  Training loss: 4.6689...  0.3404 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7412...  Training loss: 4.6572...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7413...  Training loss: 4.6797...  0.3449 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7414...  Training loss: 4.7182...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7415...  Training loss: 4.6882...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7416...  Training loss: 4.6502...  0.3407 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7417...  Training loss: 4.5921...  0.3411 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7418...  Training loss: 4.6546...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7419...  Training loss: 4.5440...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7420...  Training loss: 4.5388...  0.3401 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7421...  Training loss: 4.5826...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7422...  Training loss: 4.6173...  0.3446 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7423...  Training loss: 4.6234...  0.3448 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7424...  Training loss: 4.6652...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7425...  Training loss: 4.5601...  0.3453 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7426...  Training loss: 4.5646...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7427...  Training loss: 4.5999...  0.3424 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7428...  Training loss: 4.5116...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7429...  Training loss: 4.6515...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7430...  Training loss: 4.6065...  0.3421 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7431...  Training loss: 4.6042...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7432...  Training loss: 4.4597...  0.3407 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7433...  Training loss: 4.5047...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7434...  Training loss: 4.5334...  0.3421 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7435...  Training loss: 4.5187...  0.3419 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7436...  Training loss: 4.5387...  0.3427 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7437...  Training loss: 4.5913...  0.3428 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7438...  Training loss: 4.5570...  0.3448 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7439...  Training loss: 4.6367...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7440...  Training loss: 4.6414...  0.3406 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7441...  Training loss: 4.6929...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7442...  Training loss: 4.6464...  0.3433 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7443...  Training loss: 4.6285...  0.3392 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7444...  Training loss: 4.6081...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7445...  Training loss: 4.6496...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7446...  Training loss: 4.6197...  0.3395 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7447...  Training loss: 4.5804...  0.3416 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7448...  Training loss: 4.5689...  0.3430 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7449...  Training loss: 4.6603...  0.3441 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7450...  Training loss: 4.7144...  0.3449 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7451...  Training loss: 4.6300...  0.3447 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7452...  Training loss: 4.4847...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7453...  Training loss: 4.6326...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7454...  Training loss: 4.6162...  0.3422 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7455...  Training loss: 4.6143...  0.3404 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7456...  Training loss: 4.6065...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7457...  Training loss: 4.5602...  0.3438 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7458...  Training loss: 4.5356...  0.3418 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7459...  Training loss: 4.5686...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7460...  Training loss: 4.5556...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7461...  Training loss: 4.5842...  0.3436 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7462...  Training loss: 4.5624...  0.3429 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7463...  Training loss: 4.6448...  0.3425 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7464...  Training loss: 4.5083...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7465...  Training loss: 4.6171...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7466...  Training loss: 4.6068...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7467...  Training loss: 4.5895...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7468...  Training loss: 4.5119...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7469...  Training loss: 4.5116...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7470...  Training loss: 4.5316...  0.3394 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7471...  Training loss: 4.5648...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7472...  Training loss: 4.5730...  0.3432 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7473...  Training loss: 4.5727...  0.3425 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7474...  Training loss: 4.5267...  0.3424 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7475...  Training loss: 4.5679...  0.3419 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7476...  Training loss: 4.5307...  0.3400 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7477...  Training loss: 4.6577...  0.3422 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7478...  Training loss: 4.5587...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7479...  Training loss: 4.5694...  0.3391 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7480...  Training loss: 4.6085...  0.3424 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7481...  Training loss: 4.7744...  0.3443 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7482...  Training loss: 4.7142...  0.3409 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7483...  Training loss: 4.6533...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7484...  Training loss: 4.6084...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7485...  Training loss: 4.5789...  0.3434 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7486...  Training loss: 4.6286...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7487...  Training loss: 4.6319...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7488...  Training loss: 4.6357...  0.3432 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7489...  Training loss: 4.7175...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7490...  Training loss: 4.6465...  0.3423 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7491...  Training loss: 4.6477...  0.3415 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7492...  Training loss: 4.6253...  0.3425 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7493...  Training loss: 4.6469...  0.3436 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7494...  Training loss: 4.5324...  0.3411 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7495...  Training loss: 4.6099...  0.3439 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7496...  Training loss: 4.5309...  0.3434 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7497...  Training loss: 4.6513...  0.3411 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7498...  Training loss: 4.5473...  0.3434 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/100...  Training Step: 7499...  Training loss: 4.6226...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7500...  Training loss: 4.6796...  0.3408 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7501...  Training loss: 4.7360...  0.3406 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7502...  Training loss: 4.6949...  0.3387 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7503...  Training loss: 4.5662...  0.3435 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7504...  Training loss: 4.5131...  0.3401 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7505...  Training loss: 4.5673...  0.3394 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7506...  Training loss: 4.5983...  0.3415 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7507...  Training loss: 4.6199...  0.3422 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7508...  Training loss: 4.5856...  0.3430 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7509...  Training loss: 4.6294...  0.3411 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7510...  Training loss: 4.5735...  0.3395 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7511...  Training loss: 4.6040...  0.3421 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7512...  Training loss: 4.5210...  0.3399 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7513...  Training loss: 4.6158...  0.3406 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7514...  Training loss: 4.5941...  0.3395 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7515...  Training loss: 4.6609...  0.3430 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7516...  Training loss: 4.6321...  0.3399 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7517...  Training loss: 4.6654...  0.3410 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7518...  Training loss: 4.7022...  0.3455 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7519...  Training loss: 4.5296...  0.3423 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7520...  Training loss: 4.5669...  0.3385 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7521...  Training loss: 4.5936...  0.3405 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7522...  Training loss: 4.6417...  0.3386 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7523...  Training loss: 4.5915...  0.3430 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7524...  Training loss: 4.5942...  0.3433 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7525...  Training loss: 4.7192...  0.3419 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7526...  Training loss: 4.6397...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7527...  Training loss: 4.6017...  0.3420 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7528...  Training loss: 4.5809...  0.3412 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7529...  Training loss: 4.5825...  0.3427 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7530...  Training loss: 4.6136...  0.3446 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7531...  Training loss: 4.6004...  0.3428 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7532...  Training loss: 4.5634...  0.3411 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7533...  Training loss: 4.5963...  0.3417 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7534...  Training loss: 4.5650...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7535...  Training loss: 4.5359...  0.3390 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7536...  Training loss: 4.5237...  0.3413 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7537...  Training loss: 4.6080...  0.3397 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7538...  Training loss: 4.5779...  0.3396 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7539...  Training loss: 4.5769...  0.3404 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7540...  Training loss: 4.5311...  0.3416 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7541...  Training loss: 4.7473...  0.3431 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7542...  Training loss: 4.6391...  0.3402 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7543...  Training loss: 4.6825...  0.3384 sec/batch\n",
      "Epoch: 41/100...  Training Step: 7544...  Training loss: 4.6293...  0.3406 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7545...  Training loss: 4.6890...  0.3458 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7546...  Training loss: 4.6274...  0.3418 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7547...  Training loss: 4.5485...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7548...  Training loss: 4.5605...  0.3409 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7549...  Training loss: 4.5182...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7550...  Training loss: 4.4835...  0.3427 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7551...  Training loss: 4.6362...  0.3394 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7552...  Training loss: 4.6210...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7553...  Training loss: 4.5920...  0.3411 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7554...  Training loss: 4.6195...  0.3428 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7555...  Training loss: 4.5663...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7556...  Training loss: 4.5650...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7557...  Training loss: 4.5480...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7558...  Training loss: 4.6142...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7559...  Training loss: 4.6389...  0.3400 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7560...  Training loss: 4.5627...  0.3389 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7561...  Training loss: 4.6658...  0.3434 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7562...  Training loss: 4.5666...  0.3403 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7563...  Training loss: 4.6355...  0.3418 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7564...  Training loss: 4.5689...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7565...  Training loss: 4.5547...  0.3409 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7566...  Training loss: 4.4964...  0.3404 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7567...  Training loss: 4.5060...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7568...  Training loss: 4.5383...  0.3419 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7569...  Training loss: 4.5949...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7570...  Training loss: 4.5670...  0.3406 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7571...  Training loss: 4.5861...  0.3420 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7572...  Training loss: 4.5769...  0.3403 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7573...  Training loss: 4.6391...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7574...  Training loss: 4.6772...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7575...  Training loss: 4.6177...  0.3405 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7576...  Training loss: 4.5800...  0.3408 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7577...  Training loss: 4.3857...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7578...  Training loss: 4.4745...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7579...  Training loss: 4.5847...  0.3437 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7580...  Training loss: 4.7000...  0.3443 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7581...  Training loss: 4.6501...  0.3407 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7582...  Training loss: 4.6696...  0.3426 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7583...  Training loss: 4.6025...  0.3400 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7584...  Training loss: 4.6291...  0.3407 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7585...  Training loss: 4.6795...  0.3401 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7586...  Training loss: 4.6697...  0.3426 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7587...  Training loss: 4.7116...  0.3421 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7588...  Training loss: 4.6724...  0.3397 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7589...  Training loss: 4.6212...  0.3440 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7590...  Training loss: 4.5873...  0.3425 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7591...  Training loss: 4.6752...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7592...  Training loss: 4.5295...  0.3431 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7593...  Training loss: 4.5891...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7594...  Training loss: 4.6642...  0.3408 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7595...  Training loss: 4.6480...  0.3419 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42/100...  Training Step: 7596...  Training loss: 4.6251...  0.3398 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7597...  Training loss: 4.6750...  0.3415 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7598...  Training loss: 4.6810...  0.3400 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7599...  Training loss: 4.6769...  0.3417 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7600...  Training loss: 4.6348...  0.3403 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7601...  Training loss: 4.5638...  0.3390 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7602...  Training loss: 4.6434...  0.3432 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7603...  Training loss: 4.5415...  0.3395 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7604...  Training loss: 4.5092...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7605...  Training loss: 4.5419...  0.3427 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7606...  Training loss: 4.6011...  0.3408 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7607...  Training loss: 4.6171...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7608...  Training loss: 4.6078...  0.3405 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7609...  Training loss: 4.5543...  0.3398 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7610...  Training loss: 4.5508...  0.3421 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7611...  Training loss: 4.5601...  0.3407 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7612...  Training loss: 4.4928...  0.3407 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7613...  Training loss: 4.6199...  0.3394 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7614...  Training loss: 4.5848...  0.3412 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7615...  Training loss: 4.5561...  0.3437 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7616...  Training loss: 4.4407...  0.3439 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7617...  Training loss: 4.4897...  0.3441 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7618...  Training loss: 4.5161...  0.3401 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7619...  Training loss: 4.5071...  0.3432 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7620...  Training loss: 4.5115...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7621...  Training loss: 4.5495...  0.3398 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7622...  Training loss: 4.5370...  0.3436 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7623...  Training loss: 4.5944...  0.3409 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7624...  Training loss: 4.6116...  0.3434 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7625...  Training loss: 4.6672...  0.3429 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7626...  Training loss: 4.6187...  0.3436 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7627...  Training loss: 4.6184...  0.3425 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7628...  Training loss: 4.5846...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7629...  Training loss: 4.6226...  0.3409 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7630...  Training loss: 4.5995...  0.3392 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7631...  Training loss: 4.5777...  0.3408 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7632...  Training loss: 4.5670...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7633...  Training loss: 4.6582...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7634...  Training loss: 4.6732...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7635...  Training loss: 4.6035...  0.3432 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7636...  Training loss: 4.4678...  0.3391 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7637...  Training loss: 4.5963...  0.3429 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7638...  Training loss: 4.6048...  0.3397 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7639...  Training loss: 4.6173...  0.3400 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7640...  Training loss: 4.5994...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7641...  Training loss: 4.5203...  0.3427 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7642...  Training loss: 4.5101...  0.3443 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7643...  Training loss: 4.5292...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7644...  Training loss: 4.5357...  0.3412 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7645...  Training loss: 4.5883...  0.3411 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7646...  Training loss: 4.5534...  0.3433 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7647...  Training loss: 4.6556...  0.3393 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7648...  Training loss: 4.5256...  0.3426 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7649...  Training loss: 4.5945...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7650...  Training loss: 4.6094...  0.3420 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7651...  Training loss: 4.5621...  0.3425 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7652...  Training loss: 4.5004...  0.3383 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7653...  Training loss: 4.4920...  0.3415 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7654...  Training loss: 4.5060...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7655...  Training loss: 4.5478...  0.3435 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7656...  Training loss: 4.5605...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7657...  Training loss: 4.5241...  0.3443 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7658...  Training loss: 4.5131...  0.3392 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7659...  Training loss: 4.5084...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7660...  Training loss: 4.4865...  0.3413 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7661...  Training loss: 4.6072...  0.3418 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7662...  Training loss: 4.5046...  0.3408 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7663...  Training loss: 4.5543...  0.3395 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7664...  Training loss: 4.5914...  0.3405 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7665...  Training loss: 4.7369...  0.3411 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7666...  Training loss: 4.6902...  0.3415 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7667...  Training loss: 4.6323...  0.3401 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7668...  Training loss: 4.5912...  0.3399 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7669...  Training loss: 4.5639...  0.3397 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7670...  Training loss: 4.6184...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7671...  Training loss: 4.6063...  0.3395 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7672...  Training loss: 4.6229...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7673...  Training loss: 4.7084...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7674...  Training loss: 4.6478...  0.3417 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7675...  Training loss: 4.6479...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7676...  Training loss: 4.6204...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7677...  Training loss: 4.6498...  0.3411 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7678...  Training loss: 4.5119...  0.3426 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7679...  Training loss: 4.5876...  0.3417 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7680...  Training loss: 4.5078...  0.3422 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7681...  Training loss: 4.6151...  0.3399 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7682...  Training loss: 4.5452...  0.3417 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7683...  Training loss: 4.5977...  0.3396 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7684...  Training loss: 4.6353...  0.3417 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7685...  Training loss: 4.7113...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7686...  Training loss: 4.6794...  0.3391 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7687...  Training loss: 4.5472...  0.3403 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7688...  Training loss: 4.5222...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7689...  Training loss: 4.5432...  0.3415 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7690...  Training loss: 4.5738...  0.3415 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7691...  Training loss: 4.6144...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7692...  Training loss: 4.6078...  0.3414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42/100...  Training Step: 7693...  Training loss: 4.5988...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7694...  Training loss: 4.5656...  0.3421 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7695...  Training loss: 4.6050...  0.3419 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7696...  Training loss: 4.5312...  0.3426 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7697...  Training loss: 4.6070...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7698...  Training loss: 4.5741...  0.3395 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7699...  Training loss: 4.6355...  0.3421 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7700...  Training loss: 4.6031...  0.3420 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7701...  Training loss: 4.6274...  0.3428 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7702...  Training loss: 4.6951...  0.3395 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7703...  Training loss: 4.5184...  0.3444 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7704...  Training loss: 4.5607...  0.3406 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7705...  Training loss: 4.5826...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7706...  Training loss: 4.6210...  0.3393 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7707...  Training loss: 4.5666...  0.3414 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7708...  Training loss: 4.5671...  0.3400 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7709...  Training loss: 4.6763...  0.3435 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7710...  Training loss: 4.6221...  0.3427 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7711...  Training loss: 4.5622...  0.3410 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7712...  Training loss: 4.5409...  0.3404 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7713...  Training loss: 4.5766...  0.3405 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7714...  Training loss: 4.5836...  0.3402 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7715...  Training loss: 4.5700...  0.3406 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7716...  Training loss: 4.5253...  0.3437 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7717...  Training loss: 4.5770...  0.3452 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7718...  Training loss: 4.5416...  0.3420 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7719...  Training loss: 4.5261...  0.3437 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7720...  Training loss: 4.5448...  0.3384 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7721...  Training loss: 4.6055...  0.3398 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7722...  Training loss: 4.5834...  0.3404 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7723...  Training loss: 4.5681...  0.3401 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7724...  Training loss: 4.5380...  0.3425 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7725...  Training loss: 4.6814...  0.3397 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7726...  Training loss: 4.6284...  0.3416 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7727...  Training loss: 4.6185...  0.3423 sec/batch\n",
      "Epoch: 42/100...  Training Step: 7728...  Training loss: 4.6061...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7729...  Training loss: 4.6854...  0.3436 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7730...  Training loss: 4.5389...  0.3404 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7731...  Training loss: 4.4801...  0.3438 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7732...  Training loss: 4.5311...  0.3421 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7733...  Training loss: 4.5200...  0.3421 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7734...  Training loss: 4.4575...  0.3425 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7735...  Training loss: 4.6165...  0.3427 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7736...  Training loss: 4.6091...  0.3388 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7737...  Training loss: 4.5884...  0.3400 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7738...  Training loss: 4.6173...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7739...  Training loss: 4.5679...  0.3411 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7740...  Training loss: 4.5419...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7741...  Training loss: 4.5157...  0.3429 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7742...  Training loss: 4.5500...  0.3433 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7743...  Training loss: 4.6092...  0.3445 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7744...  Training loss: 4.5439...  0.3428 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7745...  Training loss: 4.6148...  0.3431 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7746...  Training loss: 4.5382...  0.3436 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7747...  Training loss: 4.5981...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7748...  Training loss: 4.5466...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7749...  Training loss: 4.5365...  0.3416 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7750...  Training loss: 4.4591...  0.3419 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7751...  Training loss: 4.4795...  0.3422 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7752...  Training loss: 4.4846...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7753...  Training loss: 4.5465...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7754...  Training loss: 4.5278...  0.3395 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7755...  Training loss: 4.5643...  0.3398 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7756...  Training loss: 4.5408...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7757...  Training loss: 4.6113...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7758...  Training loss: 4.6378...  0.3411 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7759...  Training loss: 4.6104...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7760...  Training loss: 4.5671...  0.3419 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7761...  Training loss: 4.3786...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7762...  Training loss: 4.4668...  0.3396 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7763...  Training loss: 4.5848...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7764...  Training loss: 4.6842...  0.3419 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7765...  Training loss: 4.6513...  0.3393 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7766...  Training loss: 4.6552...  0.3411 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7767...  Training loss: 4.5636...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7768...  Training loss: 4.6127...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7769...  Training loss: 4.6549...  0.3454 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7770...  Training loss: 4.6788...  0.3439 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7771...  Training loss: 4.7152...  0.3416 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7772...  Training loss: 4.6680...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7773...  Training loss: 4.6113...  0.3425 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7774...  Training loss: 4.5522...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7775...  Training loss: 4.6261...  0.3406 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7776...  Training loss: 4.5198...  0.3416 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7777...  Training loss: 4.5756...  0.3439 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7778...  Training loss: 4.6350...  0.3442 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7779...  Training loss: 4.6246...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7780...  Training loss: 4.6041...  0.3417 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7781...  Training loss: 4.6301...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7782...  Training loss: 4.6744...  0.3403 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7783...  Training loss: 4.6595...  0.3412 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7784...  Training loss: 4.6274...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7785...  Training loss: 4.5758...  0.3397 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7786...  Training loss: 4.6482...  0.3424 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7787...  Training loss: 4.5403...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7788...  Training loss: 4.5461...  0.3397 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7789...  Training loss: 4.5571...  0.3403 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/100...  Training Step: 7790...  Training loss: 4.5943...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7791...  Training loss: 4.6083...  0.3440 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7792...  Training loss: 4.6248...  0.3417 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7793...  Training loss: 4.5519...  0.3398 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7794...  Training loss: 4.5425...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7795...  Training loss: 4.5716...  0.3412 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7796...  Training loss: 4.4943...  0.3411 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7797...  Training loss: 4.5794...  0.3437 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7798...  Training loss: 4.5495...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7799...  Training loss: 4.5495...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7800...  Training loss: 4.4272...  0.3403 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7801...  Training loss: 4.4700...  0.3406 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7802...  Training loss: 4.4798...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7803...  Training loss: 4.4847...  0.3427 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7804...  Training loss: 4.5135...  0.3404 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7805...  Training loss: 4.5311...  0.3381 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7806...  Training loss: 4.5037...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7807...  Training loss: 4.5529...  0.3414 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7808...  Training loss: 4.5655...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7809...  Training loss: 4.6180...  0.3414 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7810...  Training loss: 4.5967...  0.3416 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7811...  Training loss: 4.5745...  0.3424 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7812...  Training loss: 4.5706...  0.3425 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7813...  Training loss: 4.5973...  0.3436 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7814...  Training loss: 4.5925...  0.3427 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7815...  Training loss: 4.5401...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7816...  Training loss: 4.5421...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7817...  Training loss: 4.6139...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7818...  Training loss: 4.6557...  0.3406 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7819...  Training loss: 4.6131...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7820...  Training loss: 4.4464...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7821...  Training loss: 4.5609...  0.3421 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7822...  Training loss: 4.5648...  0.3431 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7823...  Training loss: 4.5924...  0.3434 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7824...  Training loss: 4.5787...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7825...  Training loss: 4.5325...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7826...  Training loss: 4.5013...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7827...  Training loss: 4.5129...  0.3412 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7828...  Training loss: 4.5087...  0.3398 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7829...  Training loss: 4.5475...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7830...  Training loss: 4.4949...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7831...  Training loss: 4.6143...  0.3381 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7832...  Training loss: 4.5015...  0.3416 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7833...  Training loss: 4.5731...  0.3388 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7834...  Training loss: 4.5970...  0.3421 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7835...  Training loss: 4.5366...  0.3395 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7836...  Training loss: 4.4753...  0.3445 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7837...  Training loss: 4.4402...  0.3406 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7838...  Training loss: 4.4619...  0.3424 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7839...  Training loss: 4.5237...  0.3433 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7840...  Training loss: 4.5227...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7841...  Training loss: 4.5385...  0.3424 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7842...  Training loss: 4.4844...  0.3429 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7843...  Training loss: 4.4992...  0.3401 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7844...  Training loss: 4.4727...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7845...  Training loss: 4.5830...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7846...  Training loss: 4.4697...  0.3425 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7847...  Training loss: 4.4954...  0.3398 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7848...  Training loss: 4.5499...  0.3422 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7849...  Training loss: 4.6943...  0.3414 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7850...  Training loss: 4.6611...  0.3387 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7851...  Training loss: 4.6148...  0.3400 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7852...  Training loss: 4.5621...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7853...  Training loss: 4.5427...  0.3396 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7854...  Training loss: 4.5614...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7855...  Training loss: 4.5705...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7856...  Training loss: 4.5821...  0.3433 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7857...  Training loss: 4.6748...  0.3440 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7858...  Training loss: 4.5913...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7859...  Training loss: 4.6410...  0.3433 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7860...  Training loss: 4.6004...  0.3428 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7861...  Training loss: 4.6430...  0.3452 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7862...  Training loss: 4.5139...  0.3437 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7863...  Training loss: 4.5640...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7864...  Training loss: 4.5003...  0.3418 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7865...  Training loss: 4.5959...  0.3423 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7866...  Training loss: 4.5000...  0.3400 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7867...  Training loss: 4.5622...  0.3424 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7868...  Training loss: 4.6032...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7869...  Training loss: 4.6623...  0.3401 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7870...  Training loss: 4.6474...  0.3432 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7871...  Training loss: 4.5135...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7872...  Training loss: 4.4943...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7873...  Training loss: 4.5301...  0.3397 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7874...  Training loss: 4.5615...  0.3417 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7875...  Training loss: 4.5730...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7876...  Training loss: 4.5727...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7877...  Training loss: 4.5875...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7878...  Training loss: 4.5480...  0.3390 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7879...  Training loss: 4.5990...  0.3413 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7880...  Training loss: 4.5442...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7881...  Training loss: 4.6015...  0.3395 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7882...  Training loss: 4.5859...  0.3382 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7883...  Training loss: 4.6296...  0.3406 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7884...  Training loss: 4.5841...  0.3415 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7885...  Training loss: 4.6206...  0.3402 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7886...  Training loss: 4.6611...  0.3404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/100...  Training Step: 7887...  Training loss: 4.4751...  0.3403 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7888...  Training loss: 4.5502...  0.3427 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7889...  Training loss: 4.5666...  0.3388 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7890...  Training loss: 4.5913...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7891...  Training loss: 4.5713...  0.3419 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7892...  Training loss: 4.5530...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7893...  Training loss: 4.6588...  0.3450 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7894...  Training loss: 4.5939...  0.3414 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7895...  Training loss: 4.5407...  0.3421 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7896...  Training loss: 4.5294...  0.3384 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7897...  Training loss: 4.5473...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7898...  Training loss: 4.5831...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7899...  Training loss: 4.5583...  0.3399 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7900...  Training loss: 4.5124...  0.3394 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7901...  Training loss: 4.5557...  0.3408 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7902...  Training loss: 4.5254...  0.3431 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7903...  Training loss: 4.4921...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7904...  Training loss: 4.5082...  0.3409 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7905...  Training loss: 4.5757...  0.3405 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7906...  Training loss: 4.5545...  0.3420 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7907...  Training loss: 4.5320...  0.3426 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7908...  Training loss: 4.4843...  0.3427 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7909...  Training loss: 4.6395...  0.3430 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7910...  Training loss: 4.5956...  0.3429 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7911...  Training loss: 4.6048...  0.3395 sec/batch\n",
      "Epoch: 43/100...  Training Step: 7912...  Training loss: 4.5855...  0.3419 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7913...  Training loss: 4.6602...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7914...  Training loss: 4.5218...  0.3433 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7915...  Training loss: 4.4427...  0.3411 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7916...  Training loss: 4.4923...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7917...  Training loss: 4.4822...  0.3443 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7918...  Training loss: 4.4083...  0.3437 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7919...  Training loss: 4.5672...  0.3420 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7920...  Training loss: 4.5701...  0.3414 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7921...  Training loss: 4.5646...  0.3439 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7922...  Training loss: 4.5649...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7923...  Training loss: 4.4985...  0.3417 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7924...  Training loss: 4.5193...  0.3408 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7925...  Training loss: 4.4918...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7926...  Training loss: 4.5454...  0.3410 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7927...  Training loss: 4.5517...  0.3422 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7928...  Training loss: 4.5202...  0.3413 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7929...  Training loss: 4.6150...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7930...  Training loss: 4.5107...  0.3418 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7931...  Training loss: 4.5887...  0.3426 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7932...  Training loss: 4.5290...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7933...  Training loss: 4.5040...  0.3395 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7934...  Training loss: 4.4233...  0.3431 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7935...  Training loss: 4.4379...  0.3455 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7936...  Training loss: 4.4530...  0.3393 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7937...  Training loss: 4.5333...  0.3414 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7938...  Training loss: 4.4855...  0.3416 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7939...  Training loss: 4.5221...  0.3421 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7940...  Training loss: 4.5004...  0.3416 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7941...  Training loss: 4.5529...  0.3452 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7942...  Training loss: 4.6047...  0.3399 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7943...  Training loss: 4.5732...  0.3422 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7944...  Training loss: 4.5550...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7945...  Training loss: 4.3675...  0.3393 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7946...  Training loss: 4.4584...  0.3422 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7947...  Training loss: 4.5502...  0.3428 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7948...  Training loss: 4.6537...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7949...  Training loss: 4.6338...  0.3399 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7950...  Training loss: 4.5968...  0.3413 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7951...  Training loss: 4.5702...  0.3421 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7952...  Training loss: 4.5817...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7953...  Training loss: 4.6185...  0.3383 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7954...  Training loss: 4.6279...  0.3397 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7955...  Training loss: 4.6772...  0.3428 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7956...  Training loss: 4.6366...  0.3431 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7957...  Training loss: 4.5854...  0.3422 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7958...  Training loss: 4.5355...  0.3424 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7959...  Training loss: 4.6166...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7960...  Training loss: 4.4817...  0.3406 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7961...  Training loss: 4.5390...  0.3434 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7962...  Training loss: 4.6133...  0.3407 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7963...  Training loss: 4.5788...  0.3411 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7964...  Training loss: 4.5664...  0.3432 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7965...  Training loss: 4.5898...  0.3431 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7966...  Training loss: 4.6183...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7967...  Training loss: 4.6137...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7968...  Training loss: 4.5877...  0.3432 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7969...  Training loss: 4.5132...  0.3400 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7970...  Training loss: 4.5961...  0.3400 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7971...  Training loss: 4.4750...  0.3410 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7972...  Training loss: 4.5200...  0.3426 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7973...  Training loss: 4.5585...  0.3436 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7974...  Training loss: 4.5830...  0.3407 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7975...  Training loss: 4.5750...  0.3419 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7976...  Training loss: 4.6211...  0.3396 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7977...  Training loss: 4.5245...  0.3408 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7978...  Training loss: 4.5433...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7979...  Training loss: 4.5517...  0.3380 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7980...  Training loss: 4.4754...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7981...  Training loss: 4.5798...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7982...  Training loss: 4.5527...  0.3405 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7983...  Training loss: 4.5181...  0.3428 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/100...  Training Step: 7984...  Training loss: 4.3955...  0.3406 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7985...  Training loss: 4.4425...  0.3415 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7986...  Training loss: 4.4478...  0.3424 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7987...  Training loss: 4.4502...  0.3423 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7988...  Training loss: 4.4761...  0.3416 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7989...  Training loss: 4.5200...  0.3416 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7990...  Training loss: 4.4670...  0.3406 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7991...  Training loss: 4.5312...  0.3403 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7992...  Training loss: 4.5452...  0.3399 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7993...  Training loss: 4.5700...  0.3434 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7994...  Training loss: 4.5509...  0.3420 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7995...  Training loss: 4.5519...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7996...  Training loss: 4.5140...  0.3444 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7997...  Training loss: 4.5432...  0.3394 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7998...  Training loss: 4.5530...  0.3428 sec/batch\n",
      "Epoch: 44/100...  Training Step: 7999...  Training loss: 4.5074...  0.3420 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8000...  Training loss: 4.5035...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8001...  Training loss: 4.5633...  0.3796 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8002...  Training loss: 4.6119...  0.3405 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8003...  Training loss: 4.5610...  0.3378 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8004...  Training loss: 4.4390...  0.3373 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8005...  Training loss: 4.5408...  0.3410 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8006...  Training loss: 4.5548...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8007...  Training loss: 4.5345...  0.3395 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8008...  Training loss: 4.5522...  0.3397 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8009...  Training loss: 4.4969...  0.3443 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8010...  Training loss: 4.4776...  0.3430 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8011...  Training loss: 4.5020...  0.3417 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8012...  Training loss: 4.5144...  0.3391 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8013...  Training loss: 4.5362...  0.3407 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8014...  Training loss: 4.4797...  0.3376 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8015...  Training loss: 4.5864...  0.3377 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8016...  Training loss: 4.4702...  0.3420 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8017...  Training loss: 4.5566...  0.3368 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8018...  Training loss: 4.5418...  0.3413 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8019...  Training loss: 4.5312...  0.3388 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8020...  Training loss: 4.4448...  0.3389 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8021...  Training loss: 4.4220...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8022...  Training loss: 4.4415...  0.3398 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8023...  Training loss: 4.4824...  0.3419 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8024...  Training loss: 4.4996...  0.3424 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8025...  Training loss: 4.4879...  0.3417 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8026...  Training loss: 4.4459...  0.3398 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8027...  Training loss: 4.4728...  0.3380 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8028...  Training loss: 4.4643...  0.3376 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8029...  Training loss: 4.5678...  0.3387 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8030...  Training loss: 4.4662...  0.3394 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8031...  Training loss: 4.5055...  0.3405 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8032...  Training loss: 4.5090...  0.3391 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8033...  Training loss: 4.6796...  0.3398 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8034...  Training loss: 4.6245...  0.3413 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8035...  Training loss: 4.5702...  0.3381 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8036...  Training loss: 4.5618...  0.3393 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8037...  Training loss: 4.5238...  0.3416 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8038...  Training loss: 4.5668...  0.3429 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8039...  Training loss: 4.5456...  0.3403 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8040...  Training loss: 4.5590...  0.3374 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8041...  Training loss: 4.6331...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8042...  Training loss: 4.6018...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8043...  Training loss: 4.6038...  0.3407 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8044...  Training loss: 4.5662...  0.3381 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8045...  Training loss: 4.5912...  0.3379 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8046...  Training loss: 4.4802...  0.3391 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8047...  Training loss: 4.5430...  0.3389 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8048...  Training loss: 4.4896...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8049...  Training loss: 4.5624...  0.3400 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8050...  Training loss: 4.5100...  0.3403 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8051...  Training loss: 4.5564...  0.3386 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8052...  Training loss: 4.5631...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8053...  Training loss: 4.6332...  0.3380 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8054...  Training loss: 4.5996...  0.3379 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8055...  Training loss: 4.4846...  0.3422 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8056...  Training loss: 4.4739...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8057...  Training loss: 4.5094...  0.3393 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8058...  Training loss: 4.5146...  0.3402 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8059...  Training loss: 4.5802...  0.3388 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8060...  Training loss: 4.5433...  0.3420 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8061...  Training loss: 4.5503...  0.3383 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8062...  Training loss: 4.5037...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8063...  Training loss: 4.5454...  0.3421 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8064...  Training loss: 4.4905...  0.3406 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8065...  Training loss: 4.5749...  0.3421 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8066...  Training loss: 4.5628...  0.3407 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8067...  Training loss: 4.6174...  0.3393 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8068...  Training loss: 4.5851...  0.3415 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8069...  Training loss: 4.6201...  0.3399 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8070...  Training loss: 4.6454...  0.3426 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8071...  Training loss: 4.4667...  0.3419 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8072...  Training loss: 4.5223...  0.3381 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8073...  Training loss: 4.5238...  0.3386 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8074...  Training loss: 4.5664...  0.3390 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8075...  Training loss: 4.5447...  0.3375 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8076...  Training loss: 4.5418...  0.3398 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8077...  Training loss: 4.6586...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8078...  Training loss: 4.5782...  0.3379 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8079...  Training loss: 4.5378...  0.3379 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8080...  Training loss: 4.5088...  0.3391 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/100...  Training Step: 8081...  Training loss: 4.5491...  0.3406 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8082...  Training loss: 4.5580...  0.3385 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8083...  Training loss: 4.5296...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8084...  Training loss: 4.4905...  0.3403 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8085...  Training loss: 4.5012...  0.3408 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8086...  Training loss: 4.4748...  0.3391 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8087...  Training loss: 4.4611...  0.3392 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8088...  Training loss: 4.5205...  0.3403 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8089...  Training loss: 4.5683...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8090...  Training loss: 4.5301...  0.3392 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8091...  Training loss: 4.5360...  0.3401 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8092...  Training loss: 4.4818...  0.3411 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8093...  Training loss: 4.6241...  0.3417 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8094...  Training loss: 4.5643...  0.3385 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8095...  Training loss: 4.5705...  0.3412 sec/batch\n",
      "Epoch: 44/100...  Training Step: 8096...  Training loss: 4.5725...  0.3375 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8097...  Training loss: 4.6223...  0.3424 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8098...  Training loss: 4.5029...  0.3394 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8099...  Training loss: 4.4554...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8100...  Training loss: 4.5131...  0.3422 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8101...  Training loss: 4.4539...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8102...  Training loss: 4.4058...  0.3373 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8103...  Training loss: 4.5435...  0.3391 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8104...  Training loss: 4.5305...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8105...  Training loss: 4.5272...  0.3428 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8106...  Training loss: 4.5201...  0.3418 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8107...  Training loss: 4.5017...  0.3385 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8108...  Training loss: 4.4912...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8109...  Training loss: 4.4699...  0.3407 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8110...  Training loss: 4.5151...  0.3391 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8111...  Training loss: 4.5405...  0.3395 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8112...  Training loss: 4.4993...  0.3411 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8113...  Training loss: 4.5841...  0.3422 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8114...  Training loss: 4.4931...  0.3417 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8115...  Training loss: 4.5693...  0.3384 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8116...  Training loss: 4.5252...  0.3383 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8117...  Training loss: 4.4880...  0.3417 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8118...  Training loss: 4.4007...  0.3416 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8119...  Training loss: 4.4479...  0.3424 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8120...  Training loss: 4.4537...  0.3400 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8121...  Training loss: 4.4898...  0.3439 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8122...  Training loss: 4.4591...  0.3408 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8123...  Training loss: 4.5082...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8124...  Training loss: 4.4823...  0.3415 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8125...  Training loss: 4.5434...  0.3425 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8126...  Training loss: 4.5744...  0.3394 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8127...  Training loss: 4.5591...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8128...  Training loss: 4.5101...  0.3405 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8129...  Training loss: 4.3197...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8130...  Training loss: 4.4170...  0.3422 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8131...  Training loss: 4.5206...  0.3387 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8132...  Training loss: 4.6364...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8133...  Training loss: 4.6007...  0.3384 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8134...  Training loss: 4.5966...  0.3401 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8135...  Training loss: 4.5439...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8136...  Training loss: 4.5577...  0.3383 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8137...  Training loss: 4.6034...  0.3381 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8138...  Training loss: 4.6272...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8139...  Training loss: 4.6617...  0.3423 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8140...  Training loss: 4.6085...  0.3410 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8141...  Training loss: 4.5374...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8142...  Training loss: 4.4957...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8143...  Training loss: 4.5701...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8144...  Training loss: 4.4721...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8145...  Training loss: 4.5090...  0.3418 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8146...  Training loss: 4.5918...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8147...  Training loss: 4.5804...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8148...  Training loss: 4.5505...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8149...  Training loss: 4.5759...  0.3382 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8150...  Training loss: 4.5815...  0.3382 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8151...  Training loss: 4.5737...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8152...  Training loss: 4.5338...  0.3407 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8153...  Training loss: 4.4696...  0.3383 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8154...  Training loss: 4.5501...  0.3388 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8155...  Training loss: 4.4581...  0.3379 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8156...  Training loss: 4.4610...  0.3427 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8157...  Training loss: 4.5020...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8158...  Training loss: 4.5611...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8159...  Training loss: 4.5404...  0.3391 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8160...  Training loss: 4.5827...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8161...  Training loss: 4.5131...  0.3416 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8162...  Training loss: 4.5131...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8163...  Training loss: 4.5021...  0.3394 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8164...  Training loss: 4.4366...  0.3386 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8165...  Training loss: 4.5497...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8166...  Training loss: 4.5153...  0.3376 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8167...  Training loss: 4.5021...  0.3406 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8168...  Training loss: 4.3931...  0.3368 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8169...  Training loss: 4.4209...  0.3368 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8170...  Training loss: 4.4223...  0.3408 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8171...  Training loss: 4.4355...  0.3403 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8172...  Training loss: 4.4195...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8173...  Training loss: 4.4850...  0.3408 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8174...  Training loss: 4.4274...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8175...  Training loss: 4.4921...  0.3403 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8176...  Training loss: 4.5246...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8177...  Training loss: 4.5525...  0.3381 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/100...  Training Step: 8178...  Training loss: 4.5067...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8179...  Training loss: 4.5117...  0.3383 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8180...  Training loss: 4.5195...  0.3417 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8181...  Training loss: 4.4993...  0.3415 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8182...  Training loss: 4.5279...  0.3421 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8183...  Training loss: 4.4677...  0.3388 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8184...  Training loss: 4.4765...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8185...  Training loss: 4.5201...  0.3401 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8186...  Training loss: 4.5975...  0.3399 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8187...  Training loss: 4.5110...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8188...  Training loss: 4.4048...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8189...  Training loss: 4.5086...  0.3427 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8190...  Training loss: 4.5122...  0.3378 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8191...  Training loss: 4.5342...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8192...  Training loss: 4.4982...  0.3378 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8193...  Training loss: 4.4582...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8194...  Training loss: 4.4309...  0.3379 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8195...  Training loss: 4.4596...  0.3382 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8196...  Training loss: 4.4598...  0.3388 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8197...  Training loss: 4.5019...  0.3438 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8198...  Training loss: 4.4640...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8199...  Training loss: 4.5585...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8200...  Training loss: 4.4714...  0.3387 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8201...  Training loss: 4.5505...  0.3376 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8202...  Training loss: 4.5113...  0.3387 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8203...  Training loss: 4.4884...  0.3396 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8204...  Training loss: 4.4190...  0.3423 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8205...  Training loss: 4.3876...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8206...  Training loss: 4.4016...  0.3391 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8207...  Training loss: 4.4454...  0.3423 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8208...  Training loss: 4.4801...  0.3396 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8209...  Training loss: 4.4577...  0.3440 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8210...  Training loss: 4.3931...  0.3415 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8211...  Training loss: 4.4473...  0.3407 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8212...  Training loss: 4.4287...  0.3432 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8213...  Training loss: 4.5529...  0.3395 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8214...  Training loss: 4.4127...  0.3405 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8215...  Training loss: 4.4372...  0.3385 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8216...  Training loss: 4.4759...  0.3405 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8217...  Training loss: 4.6780...  0.3396 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8218...  Training loss: 4.6021...  0.3419 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8219...  Training loss: 4.5488...  0.3431 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8220...  Training loss: 4.5176...  0.3377 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8221...  Training loss: 4.5135...  0.3415 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8222...  Training loss: 4.5305...  0.3395 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8223...  Training loss: 4.5426...  0.3440 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8224...  Training loss: 4.5397...  0.3389 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8225...  Training loss: 4.6307...  0.3408 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8226...  Training loss: 4.5783...  0.3378 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8227...  Training loss: 4.5845...  0.3377 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8228...  Training loss: 4.5521...  0.3373 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8229...  Training loss: 4.5604...  0.3384 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8230...  Training loss: 4.4505...  0.3380 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8231...  Training loss: 4.5047...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8232...  Training loss: 4.4414...  0.3379 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8233...  Training loss: 4.5552...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8234...  Training loss: 4.4821...  0.3385 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8235...  Training loss: 4.5745...  0.3420 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8236...  Training loss: 4.5549...  0.3427 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8237...  Training loss: 4.6128...  0.3392 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8238...  Training loss: 4.5929...  0.3398 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8239...  Training loss: 4.4640...  0.3408 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8240...  Training loss: 4.4335...  0.3431 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8241...  Training loss: 4.4684...  0.3385 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8242...  Training loss: 4.4858...  0.3400 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8243...  Training loss: 4.5438...  0.3371 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8244...  Training loss: 4.5444...  0.3375 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8245...  Training loss: 4.5462...  0.3384 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8246...  Training loss: 4.4716...  0.3386 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8247...  Training loss: 4.5169...  0.3424 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8248...  Training loss: 4.4539...  0.3390 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8249...  Training loss: 4.5407...  0.3386 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8250...  Training loss: 4.5128...  0.3380 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8251...  Training loss: 4.5675...  0.3411 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8252...  Training loss: 4.5780...  0.3416 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8253...  Training loss: 4.5922...  0.3410 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8254...  Training loss: 4.6216...  0.3410 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8255...  Training loss: 4.4653...  0.3406 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8256...  Training loss: 4.5020...  0.3409 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8257...  Training loss: 4.5180...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8258...  Training loss: 4.5506...  0.3397 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8259...  Training loss: 4.5130...  0.3396 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8260...  Training loss: 4.4762...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8261...  Training loss: 4.6110...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8262...  Training loss: 4.5324...  0.3402 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8263...  Training loss: 4.5285...  0.3399 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8264...  Training loss: 4.5103...  0.3380 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8265...  Training loss: 4.5214...  0.3380 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8266...  Training loss: 4.5194...  0.3396 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8267...  Training loss: 4.5141...  0.3382 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8268...  Training loss: 4.4638...  0.3393 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8269...  Training loss: 4.4883...  0.3417 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8270...  Training loss: 4.4746...  0.3409 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8271...  Training loss: 4.4207...  0.3388 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8272...  Training loss: 4.4827...  0.3400 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8273...  Training loss: 4.5536...  0.3383 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8274...  Training loss: 4.5315...  0.3415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/100...  Training Step: 8275...  Training loss: 4.5408...  0.3399 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8276...  Training loss: 4.4816...  0.3387 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8277...  Training loss: 4.6048...  0.3414 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8278...  Training loss: 4.5378...  0.3387 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8279...  Training loss: 4.5384...  0.3367 sec/batch\n",
      "Epoch: 45/100...  Training Step: 8280...  Training loss: 4.5496...  0.3416 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8281...  Training loss: 4.5982...  0.3407 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8282...  Training loss: 4.4911...  0.3401 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8283...  Training loss: 4.4116...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8284...  Training loss: 4.4755...  0.3402 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8285...  Training loss: 4.4326...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8286...  Training loss: 4.3901...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8287...  Training loss: 4.5676...  0.3413 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8288...  Training loss: 4.5330...  0.3413 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8289...  Training loss: 4.5178...  0.3391 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8290...  Training loss: 4.5263...  0.3407 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8291...  Training loss: 4.4802...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8292...  Training loss: 4.4568...  0.3393 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8293...  Training loss: 4.4456...  0.3401 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8294...  Training loss: 4.4860...  0.3405 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8295...  Training loss: 4.5390...  0.3431 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8296...  Training loss: 4.4697...  0.3384 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8297...  Training loss: 4.5562...  0.3428 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8298...  Training loss: 4.4735...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8299...  Training loss: 4.5342...  0.3388 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8300...  Training loss: 4.5013...  0.3376 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8301...  Training loss: 4.4876...  0.3405 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8302...  Training loss: 4.4064...  0.3406 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8303...  Training loss: 4.4303...  0.3385 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8304...  Training loss: 4.4263...  0.3386 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8305...  Training loss: 4.4904...  0.3396 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8306...  Training loss: 4.4556...  0.3384 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8307...  Training loss: 4.4739...  0.3374 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8308...  Training loss: 4.4829...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8309...  Training loss: 4.5077...  0.3408 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8310...  Training loss: 4.5682...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8311...  Training loss: 4.5373...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8312...  Training loss: 4.5269...  0.3443 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8313...  Training loss: 4.3216...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8314...  Training loss: 4.3715...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8315...  Training loss: 4.4900...  0.3415 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8316...  Training loss: 4.6016...  0.3402 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8317...  Training loss: 4.5853...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8318...  Training loss: 4.6043...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8319...  Training loss: 4.5313...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8320...  Training loss: 4.5655...  0.3418 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8321...  Training loss: 4.5874...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8322...  Training loss: 4.5821...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8323...  Training loss: 4.6345...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8324...  Training loss: 4.5911...  0.3380 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8325...  Training loss: 4.5251...  0.3389 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8326...  Training loss: 4.5006...  0.3409 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8327...  Training loss: 4.5755...  0.3384 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8328...  Training loss: 4.4462...  0.3425 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8329...  Training loss: 4.5219...  0.3428 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8330...  Training loss: 4.5761...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8331...  Training loss: 4.5508...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8332...  Training loss: 4.5598...  0.3417 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8333...  Training loss: 4.5443...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8334...  Training loss: 4.5898...  0.3389 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8335...  Training loss: 4.5514...  0.3398 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8336...  Training loss: 4.5330...  0.3419 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8337...  Training loss: 4.4776...  0.3410 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8338...  Training loss: 4.5264...  0.3380 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8339...  Training loss: 4.4239...  0.3410 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8340...  Training loss: 4.4086...  0.3386 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8341...  Training loss: 4.4722...  0.3417 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8342...  Training loss: 4.5105...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8343...  Training loss: 4.5007...  0.3396 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8344...  Training loss: 4.5576...  0.3376 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8345...  Training loss: 4.4660...  0.3431 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8346...  Training loss: 4.5030...  0.3402 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8347...  Training loss: 4.4800...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8348...  Training loss: 4.4116...  0.3402 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8349...  Training loss: 4.5296...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8350...  Training loss: 4.4990...  0.3413 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8351...  Training loss: 4.4849...  0.3378 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8352...  Training loss: 4.3709...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8353...  Training loss: 4.4046...  0.3407 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8354...  Training loss: 4.4221...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8355...  Training loss: 4.4213...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8356...  Training loss: 4.4429...  0.3418 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8357...  Training loss: 4.4632...  0.3411 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8358...  Training loss: 4.4140...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8359...  Training loss: 4.4730...  0.3388 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8360...  Training loss: 4.5046...  0.3389 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8361...  Training loss: 4.5418...  0.3413 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8362...  Training loss: 4.5167...  0.3388 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8363...  Training loss: 4.4800...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8364...  Training loss: 4.4721...  0.3401 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8365...  Training loss: 4.4969...  0.3418 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8366...  Training loss: 4.4827...  0.3420 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8367...  Training loss: 4.4531...  0.3376 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8368...  Training loss: 4.4652...  0.3427 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8369...  Training loss: 4.5341...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8370...  Training loss: 4.5565...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8371...  Training loss: 4.4914...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46/100...  Training Step: 8372...  Training loss: 4.3786...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8373...  Training loss: 4.4785...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8374...  Training loss: 4.4641...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8375...  Training loss: 4.4808...  0.3385 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8376...  Training loss: 4.4669...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8377...  Training loss: 4.4203...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8378...  Training loss: 4.3895...  0.3422 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8379...  Training loss: 4.4131...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8380...  Training loss: 4.4429...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8381...  Training loss: 4.4511...  0.3401 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8382...  Training loss: 4.4446...  0.3386 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8383...  Training loss: 4.5459...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8384...  Training loss: 4.4009...  0.3389 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8385...  Training loss: 4.5001...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8386...  Training loss: 4.5150...  0.3438 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8387...  Training loss: 4.4662...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8388...  Training loss: 4.3914...  0.3388 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8389...  Training loss: 4.3668...  0.3416 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8390...  Training loss: 4.3992...  0.3376 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8391...  Training loss: 4.4033...  0.3380 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8392...  Training loss: 4.4127...  0.3409 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8393...  Training loss: 4.4213...  0.3402 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8394...  Training loss: 4.3590...  0.3393 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8395...  Training loss: 4.4134...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8396...  Training loss: 4.3834...  0.3367 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8397...  Training loss: 4.4800...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8398...  Training loss: 4.3868...  0.3396 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8399...  Training loss: 4.4249...  0.3439 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8400...  Training loss: 4.4348...  0.3411 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8401...  Training loss: 4.6222...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8402...  Training loss: 4.5760...  0.3430 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8403...  Training loss: 4.5260...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8404...  Training loss: 4.4843...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8405...  Training loss: 4.4767...  0.3399 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8406...  Training loss: 4.4731...  0.3393 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8407...  Training loss: 4.4873...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8408...  Training loss: 4.5065...  0.3393 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8409...  Training loss: 4.5933...  0.3419 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8410...  Training loss: 4.5407...  0.3408 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8411...  Training loss: 4.5536...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8412...  Training loss: 4.5392...  0.3423 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8413...  Training loss: 4.5477...  0.3406 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8414...  Training loss: 4.4403...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8415...  Training loss: 4.4956...  0.3374 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8416...  Training loss: 4.4205...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8417...  Training loss: 4.4801...  0.3406 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8418...  Training loss: 4.4469...  0.3417 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8419...  Training loss: 4.4984...  0.3405 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8420...  Training loss: 4.5012...  0.3396 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8421...  Training loss: 4.5645...  0.3376 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8422...  Training loss: 4.5710...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8423...  Training loss: 4.4538...  0.3425 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8424...  Training loss: 4.4160...  0.3393 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8425...  Training loss: 4.4312...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8426...  Training loss: 4.4466...  0.3414 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8427...  Training loss: 4.4975...  0.3385 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8428...  Training loss: 4.4949...  0.3394 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8429...  Training loss: 4.5249...  0.3375 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8430...  Training loss: 4.4644...  0.3414 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8431...  Training loss: 4.5072...  0.3380 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8432...  Training loss: 4.4215...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8433...  Training loss: 4.5160...  0.3401 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8434...  Training loss: 4.4964...  0.3386 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8435...  Training loss: 4.5472...  0.3388 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8436...  Training loss: 4.5233...  0.3430 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8437...  Training loss: 4.5744...  0.3397 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8438...  Training loss: 4.6017...  0.3397 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8439...  Training loss: 4.4769...  0.3397 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8440...  Training loss: 4.5043...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8441...  Training loss: 4.5320...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8442...  Training loss: 4.5223...  0.3414 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8443...  Training loss: 4.4737...  0.3392 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8444...  Training loss: 4.4666...  0.3391 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8445...  Training loss: 4.5921...  0.3380 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8446...  Training loss: 4.5526...  0.3390 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8447...  Training loss: 4.5027...  0.3427 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8448...  Training loss: 4.4745...  0.3427 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8449...  Training loss: 4.5094...  0.3426 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8450...  Training loss: 4.5211...  0.3413 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8451...  Training loss: 4.4933...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8452...  Training loss: 4.4465...  0.3375 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8453...  Training loss: 4.4573...  0.3414 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8454...  Training loss: 4.4271...  0.3400 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8455...  Training loss: 4.4136...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8456...  Training loss: 4.4425...  0.3398 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8457...  Training loss: 4.5001...  0.3395 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8458...  Training loss: 4.4706...  0.3412 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8459...  Training loss: 4.4960...  0.3407 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8460...  Training loss: 4.4561...  0.3386 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8461...  Training loss: 4.5852...  0.3419 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8462...  Training loss: 4.5175...  0.3403 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8463...  Training loss: 4.5445...  0.3387 sec/batch\n",
      "Epoch: 46/100...  Training Step: 8464...  Training loss: 4.5606...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8465...  Training loss: 4.5532...  0.3422 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8466...  Training loss: 4.4439...  0.3393 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8467...  Training loss: 4.3729...  0.3410 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8468...  Training loss: 4.4360...  0.3409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/100...  Training Step: 8469...  Training loss: 4.3723...  0.3411 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8470...  Training loss: 4.3379...  0.3407 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8471...  Training loss: 4.5390...  0.3390 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8472...  Training loss: 4.4661...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8473...  Training loss: 4.4802...  0.3395 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8474...  Training loss: 4.4825...  0.3375 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8475...  Training loss: 4.4547...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8476...  Training loss: 4.4265...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8477...  Training loss: 4.4178...  0.3417 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8478...  Training loss: 4.4360...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8479...  Training loss: 4.4832...  0.3408 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8480...  Training loss: 4.4294...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8481...  Training loss: 4.5386...  0.3414 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8482...  Training loss: 4.4445...  0.3400 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8483...  Training loss: 4.5445...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8484...  Training loss: 4.4763...  0.3389 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8485...  Training loss: 4.4665...  0.3389 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8486...  Training loss: 4.4011...  0.3390 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8487...  Training loss: 4.3800...  0.3401 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8488...  Training loss: 4.4240...  0.3406 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8489...  Training loss: 4.4726...  0.3400 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8490...  Training loss: 4.4534...  0.3395 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8491...  Training loss: 4.4876...  0.3387 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8492...  Training loss: 4.4613...  0.3388 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8493...  Training loss: 4.4857...  0.3443 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8494...  Training loss: 4.5402...  0.3401 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8495...  Training loss: 4.5073...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8496...  Training loss: 4.4919...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8497...  Training loss: 4.3091...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8498...  Training loss: 4.3655...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8499...  Training loss: 4.4824...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8500...  Training loss: 4.5776...  0.3419 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8501...  Training loss: 4.5682...  0.3387 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8502...  Training loss: 4.5668...  0.3373 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8503...  Training loss: 4.5168...  0.3397 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8504...  Training loss: 4.5384...  0.3462 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8505...  Training loss: 4.5809...  0.3416 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8506...  Training loss: 4.5774...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8507...  Training loss: 4.6100...  0.3384 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8508...  Training loss: 4.5655...  0.3403 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8509...  Training loss: 4.5053...  0.3388 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8510...  Training loss: 4.4840...  0.3394 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8511...  Training loss: 4.5498...  0.3409 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8512...  Training loss: 4.4441...  0.3415 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8513...  Training loss: 4.5100...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8514...  Training loss: 4.5612...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8515...  Training loss: 4.5518...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8516...  Training loss: 4.5444...  0.3393 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8517...  Training loss: 4.5552...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8518...  Training loss: 4.5874...  0.3395 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8519...  Training loss: 4.5454...  0.3402 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8520...  Training loss: 4.5111...  0.3410 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8521...  Training loss: 4.4501...  0.3384 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8522...  Training loss: 4.4920...  0.3407 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8523...  Training loss: 4.3907...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8524...  Training loss: 4.3998...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8525...  Training loss: 4.4328...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8526...  Training loss: 4.4859...  0.3416 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8527...  Training loss: 4.4769...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8528...  Training loss: 4.5087...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8529...  Training loss: 4.4456...  0.3413 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8530...  Training loss: 4.4352...  0.3382 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8531...  Training loss: 4.4774...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8532...  Training loss: 4.4198...  0.3393 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8533...  Training loss: 4.5276...  0.3408 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8534...  Training loss: 4.5103...  0.3422 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8535...  Training loss: 4.4866...  0.3403 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8536...  Training loss: 4.3456...  0.3377 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8537...  Training loss: 4.4033...  0.3411 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8538...  Training loss: 4.4183...  0.3381 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8539...  Training loss: 4.4102...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8540...  Training loss: 4.4310...  0.3417 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8541...  Training loss: 4.4620...  0.3382 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8542...  Training loss: 4.3930...  0.3407 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8543...  Training loss: 4.4674...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8544...  Training loss: 4.4682...  0.3403 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8545...  Training loss: 4.4999...  0.3382 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8546...  Training loss: 4.4762...  0.3402 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8547...  Training loss: 4.4420...  0.3390 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8548...  Training loss: 4.4653...  0.3381 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8549...  Training loss: 4.4881...  0.3413 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8550...  Training loss: 4.4683...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8551...  Training loss: 4.4229...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8552...  Training loss: 4.4166...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8553...  Training loss: 4.5128...  0.3410 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8554...  Training loss: 4.5615...  0.3402 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8555...  Training loss: 4.4752...  0.3410 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8556...  Training loss: 4.3664...  0.3406 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8557...  Training loss: 4.4388...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8558...  Training loss: 4.4414...  0.3394 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8559...  Training loss: 4.4656...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8560...  Training loss: 4.4491...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8561...  Training loss: 4.3942...  0.3423 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8562...  Training loss: 4.3489...  0.3400 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8563...  Training loss: 4.4166...  0.3371 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8564...  Training loss: 4.4248...  0.3411 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8565...  Training loss: 4.4400...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/100...  Training Step: 8566...  Training loss: 4.4086...  0.3412 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8567...  Training loss: 4.5080...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8568...  Training loss: 4.3895...  0.3398 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8569...  Training loss: 4.4948...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8570...  Training loss: 4.4804...  0.3394 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8571...  Training loss: 4.4383...  0.3421 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8572...  Training loss: 4.4102...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8573...  Training loss: 4.3957...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8574...  Training loss: 4.4074...  0.3408 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8575...  Training loss: 4.4012...  0.3403 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8576...  Training loss: 4.3954...  0.3401 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8577...  Training loss: 4.4115...  0.3408 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8578...  Training loss: 4.3476...  0.3406 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8579...  Training loss: 4.4201...  0.3418 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8580...  Training loss: 4.3629...  0.3390 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8581...  Training loss: 4.4538...  0.3389 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8582...  Training loss: 4.3590...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8583...  Training loss: 4.4032...  0.3397 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8584...  Training loss: 4.4187...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8585...  Training loss: 4.5839...  0.3379 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8586...  Training loss: 4.5459...  0.3390 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8587...  Training loss: 4.5047...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8588...  Training loss: 4.4698...  0.3397 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8589...  Training loss: 4.4597...  0.3414 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8590...  Training loss: 4.4502...  0.3400 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8591...  Training loss: 4.4559...  0.3411 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8592...  Training loss: 4.4797...  0.3407 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8593...  Training loss: 4.5641...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8594...  Training loss: 4.5347...  0.3420 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8595...  Training loss: 4.5345...  0.3420 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8596...  Training loss: 4.4932...  0.3425 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8597...  Training loss: 4.5078...  0.3421 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8598...  Training loss: 4.4314...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8599...  Training loss: 4.4938...  0.3401 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8600...  Training loss: 4.3985...  0.3382 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8601...  Training loss: 4.4834...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8602...  Training loss: 4.4319...  0.3418 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8603...  Training loss: 4.5006...  0.3394 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8604...  Training loss: 4.5012...  0.3393 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8605...  Training loss: 4.5392...  0.3425 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8606...  Training loss: 4.5258...  0.3412 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8607...  Training loss: 4.4148...  0.3383 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8608...  Training loss: 4.3757...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8609...  Training loss: 4.4072...  0.3415 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8610...  Training loss: 4.4431...  0.3381 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8611...  Training loss: 4.4731...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8612...  Training loss: 4.4600...  0.3410 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8613...  Training loss: 4.4722...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8614...  Training loss: 4.4171...  0.3402 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8615...  Training loss: 4.4603...  0.3424 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8616...  Training loss: 4.3926...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8617...  Training loss: 4.4765...  0.3405 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8618...  Training loss: 4.4647...  0.3387 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8619...  Training loss: 4.5057...  0.3396 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8620...  Training loss: 4.5046...  0.3409 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8621...  Training loss: 4.5322...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8622...  Training loss: 4.5815...  0.3426 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8623...  Training loss: 4.4431...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8624...  Training loss: 4.4836...  0.3411 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8625...  Training loss: 4.5028...  0.3378 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8626...  Training loss: 4.5020...  0.3391 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8627...  Training loss: 4.4607...  0.3380 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8628...  Training loss: 4.4478...  0.3395 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8629...  Training loss: 4.5611...  0.3406 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8630...  Training loss: 4.5032...  0.3399 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8631...  Training loss: 4.4788...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8632...  Training loss: 4.4597...  0.3414 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8633...  Training loss: 4.5056...  0.3382 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8634...  Training loss: 4.4874...  0.3420 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8635...  Training loss: 4.4643...  0.3387 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8636...  Training loss: 4.4203...  0.3385 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8637...  Training loss: 4.4583...  0.3378 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8638...  Training loss: 4.4096...  0.3392 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8639...  Training loss: 4.4173...  0.3412 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8640...  Training loss: 4.4455...  0.3380 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8641...  Training loss: 4.4807...  0.3414 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8642...  Training loss: 4.4576...  0.3386 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8643...  Training loss: 4.4526...  0.3404 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8644...  Training loss: 4.4384...  0.3417 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8645...  Training loss: 4.5578...  0.3433 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8646...  Training loss: 4.5164...  0.3395 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8647...  Training loss: 4.5196...  0.3406 sec/batch\n",
      "Epoch: 47/100...  Training Step: 8648...  Training loss: 4.5267...  0.3421 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8649...  Training loss: 4.5323...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8650...  Training loss: 4.4149...  0.3419 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8651...  Training loss: 4.3498...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8652...  Training loss: 4.4357...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8653...  Training loss: 4.3995...  0.3423 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8654...  Training loss: 4.3549...  0.3420 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8655...  Training loss: 4.5269...  0.3405 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8656...  Training loss: 4.4958...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8657...  Training loss: 4.4883...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8658...  Training loss: 4.4940...  0.3381 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8659...  Training loss: 4.4475...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8660...  Training loss: 4.4404...  0.3395 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8661...  Training loss: 4.4163...  0.3423 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8662...  Training loss: 4.4467...  0.3398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/100...  Training Step: 8663...  Training loss: 4.4755...  0.3398 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8664...  Training loss: 4.4184...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8665...  Training loss: 4.5033...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8666...  Training loss: 4.4356...  0.3379 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8667...  Training loss: 4.5344...  0.3383 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8668...  Training loss: 4.4482...  0.3428 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8669...  Training loss: 4.4181...  0.3416 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8670...  Training loss: 4.3714...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8671...  Training loss: 4.3755...  0.3385 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8672...  Training loss: 4.3737...  0.3399 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8673...  Training loss: 4.4391...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8674...  Training loss: 4.4469...  0.3399 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8675...  Training loss: 4.4739...  0.3416 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8676...  Training loss: 4.4683...  0.3436 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8677...  Training loss: 4.5122...  0.3385 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8678...  Training loss: 4.5298...  0.3426 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8679...  Training loss: 4.4918...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8680...  Training loss: 4.4844...  0.3428 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8681...  Training loss: 4.2917...  0.3438 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8682...  Training loss: 4.3513...  0.3421 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8683...  Training loss: 4.4460...  0.3433 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8684...  Training loss: 4.5671...  0.3384 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8685...  Training loss: 4.5287...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8686...  Training loss: 4.5341...  0.3402 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8687...  Training loss: 4.5044...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8688...  Training loss: 4.4932...  0.3416 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8689...  Training loss: 4.5416...  0.3385 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8690...  Training loss: 4.5476...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8691...  Training loss: 4.5834...  0.3386 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8692...  Training loss: 4.5702...  0.3425 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8693...  Training loss: 4.5010...  0.3398 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8694...  Training loss: 4.4485...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8695...  Training loss: 4.5088...  0.3392 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8696...  Training loss: 4.4097...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8697...  Training loss: 4.4790...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8698...  Training loss: 4.5129...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8699...  Training loss: 4.5122...  0.3426 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8700...  Training loss: 4.5237...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8701...  Training loss: 4.5172...  0.3411 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8702...  Training loss: 4.5335...  0.3425 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8703...  Training loss: 4.5329...  0.3421 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8704...  Training loss: 4.5022...  0.3407 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8705...  Training loss: 4.4226...  0.3415 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8706...  Training loss: 4.4770...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8707...  Training loss: 4.3967...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8708...  Training loss: 4.3670...  0.3425 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8709...  Training loss: 4.4141...  0.3415 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8710...  Training loss: 4.4655...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8711...  Training loss: 4.4592...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8712...  Training loss: 4.5235...  0.3387 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8713...  Training loss: 4.3960...  0.3407 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8714...  Training loss: 4.4248...  0.3394 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8715...  Training loss: 4.4397...  0.3427 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8716...  Training loss: 4.3878...  0.3394 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8717...  Training loss: 4.4932...  0.3380 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8718...  Training loss: 4.4398...  0.3376 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8719...  Training loss: 4.4473...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8720...  Training loss: 4.3288...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8721...  Training loss: 4.3801...  0.3400 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8722...  Training loss: 4.3877...  0.3393 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8723...  Training loss: 4.3659...  0.3409 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8724...  Training loss: 4.4026...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8725...  Training loss: 4.4270...  0.3383 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8726...  Training loss: 4.3952...  0.3400 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8727...  Training loss: 4.4480...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8728...  Training loss: 4.4283...  0.3394 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8729...  Training loss: 4.4817...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8730...  Training loss: 4.4401...  0.3398 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8731...  Training loss: 4.4621...  0.3400 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8732...  Training loss: 4.4437...  0.3405 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8733...  Training loss: 4.4474...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8734...  Training loss: 4.4856...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8735...  Training loss: 4.3982...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8736...  Training loss: 4.3922...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8737...  Training loss: 4.4811...  0.3408 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8738...  Training loss: 4.5342...  0.3405 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8739...  Training loss: 4.4543...  0.3435 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8740...  Training loss: 4.3472...  0.3412 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8741...  Training loss: 4.4353...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8742...  Training loss: 4.4337...  0.3399 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8743...  Training loss: 4.4398...  0.3385 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8744...  Training loss: 4.4266...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8745...  Training loss: 4.3793...  0.3387 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8746...  Training loss: 4.3439...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8747...  Training loss: 4.4071...  0.3389 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8748...  Training loss: 4.4136...  0.3421 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8749...  Training loss: 4.4054...  0.3392 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8750...  Training loss: 4.3619...  0.3406 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8751...  Training loss: 4.4740...  0.3385 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8752...  Training loss: 4.3653...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8753...  Training loss: 4.4885...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8754...  Training loss: 4.4778...  0.3380 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8755...  Training loss: 4.4318...  0.3402 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8756...  Training loss: 4.4096...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8757...  Training loss: 4.4098...  0.3402 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8758...  Training loss: 4.3829...  0.3407 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8759...  Training loss: 4.4314...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/100...  Training Step: 8760...  Training loss: 4.4239...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8761...  Training loss: 4.4194...  0.3383 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8762...  Training loss: 4.3472...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8763...  Training loss: 4.4038...  0.3398 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8764...  Training loss: 4.3536...  0.3407 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8765...  Training loss: 4.4379...  0.3394 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8766...  Training loss: 4.3441...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8767...  Training loss: 4.3889...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8768...  Training loss: 4.3990...  0.3387 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8769...  Training loss: 4.5612...  0.3436 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8770...  Training loss: 4.4982...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8771...  Training loss: 4.4546...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8772...  Training loss: 4.4157...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8773...  Training loss: 4.4271...  0.3399 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8774...  Training loss: 4.4382...  0.3414 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8775...  Training loss: 4.4347...  0.3411 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8776...  Training loss: 4.4482...  0.3390 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8777...  Training loss: 4.5405...  0.3392 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8778...  Training loss: 4.4788...  0.3375 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8779...  Training loss: 4.5082...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8780...  Training loss: 4.4557...  0.3423 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8781...  Training loss: 4.5017...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8782...  Training loss: 4.4030...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8783...  Training loss: 4.4717...  0.3389 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8784...  Training loss: 4.3883...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8785...  Training loss: 4.4528...  0.3387 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8786...  Training loss: 4.4118...  0.3392 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8787...  Training loss: 4.4629...  0.3393 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8788...  Training loss: 4.4652...  0.3384 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8789...  Training loss: 4.5328...  0.3383 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8790...  Training loss: 4.4969...  0.3438 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8791...  Training loss: 4.4039...  0.3429 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8792...  Training loss: 4.3548...  0.3387 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8793...  Training loss: 4.3871...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8794...  Training loss: 4.4240...  0.3392 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8795...  Training loss: 4.4453...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8796...  Training loss: 4.4426...  0.3391 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8797...  Training loss: 4.4618...  0.3378 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8798...  Training loss: 4.3687...  0.3416 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8799...  Training loss: 4.4600...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8800...  Training loss: 4.3800...  0.3405 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8801...  Training loss: 4.4437...  0.3386 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8802...  Training loss: 4.4244...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8803...  Training loss: 4.5086...  0.3403 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8804...  Training loss: 4.4818...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8805...  Training loss: 4.5177...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8806...  Training loss: 4.5706...  0.3413 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8807...  Training loss: 4.4335...  0.3396 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8808...  Training loss: 4.4584...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8809...  Training loss: 4.4981...  0.3383 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8810...  Training loss: 4.4858...  0.3424 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8811...  Training loss: 4.4468...  0.3415 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8812...  Training loss: 4.4299...  0.3388 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8813...  Training loss: 4.5346...  0.3394 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8814...  Training loss: 4.4812...  0.3420 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8815...  Training loss: 4.4366...  0.3418 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8816...  Training loss: 4.4267...  0.3427 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8817...  Training loss: 4.4745...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8818...  Training loss: 4.4688...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8819...  Training loss: 4.4452...  0.3397 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8820...  Training loss: 4.4158...  0.3399 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8821...  Training loss: 4.3990...  0.3411 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8822...  Training loss: 4.3922...  0.3389 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8823...  Training loss: 4.3771...  0.3382 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8824...  Training loss: 4.4333...  0.3401 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8825...  Training loss: 4.4799...  0.3381 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8826...  Training loss: 4.4603...  0.3416 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8827...  Training loss: 4.4484...  0.3404 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8828...  Training loss: 4.3994...  0.3372 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8829...  Training loss: 4.5092...  0.3405 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8830...  Training loss: 4.4746...  0.3410 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8831...  Training loss: 4.5027...  0.3411 sec/batch\n",
      "Epoch: 48/100...  Training Step: 8832...  Training loss: 4.4782...  0.3392 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8833...  Training loss: 4.5275...  0.3406 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8834...  Training loss: 4.4111...  0.3398 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8835...  Training loss: 4.3558...  0.3400 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8836...  Training loss: 4.4343...  0.3393 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8837...  Training loss: 4.3763...  0.3442 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8838...  Training loss: 4.3423...  0.3400 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8839...  Training loss: 4.5368...  0.3396 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8840...  Training loss: 4.4827...  0.3384 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8841...  Training loss: 4.4539...  0.3387 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8842...  Training loss: 4.4859...  0.3377 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8843...  Training loss: 4.4443...  0.3402 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8844...  Training loss: 4.4122...  0.3395 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8845...  Training loss: 4.3997...  0.3401 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8846...  Training loss: 4.4600...  0.3412 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8847...  Training loss: 4.4918...  0.3388 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8848...  Training loss: 4.4116...  0.3404 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8849...  Training loss: 4.4952...  0.3415 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8850...  Training loss: 4.4384...  0.3373 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8851...  Training loss: 4.5224...  0.3412 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8852...  Training loss: 4.4895...  0.3383 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8853...  Training loss: 4.4543...  0.3386 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8854...  Training loss: 4.3594...  0.3391 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8855...  Training loss: 4.3939...  0.3395 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8856...  Training loss: 4.4054...  0.3434 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/100...  Training Step: 8857...  Training loss: 4.4366...  0.3420 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8858...  Training loss: 4.4206...  0.3407 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8859...  Training loss: 4.4663...  0.3391 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8860...  Training loss: 4.4193...  0.3376 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8861...  Training loss: 4.5061...  0.3376 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8862...  Training loss: 4.5439...  0.3396 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8863...  Training loss: 4.5104...  0.3408 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8864...  Training loss: 4.5068...  0.3390 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8865...  Training loss: 4.3076...  0.3400 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8866...  Training loss: 4.3746...  0.3392 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8867...  Training loss: 4.4452...  0.3395 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8868...  Training loss: 4.5712...  0.3393 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8869...  Training loss: 4.5310...  0.3393 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8870...  Training loss: 4.5429...  0.3389 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8871...  Training loss: 4.4910...  0.3413 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8872...  Training loss: 4.5028...  0.3413 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8873...  Training loss: 4.5307...  0.3399 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8874...  Training loss: 4.5514...  0.3402 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8875...  Training loss: 4.5874...  0.3404 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8876...  Training loss: 4.5353...  0.3424 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8877...  Training loss: 4.5039...  0.3425 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8878...  Training loss: 4.4626...  0.3412 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8879...  Training loss: 4.5263...  0.3435 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8880...  Training loss: 4.4199...  0.3432 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8881...  Training loss: 4.4569...  0.3442 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8882...  Training loss: 4.5057...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8883...  Training loss: 4.4681...  0.3438 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8884...  Training loss: 4.5008...  0.3437 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8885...  Training loss: 4.5006...  0.3444 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8886...  Training loss: 4.5357...  0.3422 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8887...  Training loss: 4.5116...  0.3436 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8888...  Training loss: 4.4955...  0.3423 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8889...  Training loss: 4.4311...  0.3402 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8890...  Training loss: 4.4734...  0.3390 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8891...  Training loss: 4.3864...  0.3419 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8892...  Training loss: 4.3804...  0.3401 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8893...  Training loss: 4.4103...  0.3437 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8894...  Training loss: 4.4493...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8895...  Training loss: 4.4503...  0.3404 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8896...  Training loss: 4.4834...  0.3430 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8897...  Training loss: 4.3971...  0.3454 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8898...  Training loss: 4.4165...  0.3396 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8899...  Training loss: 4.4078...  0.3402 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8900...  Training loss: 4.3592...  0.3411 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8901...  Training loss: 4.4638...  0.3396 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8902...  Training loss: 4.4358...  0.3394 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8903...  Training loss: 4.4225...  0.3435 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8904...  Training loss: 4.3158...  0.3415 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8905...  Training loss: 4.3462...  0.3418 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8906...  Training loss: 4.3813...  0.3423 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8907...  Training loss: 4.3439...  0.3405 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8908...  Training loss: 4.3790...  0.3399 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8909...  Training loss: 4.4015...  0.3391 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8910...  Training loss: 4.3567...  0.3411 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8911...  Training loss: 4.4046...  0.3445 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8912...  Training loss: 4.3917...  0.3423 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8913...  Training loss: 4.4651...  0.3408 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8914...  Training loss: 4.4113...  0.3432 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8915...  Training loss: 4.4362...  0.3418 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8916...  Training loss: 4.3744...  0.3428 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8917...  Training loss: 4.4438...  0.3410 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8918...  Training loss: 4.4509...  0.3434 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8919...  Training loss: 4.3835...  0.3398 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8920...  Training loss: 4.3971...  0.3425 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8921...  Training loss: 4.4664...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8922...  Training loss: 4.5135...  0.3406 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8923...  Training loss: 4.4318...  0.3420 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8924...  Training loss: 4.3380...  0.3437 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8925...  Training loss: 4.4553...  0.3420 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8926...  Training loss: 4.4362...  0.3408 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8927...  Training loss: 4.4837...  0.3405 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8928...  Training loss: 4.4543...  0.3428 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8929...  Training loss: 4.3673...  0.3404 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8930...  Training loss: 4.3578...  0.3403 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8931...  Training loss: 4.3957...  0.3407 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8932...  Training loss: 4.3820...  0.3400 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8933...  Training loss: 4.4296...  0.3437 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8934...  Training loss: 4.3628...  0.3441 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8935...  Training loss: 4.4638...  0.3401 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8936...  Training loss: 4.3384...  0.3409 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8937...  Training loss: 4.4704...  0.3422 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8938...  Training loss: 4.4449...  0.3433 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8939...  Training loss: 4.4001...  0.3433 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8940...  Training loss: 4.3897...  0.3431 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8941...  Training loss: 4.3667...  0.3416 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8942...  Training loss: 4.3752...  0.3416 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8943...  Training loss: 4.4185...  0.3392 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8944...  Training loss: 4.4042...  0.3426 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8945...  Training loss: 4.4109...  0.3452 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8946...  Training loss: 4.3407...  0.3433 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8947...  Training loss: 4.4161...  0.3390 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8948...  Training loss: 4.3879...  0.3430 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8949...  Training loss: 4.4370...  0.3435 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8950...  Training loss: 4.3604...  0.3409 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8951...  Training loss: 4.3911...  0.3422 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8952...  Training loss: 4.4232...  0.3412 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8953...  Training loss: 4.5562...  0.3416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49/100...  Training Step: 8954...  Training loss: 4.5091...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8955...  Training loss: 4.4683...  0.3403 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8956...  Training loss: 4.4086...  0.3450 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8957...  Training loss: 4.3950...  0.3443 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8958...  Training loss: 4.4361...  0.3418 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8959...  Training loss: 4.4073...  0.3395 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8960...  Training loss: 4.4010...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8961...  Training loss: 4.5017...  0.3432 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8962...  Training loss: 4.4524...  0.3406 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8963...  Training loss: 4.4995...  0.3403 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8964...  Training loss: 4.4522...  0.3419 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8965...  Training loss: 4.4547...  0.3441 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8966...  Training loss: 4.3743...  0.3431 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8967...  Training loss: 4.4364...  0.3430 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8968...  Training loss: 4.3746...  0.3413 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8969...  Training loss: 4.4400...  0.3432 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8970...  Training loss: 4.4081...  0.3397 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8971...  Training loss: 4.4617...  0.3421 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8972...  Training loss: 4.4370...  0.3420 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8973...  Training loss: 4.5034...  0.3410 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8974...  Training loss: 4.4612...  0.3425 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8975...  Training loss: 4.3729...  0.3426 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8976...  Training loss: 4.3367...  0.3425 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8977...  Training loss: 4.3677...  0.3447 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8978...  Training loss: 4.3709...  0.3433 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8979...  Training loss: 4.3834...  0.3443 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8980...  Training loss: 4.3981...  0.3408 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8981...  Training loss: 4.4272...  0.3460 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8982...  Training loss: 4.3450...  0.3448 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8983...  Training loss: 4.4181...  0.3412 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8984...  Training loss: 4.3347...  0.3422 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8985...  Training loss: 4.4194...  0.3417 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8986...  Training loss: 4.4231...  0.3431 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8987...  Training loss: 4.4598...  0.3433 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8988...  Training loss: 4.4407...  0.3425 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8989...  Training loss: 4.4671...  0.3438 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8990...  Training loss: 4.5117...  0.3403 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8991...  Training loss: 4.3924...  0.3416 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8992...  Training loss: 4.4367...  0.3444 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8993...  Training loss: 4.4679...  0.3409 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8994...  Training loss: 4.4595...  0.3414 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8995...  Training loss: 4.4307...  0.3427 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8996...  Training loss: 4.4040...  0.3418 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8997...  Training loss: 4.4951...  0.3423 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8998...  Training loss: 4.4574...  0.3426 sec/batch\n",
      "Epoch: 49/100...  Training Step: 8999...  Training loss: 4.4129...  0.3418 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9000...  Training loss: 4.4199...  0.3403 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9001...  Training loss: 4.4671...  0.3866 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9002...  Training loss: 4.4493...  0.3474 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9003...  Training loss: 4.4110...  0.3443 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9004...  Training loss: 4.3599...  0.3427 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9005...  Training loss: 4.3808...  0.3429 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9006...  Training loss: 4.3818...  0.3426 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9007...  Training loss: 4.3363...  0.3442 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9008...  Training loss: 4.4096...  0.3451 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9009...  Training loss: 4.4492...  0.3414 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9010...  Training loss: 4.4602...  0.3437 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9011...  Training loss: 4.4340...  0.3442 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9012...  Training loss: 4.4186...  0.3453 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9013...  Training loss: 4.5013...  0.3435 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9014...  Training loss: 4.4578...  0.3421 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9015...  Training loss: 4.4567...  0.3467 sec/batch\n",
      "Epoch: 49/100...  Training Step: 9016...  Training loss: 4.4538...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9017...  Training loss: 4.4835...  0.3449 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9018...  Training loss: 4.3677...  0.3433 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9019...  Training loss: 4.3262...  0.3416 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9020...  Training loss: 4.4192...  0.3428 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9021...  Training loss: 4.3567...  0.3404 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9022...  Training loss: 4.3026...  0.3431 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9023...  Training loss: 4.5156...  0.3463 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9024...  Training loss: 4.4635...  0.3408 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9025...  Training loss: 4.4527...  0.3419 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9026...  Training loss: 4.4625...  0.3454 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9027...  Training loss: 4.4406...  0.3441 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9028...  Training loss: 4.4125...  0.3428 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9029...  Training loss: 4.3982...  0.3412 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9030...  Training loss: 4.4086...  0.3409 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9031...  Training loss: 4.4436...  0.3422 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9032...  Training loss: 4.4248...  0.3436 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9033...  Training loss: 4.4639...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9034...  Training loss: 4.3897...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9035...  Training loss: 4.4831...  0.3412 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9036...  Training loss: 4.4356...  0.3410 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9037...  Training loss: 4.4222...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9038...  Training loss: 4.3248...  0.3463 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9039...  Training loss: 4.3590...  0.3412 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9040...  Training loss: 4.3631...  0.3425 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9041...  Training loss: 4.4145...  0.3452 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9042...  Training loss: 4.3972...  0.3425 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9043...  Training loss: 4.4212...  0.3420 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9044...  Training loss: 4.3684...  0.3428 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9045...  Training loss: 4.4223...  0.3423 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9046...  Training loss: 4.4865...  0.3415 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9047...  Training loss: 4.4701...  0.3411 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9048...  Training loss: 4.4599...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9049...  Training loss: 4.2785...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9050...  Training loss: 4.3275...  0.3405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/100...  Training Step: 9051...  Training loss: 4.4443...  0.3408 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9052...  Training loss: 4.5325...  0.3433 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9053...  Training loss: 4.5102...  0.3444 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9054...  Training loss: 4.5165...  0.3407 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9055...  Training loss: 4.4686...  0.3429 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9056...  Training loss: 4.4723...  0.3436 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9057...  Training loss: 4.5100...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9058...  Training loss: 4.5417...  0.3428 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9059...  Training loss: 4.5619...  0.3460 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9060...  Training loss: 4.5217...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9061...  Training loss: 4.4811...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9062...  Training loss: 4.4383...  0.3444 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9063...  Training loss: 4.5054...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9064...  Training loss: 4.4085...  0.3456 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9065...  Training loss: 4.4300...  0.3443 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9066...  Training loss: 4.4875...  0.3432 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9067...  Training loss: 4.4961...  0.3410 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9068...  Training loss: 4.4754...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9069...  Training loss: 4.4753...  0.3407 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9070...  Training loss: 4.5089...  0.3430 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9071...  Training loss: 4.4748...  0.3441 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9072...  Training loss: 4.4561...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9073...  Training loss: 4.3981...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9074...  Training loss: 4.4407...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9075...  Training loss: 4.3445...  0.3425 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9076...  Training loss: 4.3602...  0.3431 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9077...  Training loss: 4.3878...  0.3420 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9078...  Training loss: 4.4313...  0.3455 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9079...  Training loss: 4.4062...  0.3450 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9080...  Training loss: 4.4744...  0.3441 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9081...  Training loss: 4.3875...  0.3447 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9082...  Training loss: 4.3647...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9083...  Training loss: 4.4100...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9084...  Training loss: 4.3379...  0.3435 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9085...  Training loss: 4.4491...  0.3420 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9086...  Training loss: 4.4598...  0.3452 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9087...  Training loss: 4.4026...  0.3403 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9088...  Training loss: 4.2854...  0.3424 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9089...  Training loss: 4.3242...  0.3429 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9090...  Training loss: 4.3464...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9091...  Training loss: 4.3439...  0.3413 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9092...  Training loss: 4.3577...  0.3418 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9093...  Training loss: 4.3897...  0.3409 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9094...  Training loss: 4.3294...  0.3449 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9095...  Training loss: 4.3814...  0.3427 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9096...  Training loss: 4.3871...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9097...  Training loss: 4.4315...  0.3430 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9098...  Training loss: 4.4009...  0.3416 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9099...  Training loss: 4.3927...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9100...  Training loss: 4.3866...  0.3430 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9101...  Training loss: 4.3999...  0.3423 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9102...  Training loss: 4.4117...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9103...  Training loss: 4.3647...  0.3455 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9104...  Training loss: 4.3373...  0.3449 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9105...  Training loss: 4.4373...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9106...  Training loss: 4.4654...  0.3419 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9107...  Training loss: 4.4128...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9108...  Training loss: 4.3090...  0.3458 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9109...  Training loss: 4.3813...  0.3456 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9110...  Training loss: 4.3845...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9111...  Training loss: 4.4358...  0.3422 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9112...  Training loss: 4.4256...  0.3453 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9113...  Training loss: 4.3657...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9114...  Training loss: 4.3242...  0.3406 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9115...  Training loss: 4.3482...  0.3416 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9116...  Training loss: 4.3696...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9117...  Training loss: 4.3772...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9118...  Training loss: 4.3588...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9119...  Training loss: 4.4572...  0.3430 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9120...  Training loss: 4.3233...  0.3444 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9121...  Training loss: 4.4472...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9122...  Training loss: 4.4276...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9123...  Training loss: 4.3809...  0.3417 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9124...  Training loss: 4.3492...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9125...  Training loss: 4.3015...  0.3407 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9126...  Training loss: 4.3473...  0.3408 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9127...  Training loss: 4.3758...  0.3405 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9128...  Training loss: 4.3710...  0.3459 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9129...  Training loss: 4.3656...  0.3426 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9130...  Training loss: 4.3204...  0.3416 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9131...  Training loss: 4.3946...  0.3444 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9132...  Training loss: 4.3181...  0.3461 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9133...  Training loss: 4.4225...  0.3423 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9134...  Training loss: 4.3079...  0.3401 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9135...  Training loss: 4.3770...  0.3429 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9136...  Training loss: 4.3768...  0.3418 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9137...  Training loss: 4.5404...  0.3432 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9138...  Training loss: 4.4784...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9139...  Training loss: 4.4454...  0.3426 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9140...  Training loss: 4.4003...  0.3432 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9141...  Training loss: 4.3942...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9142...  Training loss: 4.3964...  0.3463 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9143...  Training loss: 4.3962...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9144...  Training loss: 4.3844...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9145...  Training loss: 4.4771...  0.3435 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9146...  Training loss: 4.4265...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9147...  Training loss: 4.4552...  0.3445 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/100...  Training Step: 9148...  Training loss: 4.4102...  0.3424 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9149...  Training loss: 4.4611...  0.3452 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9150...  Training loss: 4.3567...  0.3447 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9151...  Training loss: 4.3931...  0.3459 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9152...  Training loss: 4.3315...  0.3454 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9153...  Training loss: 4.4004...  0.3451 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9154...  Training loss: 4.3523...  0.3452 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9155...  Training loss: 4.4502...  0.3399 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9156...  Training loss: 4.4140...  0.3405 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9157...  Training loss: 4.4929...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9158...  Training loss: 4.4311...  0.3440 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9159...  Training loss: 4.3556...  0.3433 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9160...  Training loss: 4.3104...  0.3456 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9161...  Training loss: 4.3509...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9162...  Training loss: 4.3558...  0.3458 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9163...  Training loss: 4.3695...  0.3442 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9164...  Training loss: 4.3872...  0.3456 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9165...  Training loss: 4.4084...  0.3433 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9166...  Training loss: 4.3475...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9167...  Training loss: 4.3945...  0.3411 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9168...  Training loss: 4.3232...  0.3429 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9169...  Training loss: 4.4005...  0.3443 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9170...  Training loss: 4.3837...  0.3457 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9171...  Training loss: 4.4298...  0.3447 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9172...  Training loss: 4.4003...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9173...  Training loss: 4.4251...  0.3443 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9174...  Training loss: 4.4824...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9175...  Training loss: 4.3537...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9176...  Training loss: 4.3887...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9177...  Training loss: 4.4118...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9178...  Training loss: 4.4304...  0.3460 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9179...  Training loss: 4.3826...  0.3425 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9180...  Training loss: 4.3847...  0.3459 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9181...  Training loss: 4.4599...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9182...  Training loss: 4.4182...  0.3464 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9183...  Training loss: 4.3699...  0.3448 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9184...  Training loss: 4.3682...  0.3446 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9185...  Training loss: 4.4300...  0.3422 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9186...  Training loss: 4.4382...  0.3411 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9187...  Training loss: 4.3970...  0.3427 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9188...  Training loss: 4.3563...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9189...  Training loss: 4.3462...  0.3438 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9190...  Training loss: 4.3147...  0.3422 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9191...  Training loss: 4.2896...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9192...  Training loss: 4.3466...  0.3414 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9193...  Training loss: 4.3936...  0.3441 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9194...  Training loss: 4.4003...  0.3439 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9195...  Training loss: 4.4083...  0.3445 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9196...  Training loss: 4.3746...  0.3430 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9197...  Training loss: 4.5002...  0.3456 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9198...  Training loss: 4.4559...  0.3434 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9199...  Training loss: 4.4508...  0.3436 sec/batch\n",
      "Epoch: 50/100...  Training Step: 9200...  Training loss: 4.4424...  0.3405 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9201...  Training loss: 4.4217...  0.3434 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9202...  Training loss: 4.3251...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9203...  Training loss: 4.2983...  0.3426 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9204...  Training loss: 4.3797...  0.3455 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9205...  Training loss: 4.3607...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9206...  Training loss: 4.3106...  0.3431 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9207...  Training loss: 4.5248...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9208...  Training loss: 4.4422...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9209...  Training loss: 4.4713...  0.3431 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9210...  Training loss: 4.4442...  0.3436 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9211...  Training loss: 4.3905...  0.3443 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9212...  Training loss: 4.3732...  0.3444 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9213...  Training loss: 4.3667...  0.3448 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9214...  Training loss: 4.3875...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9215...  Training loss: 4.4322...  0.3465 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9216...  Training loss: 4.3907...  0.3451 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9217...  Training loss: 4.4536...  0.3458 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9218...  Training loss: 4.3754...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9219...  Training loss: 4.4726...  0.3430 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9220...  Training loss: 4.4097...  0.3426 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9221...  Training loss: 4.3918...  0.3416 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9222...  Training loss: 4.3263...  0.3438 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9223...  Training loss: 4.3561...  0.3452 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9224...  Training loss: 4.3540...  0.3471 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9225...  Training loss: 4.4022...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9226...  Training loss: 4.3909...  0.3437 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9227...  Training loss: 4.3884...  0.3419 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9228...  Training loss: 4.3812...  0.3451 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9229...  Training loss: 4.4237...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9230...  Training loss: 4.4318...  0.3477 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9231...  Training loss: 4.4153...  0.3425 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9232...  Training loss: 4.4081...  0.3463 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9233...  Training loss: 4.2676...  0.3444 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9234...  Training loss: 4.3323...  0.3421 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9235...  Training loss: 4.4318...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9236...  Training loss: 4.5119...  0.3414 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9237...  Training loss: 4.5017...  0.3429 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9238...  Training loss: 4.5121...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9239...  Training loss: 4.4535...  0.3458 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9240...  Training loss: 4.4624...  0.3408 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9241...  Training loss: 4.4763...  0.3421 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9242...  Training loss: 4.5034...  0.3417 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9243...  Training loss: 4.5286...  0.3410 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9244...  Training loss: 4.4827...  0.3425 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/100...  Training Step: 9245...  Training loss: 4.4400...  0.3473 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9246...  Training loss: 4.3917...  0.3458 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9247...  Training loss: 4.4730...  0.3454 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9248...  Training loss: 4.3726...  0.3399 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9249...  Training loss: 4.4191...  0.3428 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9250...  Training loss: 4.4589...  0.3455 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9251...  Training loss: 4.4511...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9252...  Training loss: 4.4312...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9253...  Training loss: 4.4475...  0.3434 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9254...  Training loss: 4.4706...  0.3454 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9255...  Training loss: 4.4623...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9256...  Training loss: 4.4508...  0.3450 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9257...  Training loss: 4.3752...  0.3404 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9258...  Training loss: 4.4377...  0.3425 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9259...  Training loss: 4.3325...  0.3452 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9260...  Training loss: 4.3277...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9261...  Training loss: 4.3732...  0.3390 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9262...  Training loss: 4.3934...  0.3443 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9263...  Training loss: 4.4077...  0.3424 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9264...  Training loss: 4.4766...  0.3414 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9265...  Training loss: 4.3963...  0.3451 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9266...  Training loss: 4.3508...  0.3404 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9267...  Training loss: 4.3810...  0.3458 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9268...  Training loss: 4.3079...  0.3461 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9269...  Training loss: 4.4172...  0.3460 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9270...  Training loss: 4.3909...  0.3460 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9271...  Training loss: 4.3664...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9272...  Training loss: 4.2654...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9273...  Training loss: 4.3040...  0.3407 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9274...  Training loss: 4.3212...  0.3423 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9275...  Training loss: 4.2980...  0.3436 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9276...  Training loss: 4.3499...  0.3444 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9277...  Training loss: 4.3573...  0.3436 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9278...  Training loss: 4.3055...  0.3409 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9279...  Training loss: 4.3635...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9280...  Training loss: 4.3562...  0.3435 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9281...  Training loss: 4.4212...  0.3416 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9282...  Training loss: 4.3752...  0.3442 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9283...  Training loss: 4.3701...  0.3430 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9284...  Training loss: 4.3547...  0.3454 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9285...  Training loss: 4.3845...  0.3433 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9286...  Training loss: 4.3905...  0.3429 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9287...  Training loss: 4.3159...  0.3401 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9288...  Training loss: 4.3327...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9289...  Training loss: 4.4008...  0.3452 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9290...  Training loss: 4.4601...  0.3405 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9291...  Training loss: 4.3842...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9292...  Training loss: 4.2878...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9293...  Training loss: 4.4015...  0.3460 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9294...  Training loss: 4.3776...  0.3424 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9295...  Training loss: 4.4141...  0.3413 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9296...  Training loss: 4.4214...  0.3438 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9297...  Training loss: 4.3427...  0.3424 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9298...  Training loss: 4.3131...  0.3441 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9299...  Training loss: 4.3407...  0.3408 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9300...  Training loss: 4.3410...  0.3451 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9301...  Training loss: 4.3531...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9302...  Training loss: 4.3274...  0.3423 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9303...  Training loss: 4.3737...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9304...  Training loss: 4.2977...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9305...  Training loss: 4.4363...  0.3431 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9306...  Training loss: 4.4192...  0.3449 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9307...  Training loss: 4.4345...  0.3468 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9308...  Training loss: 4.3358...  0.3419 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9309...  Training loss: 4.2893...  0.3452 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9310...  Training loss: 4.3350...  0.3423 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9311...  Training loss: 4.3669...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9312...  Training loss: 4.3425...  0.3447 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9313...  Training loss: 4.3640...  0.3424 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9314...  Training loss: 4.3147...  0.3414 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9315...  Training loss: 4.4121...  0.3409 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9316...  Training loss: 4.3398...  0.3411 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9317...  Training loss: 4.3883...  0.3414 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9318...  Training loss: 4.2901...  0.3442 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9319...  Training loss: 4.3523...  0.3423 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9320...  Training loss: 4.3793...  0.3415 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9321...  Training loss: 4.5129...  0.3421 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9322...  Training loss: 4.4885...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9323...  Training loss: 4.4420...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9324...  Training loss: 4.3647...  0.3444 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9325...  Training loss: 4.3692...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9326...  Training loss: 4.3938...  0.3435 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9327...  Training loss: 4.3779...  0.3413 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9328...  Training loss: 4.3731...  0.3464 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9329...  Training loss: 4.4596...  0.3406 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9330...  Training loss: 4.4102...  0.3419 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9331...  Training loss: 4.4371...  0.3456 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9332...  Training loss: 4.3798...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9333...  Training loss: 4.4332...  0.3423 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9334...  Training loss: 4.3306...  0.3409 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9335...  Training loss: 4.3744...  0.3444 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9336...  Training loss: 4.3322...  0.3401 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9337...  Training loss: 4.3908...  0.3420 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9338...  Training loss: 4.3503...  0.3417 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9339...  Training loss: 4.4047...  0.3447 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9340...  Training loss: 4.3663...  0.3476 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9341...  Training loss: 4.4670...  0.3444 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/100...  Training Step: 9342...  Training loss: 4.4200...  0.3425 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9343...  Training loss: 4.3453...  0.3447 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9344...  Training loss: 4.2954...  0.3457 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9345...  Training loss: 4.3629...  0.3450 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9346...  Training loss: 4.3467...  0.3459 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9347...  Training loss: 4.3665...  0.3451 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9348...  Training loss: 4.3775...  0.3436 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9349...  Training loss: 4.3943...  0.3441 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9350...  Training loss: 4.3119...  0.3425 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9351...  Training loss: 4.3570...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9352...  Training loss: 4.3138...  0.3438 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9353...  Training loss: 4.3574...  0.3432 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9354...  Training loss: 4.3702...  0.3448 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9355...  Training loss: 4.3874...  0.3424 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9356...  Training loss: 4.3894...  0.3441 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9357...  Training loss: 4.4279...  0.3441 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9358...  Training loss: 4.4858...  0.3432 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9359...  Training loss: 4.3389...  0.3408 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9360...  Training loss: 4.3649...  0.3426 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9361...  Training loss: 4.3740...  0.3439 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9362...  Training loss: 4.4173...  0.3422 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9363...  Training loss: 4.3525...  0.3452 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9364...  Training loss: 4.3577...  0.3407 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9365...  Training loss: 4.4319...  0.3431 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9366...  Training loss: 4.3769...  0.3432 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9367...  Training loss: 4.3748...  0.3442 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9368...  Training loss: 4.3331...  0.3426 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9369...  Training loss: 4.3949...  0.3427 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9370...  Training loss: 4.4015...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9371...  Training loss: 4.3662...  0.3417 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9372...  Training loss: 4.3231...  0.3412 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9373...  Training loss: 4.3139...  0.3413 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9374...  Training loss: 4.2902...  0.3420 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9375...  Training loss: 4.2579...  0.3406 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9376...  Training loss: 4.3088...  0.3418 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9377...  Training loss: 4.3759...  0.3427 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9378...  Training loss: 4.3412...  0.3438 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9379...  Training loss: 4.3568...  0.3413 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9380...  Training loss: 4.3244...  0.3429 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9381...  Training loss: 4.4283...  0.3416 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9382...  Training loss: 4.4221...  0.3446 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9383...  Training loss: 4.4133...  0.3427 sec/batch\n",
      "Epoch: 51/100...  Training Step: 9384...  Training loss: 4.4273...  0.3409 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9385...  Training loss: 4.3996...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9386...  Training loss: 4.2847...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9387...  Training loss: 4.2672...  0.3405 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9388...  Training loss: 4.3377...  0.3426 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9389...  Training loss: 4.3123...  0.3418 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9390...  Training loss: 4.2733...  0.3456 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9391...  Training loss: 4.4723...  0.3435 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9392...  Training loss: 4.4213...  0.3410 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9393...  Training loss: 4.4289...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9394...  Training loss: 4.4020...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9395...  Training loss: 4.3798...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9396...  Training loss: 4.3426...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9397...  Training loss: 4.3285...  0.3405 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9398...  Training loss: 4.3271...  0.3421 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9399...  Training loss: 4.4014...  0.3417 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9400...  Training loss: 4.3096...  0.3409 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9401...  Training loss: 4.4475...  0.3435 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9402...  Training loss: 4.3478...  0.3408 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9403...  Training loss: 4.4574...  0.3423 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9404...  Training loss: 4.3984...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9405...  Training loss: 4.3536...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9406...  Training loss: 4.2767...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9407...  Training loss: 4.3229...  0.3453 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9408...  Training loss: 4.3363...  0.3410 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9409...  Training loss: 4.3977...  0.3404 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9410...  Training loss: 4.3755...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9411...  Training loss: 4.3986...  0.3396 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9412...  Training loss: 4.3685...  0.3416 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9413...  Training loss: 4.4319...  0.3426 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9414...  Training loss: 4.4506...  0.3429 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9415...  Training loss: 4.4129...  0.3426 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9416...  Training loss: 4.3932...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9417...  Training loss: 4.2464...  0.3438 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9418...  Training loss: 4.3143...  0.3407 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9419...  Training loss: 4.4080...  0.3423 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9420...  Training loss: 4.5044...  0.3433 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9421...  Training loss: 4.4790...  0.3413 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9422...  Training loss: 4.4758...  0.3437 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9423...  Training loss: 4.4464...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9424...  Training loss: 4.4333...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9425...  Training loss: 4.4646...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9426...  Training loss: 4.4975...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9427...  Training loss: 4.5147...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9428...  Training loss: 4.4727...  0.3420 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9429...  Training loss: 4.4031...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9430...  Training loss: 4.3728...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9431...  Training loss: 4.4331...  0.3464 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9432...  Training loss: 4.3561...  0.3463 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9433...  Training loss: 4.3827...  0.3451 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9434...  Training loss: 4.4214...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9435...  Training loss: 4.4267...  0.3431 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9436...  Training loss: 4.4349...  0.3429 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9437...  Training loss: 4.4369...  0.3405 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9438...  Training loss: 4.4354...  0.3465 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/100...  Training Step: 9439...  Training loss: 4.4272...  0.3456 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9440...  Training loss: 4.4351...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9441...  Training loss: 4.3242...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9442...  Training loss: 4.3884...  0.3440 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9443...  Training loss: 4.3150...  0.3420 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9444...  Training loss: 4.3160...  0.3460 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9445...  Training loss: 4.3576...  0.3452 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9446...  Training loss: 4.3725...  0.3403 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9447...  Training loss: 4.3789...  0.3435 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9448...  Training loss: 4.4523...  0.3418 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9449...  Training loss: 4.3382...  0.3459 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9450...  Training loss: 4.3586...  0.3465 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9451...  Training loss: 4.3475...  0.3518 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9452...  Training loss: 4.2989...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9453...  Training loss: 4.3829...  0.3407 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9454...  Training loss: 4.3780...  0.3433 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9455...  Training loss: 4.3319...  0.3465 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9456...  Training loss: 4.2281...  0.3425 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9457...  Training loss: 4.2965...  0.3401 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9458...  Training loss: 4.2842...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9459...  Training loss: 4.2731...  0.3446 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9460...  Training loss: 4.3217...  0.3444 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9461...  Training loss: 4.3378...  0.3451 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9462...  Training loss: 4.2917...  0.3456 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9463...  Training loss: 4.3482...  0.3453 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9464...  Training loss: 4.3222...  0.3453 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9465...  Training loss: 4.3886...  0.3444 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9466...  Training loss: 4.3586...  0.3430 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9467...  Training loss: 4.3457...  0.3453 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9468...  Training loss: 4.3550...  0.3433 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9469...  Training loss: 4.3814...  0.3441 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9470...  Training loss: 4.3959...  0.3444 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9471...  Training loss: 4.3231...  0.3404 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9472...  Training loss: 4.3062...  0.3431 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9473...  Training loss: 4.4048...  0.3414 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9474...  Training loss: 4.4359...  0.3422 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9475...  Training loss: 4.3826...  0.3459 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9476...  Training loss: 4.2597...  0.3454 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9477...  Training loss: 4.3769...  0.3415 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9478...  Training loss: 4.3353...  0.3415 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9479...  Training loss: 4.3844...  0.3411 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9480...  Training loss: 4.3855...  0.3454 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9481...  Training loss: 4.3467...  0.3437 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9482...  Training loss: 4.3130...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9483...  Training loss: 4.3597...  0.3403 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9484...  Training loss: 4.3615...  0.3422 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9485...  Training loss: 4.3958...  0.3454 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9486...  Training loss: 4.3409...  0.3416 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9487...  Training loss: 4.4090...  0.3435 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9488...  Training loss: 4.2902...  0.3411 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9489...  Training loss: 4.3875...  0.3402 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9490...  Training loss: 4.4081...  0.3447 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9491...  Training loss: 4.3871...  0.3432 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9492...  Training loss: 4.3471...  0.3441 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9493...  Training loss: 4.3119...  0.3424 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9494...  Training loss: 4.3578...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9495...  Training loss: 4.3753...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9496...  Training loss: 4.3464...  0.3448 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9497...  Training loss: 4.3556...  0.3448 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9498...  Training loss: 4.3066...  0.3420 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9499...  Training loss: 4.4051...  0.3423 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9500...  Training loss: 4.3766...  0.3403 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9501...  Training loss: 4.3948...  0.3419 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9502...  Training loss: 4.3095...  0.3441 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9503...  Training loss: 4.3617...  0.3440 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9504...  Training loss: 4.3837...  0.3427 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9505...  Training loss: 4.5187...  0.3416 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9506...  Training loss: 4.4791...  0.3433 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9507...  Training loss: 4.4785...  0.3423 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9508...  Training loss: 4.4038...  0.3449 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9509...  Training loss: 4.4149...  0.3470 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9510...  Training loss: 4.4326...  0.3453 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9511...  Training loss: 4.4322...  0.3418 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9512...  Training loss: 4.4076...  0.3419 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9513...  Training loss: 4.5358...  0.3435 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9514...  Training loss: 4.4443...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9515...  Training loss: 4.4657...  0.3442 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9516...  Training loss: 4.4014...  0.3419 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9517...  Training loss: 4.4547...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9518...  Training loss: 4.3696...  0.3399 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9519...  Training loss: 4.3882...  0.3438 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9520...  Training loss: 4.3341...  0.3449 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9521...  Training loss: 4.3753...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9522...  Training loss: 4.3572...  0.3403 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9523...  Training loss: 4.4036...  0.3420 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9524...  Training loss: 4.3661...  0.3398 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9525...  Training loss: 4.4673...  0.3399 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9526...  Training loss: 4.4383...  0.3421 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9527...  Training loss: 4.3463...  0.3419 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9528...  Training loss: 4.3103...  0.3451 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9529...  Training loss: 4.3682...  0.3409 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9530...  Training loss: 4.3798...  0.3443 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9531...  Training loss: 4.3794...  0.3416 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9532...  Training loss: 4.3776...  0.3417 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9533...  Training loss: 4.3909...  0.3452 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9534...  Training loss: 4.3026...  0.3405 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9535...  Training loss: 4.3637...  0.3459 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/100...  Training Step: 9536...  Training loss: 4.3027...  0.3457 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9537...  Training loss: 4.3506...  0.3438 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9538...  Training loss: 4.3710...  0.3413 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9539...  Training loss: 4.3980...  0.3428 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9540...  Training loss: 4.3870...  0.3436 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9541...  Training loss: 4.4147...  0.3408 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9542...  Training loss: 4.4671...  0.3426 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9543...  Training loss: 4.3102...  0.3436 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9544...  Training loss: 4.3827...  0.3445 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9545...  Training loss: 4.4167...  0.3467 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9546...  Training loss: 4.4202...  0.3458 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9547...  Training loss: 4.3489...  0.3425 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9548...  Training loss: 4.3496...  0.3403 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9549...  Training loss: 4.4498...  0.3428 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9550...  Training loss: 4.3698...  0.3441 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9551...  Training loss: 4.3434...  0.3463 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9552...  Training loss: 4.3585...  0.3438 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9553...  Training loss: 4.3872...  0.3424 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9554...  Training loss: 4.4056...  0.3430 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9555...  Training loss: 4.3687...  0.3454 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9556...  Training loss: 4.3211...  0.3410 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9557...  Training loss: 4.3037...  0.3418 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9558...  Training loss: 4.3038...  0.3439 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9559...  Training loss: 4.2490...  0.3412 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9560...  Training loss: 4.3468...  0.3421 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9561...  Training loss: 4.3683...  0.3417 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9562...  Training loss: 4.3437...  0.3410 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9563...  Training loss: 4.3584...  0.3428 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9564...  Training loss: 4.3096...  0.3425 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9565...  Training loss: 4.4020...  0.3455 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9566...  Training loss: 4.3851...  0.3433 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9567...  Training loss: 4.4007...  0.3452 sec/batch\n",
      "Epoch: 52/100...  Training Step: 9568...  Training loss: 4.3903...  0.3438 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9569...  Training loss: 4.3718...  0.3410 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9570...  Training loss: 4.2814...  0.3428 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9571...  Training loss: 4.2346...  0.3422 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9572...  Training loss: 4.3193...  0.3403 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9573...  Training loss: 4.2479...  0.3406 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9574...  Training loss: 4.2188...  0.3415 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9575...  Training loss: 4.4425...  0.3405 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9576...  Training loss: 4.3927...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9577...  Training loss: 4.4351...  0.3434 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9578...  Training loss: 4.4258...  0.3413 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9579...  Training loss: 4.3539...  0.3406 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9580...  Training loss: 4.3134...  0.3422 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9581...  Training loss: 4.3368...  0.3392 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9582...  Training loss: 4.3357...  0.3412 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9583...  Training loss: 4.3764...  0.3401 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9584...  Training loss: 4.3082...  0.3430 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9585...  Training loss: 4.3893...  0.3389 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9586...  Training loss: 4.3413...  0.3401 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9587...  Training loss: 4.4530...  0.3396 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9588...  Training loss: 4.4011...  0.3434 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9589...  Training loss: 4.3608...  0.3395 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9590...  Training loss: 4.2768...  0.3410 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9591...  Training loss: 4.2847...  0.3439 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9592...  Training loss: 4.2888...  0.3425 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9593...  Training loss: 4.3383...  0.3413 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9594...  Training loss: 4.3509...  0.3462 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9595...  Training loss: 4.3938...  0.3439 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9596...  Training loss: 4.3346...  0.3433 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9597...  Training loss: 4.3887...  0.3433 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9598...  Training loss: 4.4243...  0.3430 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9599...  Training loss: 4.3834...  0.3453 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9600...  Training loss: 4.3721...  0.3453 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9601...  Training loss: 4.2191...  0.3460 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9602...  Training loss: 4.3076...  0.3419 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9603...  Training loss: 4.3932...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9604...  Training loss: 4.4979...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9605...  Training loss: 4.4896...  0.3418 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9606...  Training loss: 4.4815...  0.3451 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9607...  Training loss: 4.4451...  0.3446 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9608...  Training loss: 4.4383...  0.3453 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9609...  Training loss: 4.4938...  0.3426 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9610...  Training loss: 4.5113...  0.3441 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9611...  Training loss: 4.5147...  0.3450 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9612...  Training loss: 4.4545...  0.3404 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9613...  Training loss: 4.4038...  0.3447 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9614...  Training loss: 4.3456...  0.3423 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9615...  Training loss: 4.4224...  0.3441 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9616...  Training loss: 4.2980...  0.3459 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9617...  Training loss: 4.3469...  0.3451 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9618...  Training loss: 4.4059...  0.3404 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9619...  Training loss: 4.4108...  0.3418 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9620...  Training loss: 4.4131...  0.3413 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9621...  Training loss: 4.4256...  0.3424 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9622...  Training loss: 4.4191...  0.3448 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9623...  Training loss: 4.4396...  0.3414 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9624...  Training loss: 4.3990...  0.3431 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9625...  Training loss: 4.3481...  0.3420 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9626...  Training loss: 4.3888...  0.3451 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9627...  Training loss: 4.2924...  0.3443 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9628...  Training loss: 4.3111...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9629...  Training loss: 4.3377...  0.3468 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9630...  Training loss: 4.3773...  0.3439 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9631...  Training loss: 4.3565...  0.3403 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9632...  Training loss: 4.4383...  0.3452 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100...  Training Step: 9633...  Training loss: 4.3463...  0.3413 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9634...  Training loss: 4.3435...  0.3460 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9635...  Training loss: 4.3423...  0.3452 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9636...  Training loss: 4.2625...  0.3439 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9637...  Training loss: 4.3683...  0.3427 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9638...  Training loss: 4.3833...  0.3428 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9639...  Training loss: 4.3512...  0.3421 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9640...  Training loss: 4.2408...  0.3461 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9641...  Training loss: 4.2798...  0.3437 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9642...  Training loss: 4.2624...  0.3426 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9643...  Training loss: 4.2572...  0.3422 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9644...  Training loss: 4.2889...  0.3423 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9645...  Training loss: 4.3144...  0.3425 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9646...  Training loss: 4.2716...  0.3405 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9647...  Training loss: 4.3309...  0.3402 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9648...  Training loss: 4.2964...  0.3437 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9649...  Training loss: 4.3462...  0.3405 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9650...  Training loss: 4.3288...  0.3458 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9651...  Training loss: 4.3151...  0.3416 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9652...  Training loss: 4.3225...  0.3443 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9653...  Training loss: 4.3493...  0.3443 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9654...  Training loss: 4.3472...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9655...  Training loss: 4.3032...  0.3435 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9656...  Training loss: 4.2708...  0.3428 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9657...  Training loss: 4.3543...  0.3452 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9658...  Training loss: 4.4165...  0.3447 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9659...  Training loss: 4.3481...  0.3436 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9660...  Training loss: 4.2452...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9661...  Training loss: 4.3518...  0.3442 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9662...  Training loss: 4.3549...  0.3459 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9663...  Training loss: 4.3788...  0.3448 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9664...  Training loss: 4.3736...  0.3415 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9665...  Training loss: 4.2997...  0.3446 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9666...  Training loss: 4.2955...  0.3437 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9667...  Training loss: 4.3401...  0.3454 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9668...  Training loss: 4.3123...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9669...  Training loss: 4.3484...  0.3410 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9670...  Training loss: 4.3208...  0.3425 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9671...  Training loss: 4.3978...  0.3404 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9672...  Training loss: 4.2390...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9673...  Training loss: 4.3836...  0.3413 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9674...  Training loss: 4.3392...  0.3450 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9675...  Training loss: 4.3191...  0.3439 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9676...  Training loss: 4.2611...  0.3417 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9677...  Training loss: 4.2743...  0.3423 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9678...  Training loss: 4.2928...  0.3425 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9679...  Training loss: 4.3595...  0.3453 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9680...  Training loss: 4.2853...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9681...  Training loss: 4.3149...  0.3434 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9682...  Training loss: 4.2574...  0.3416 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9683...  Training loss: 4.3329...  0.3455 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9684...  Training loss: 4.2901...  0.3440 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9685...  Training loss: 4.3933...  0.3430 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9686...  Training loss: 4.2825...  0.3424 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9687...  Training loss: 4.3140...  0.3426 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9688...  Training loss: 4.3308...  0.3441 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9689...  Training loss: 4.4930...  0.3438 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9690...  Training loss: 4.4497...  0.3436 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9691...  Training loss: 4.4184...  0.3445 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9692...  Training loss: 4.3227...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9693...  Training loss: 4.3511...  0.3398 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9694...  Training loss: 4.3780...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9695...  Training loss: 4.3869...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9696...  Training loss: 4.4161...  0.3437 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9697...  Training loss: 4.4752...  0.3408 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9698...  Training loss: 4.4228...  0.3429 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9699...  Training loss: 4.4194...  0.3462 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9700...  Training loss: 4.3714...  0.3436 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9701...  Training loss: 4.4399...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9702...  Training loss: 4.3395...  0.3452 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9703...  Training loss: 4.4063...  0.3432 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9704...  Training loss: 4.3261...  0.3466 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9705...  Training loss: 4.3707...  0.3460 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9706...  Training loss: 4.3330...  0.3418 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9707...  Training loss: 4.3701...  0.3437 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9708...  Training loss: 4.3151...  0.3454 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9709...  Training loss: 4.4147...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9710...  Training loss: 4.4104...  0.3435 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9711...  Training loss: 4.3136...  0.3445 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9712...  Training loss: 4.2757...  0.3435 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9713...  Training loss: 4.3146...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9714...  Training loss: 4.3328...  0.3447 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9715...  Training loss: 4.3555...  0.3450 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9716...  Training loss: 4.3435...  0.3445 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9717...  Training loss: 4.3909...  0.3453 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9718...  Training loss: 4.2949...  0.3516 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9719...  Training loss: 4.3560...  0.3434 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9720...  Training loss: 4.2846...  0.3404 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9721...  Training loss: 4.3397...  0.3422 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9722...  Training loss: 4.3511...  0.3434 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9723...  Training loss: 4.3861...  0.3416 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9724...  Training loss: 4.3688...  0.3447 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9725...  Training loss: 4.3944...  0.3444 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9726...  Training loss: 4.4302...  0.3469 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9727...  Training loss: 4.2804...  0.3412 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9728...  Training loss: 4.3097...  0.3460 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9729...  Training loss: 4.3300...  0.3445 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53/100...  Training Step: 9730...  Training loss: 4.3739...  0.3445 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9731...  Training loss: 4.3262...  0.3455 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9732...  Training loss: 4.3201...  0.3446 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9733...  Training loss: 4.4024...  0.3450 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9734...  Training loss: 4.3434...  0.3449 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9735...  Training loss: 4.3140...  0.3411 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9736...  Training loss: 4.3242...  0.3406 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9737...  Training loss: 4.3575...  0.3400 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9738...  Training loss: 4.3703...  0.3428 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9739...  Training loss: 4.3206...  0.3432 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9740...  Training loss: 4.2803...  0.3408 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9741...  Training loss: 4.2677...  0.3447 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9742...  Training loss: 4.2764...  0.3442 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9743...  Training loss: 4.2215...  0.3455 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9744...  Training loss: 4.2920...  0.3431 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9745...  Training loss: 4.3357...  0.3432 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9746...  Training loss: 4.2945...  0.3435 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9747...  Training loss: 4.2943...  0.3409 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9748...  Training loss: 4.2780...  0.3414 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9749...  Training loss: 4.3352...  0.3455 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9750...  Training loss: 4.3189...  0.3431 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9751...  Training loss: 4.3051...  0.3417 sec/batch\n",
      "Epoch: 53/100...  Training Step: 9752...  Training loss: 4.3730...  0.3462 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9753...  Training loss: 4.3162...  0.3452 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9754...  Training loss: 4.2057...  0.3420 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9755...  Training loss: 4.1814...  0.3443 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9756...  Training loss: 4.2326...  0.3443 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9757...  Training loss: 4.2022...  0.3443 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9758...  Training loss: 4.1798...  0.3459 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9759...  Training loss: 4.3946...  0.3413 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9760...  Training loss: 4.3577...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9761...  Training loss: 4.3797...  0.3403 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9762...  Training loss: 4.3755...  0.3407 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9763...  Training loss: 4.3432...  0.3411 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9764...  Training loss: 4.3071...  0.3437 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9765...  Training loss: 4.2962...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9766...  Training loss: 4.3411...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9767...  Training loss: 4.3391...  0.3421 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9768...  Training loss: 4.3209...  0.3445 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9769...  Training loss: 4.3846...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9770...  Training loss: 4.2965...  0.3417 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9771...  Training loss: 4.3962...  0.3402 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9772...  Training loss: 4.3473...  0.3413 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9773...  Training loss: 4.3073...  0.3427 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9774...  Training loss: 4.2326...  0.3455 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9775...  Training loss: 4.2612...  0.3434 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9776...  Training loss: 4.2659...  0.3447 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9777...  Training loss: 4.3144...  0.3450 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9778...  Training loss: 4.2962...  0.3461 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9779...  Training loss: 4.3217...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9780...  Training loss: 4.2913...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9781...  Training loss: 4.3469...  0.3452 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9782...  Training loss: 4.3991...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9783...  Training loss: 4.3468...  0.3440 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9784...  Training loss: 4.3425...  0.3413 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9785...  Training loss: 4.1815...  0.3452 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9786...  Training loss: 4.2479...  0.3454 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9787...  Training loss: 4.3739...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9788...  Training loss: 4.4658...  0.3403 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9789...  Training loss: 4.4714...  0.3423 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9790...  Training loss: 4.4120...  0.3426 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9791...  Training loss: 4.4202...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9792...  Training loss: 4.4216...  0.3414 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9793...  Training loss: 4.4477...  0.3414 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9794...  Training loss: 4.4943...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9795...  Training loss: 4.5247...  0.3410 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9796...  Training loss: 4.4799...  0.3409 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9797...  Training loss: 4.4245...  0.3460 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9798...  Training loss: 4.3674...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9799...  Training loss: 4.4301...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9800...  Training loss: 4.3025...  0.3436 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9801...  Training loss: 4.3453...  0.3456 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9802...  Training loss: 4.3982...  0.3443 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9803...  Training loss: 4.3649...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9804...  Training loss: 4.3875...  0.3408 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9805...  Training loss: 4.4026...  0.3410 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9806...  Training loss: 4.4334...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9807...  Training loss: 4.4019...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9808...  Training loss: 4.3977...  0.3401 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9809...  Training loss: 4.3271...  0.3417 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9810...  Training loss: 4.3616...  0.3404 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9811...  Training loss: 4.2754...  0.3427 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9812...  Training loss: 4.2843...  0.3427 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9813...  Training loss: 4.3136...  0.3457 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9814...  Training loss: 4.3770...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9815...  Training loss: 4.3550...  0.3431 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9816...  Training loss: 4.4256...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9817...  Training loss: 4.3427...  0.3444 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9818...  Training loss: 4.3405...  0.3408 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9819...  Training loss: 4.3385...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9820...  Training loss: 4.2596...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9821...  Training loss: 4.3847...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9822...  Training loss: 4.3566...  0.3445 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9823...  Training loss: 4.3556...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9824...  Training loss: 4.2695...  0.3451 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9825...  Training loss: 4.2946...  0.3445 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9826...  Training loss: 4.2896...  0.3429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100...  Training Step: 9827...  Training loss: 4.2569...  0.3407 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9828...  Training loss: 4.2877...  0.3450 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9829...  Training loss: 4.3125...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9830...  Training loss: 4.2706...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9831...  Training loss: 4.3096...  0.3428 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9832...  Training loss: 4.3040...  0.3447 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9833...  Training loss: 4.3392...  0.3428 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9834...  Training loss: 4.3068...  0.3449 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9835...  Training loss: 4.3002...  0.3413 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9836...  Training loss: 4.2882...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9837...  Training loss: 4.3271...  0.3412 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9838...  Training loss: 4.3406...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9839...  Training loss: 4.2786...  0.3440 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9840...  Training loss: 4.2446...  0.3436 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9841...  Training loss: 4.3493...  0.3435 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9842...  Training loss: 4.3776...  0.3456 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9843...  Training loss: 4.2900...  0.3409 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9844...  Training loss: 4.2094...  0.3420 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9845...  Training loss: 4.3088...  0.3414 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9846...  Training loss: 4.2889...  0.3459 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9847...  Training loss: 4.3538...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9848...  Training loss: 4.3438...  0.3458 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9849...  Training loss: 4.2947...  0.3420 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9850...  Training loss: 4.2666...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9851...  Training loss: 4.2732...  0.3458 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9852...  Training loss: 4.2825...  0.3457 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9853...  Training loss: 4.3222...  0.3433 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9854...  Training loss: 4.3022...  0.3451 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9855...  Training loss: 4.3651...  0.3450 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9856...  Training loss: 4.2334...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9857...  Training loss: 4.3452...  0.3434 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9858...  Training loss: 4.3458...  0.3456 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9859...  Training loss: 4.3140...  0.3431 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9860...  Training loss: 4.2370...  0.3435 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9861...  Training loss: 4.2119...  0.3410 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9862...  Training loss: 4.2725...  0.3427 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9863...  Training loss: 4.3287...  0.3429 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9864...  Training loss: 4.2742...  0.3404 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9865...  Training loss: 4.2778...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9866...  Training loss: 4.2286...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9867...  Training loss: 4.3021...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9868...  Training loss: 4.2741...  0.3429 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9869...  Training loss: 4.3119...  0.3427 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9870...  Training loss: 4.2307...  0.3448 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9871...  Training loss: 4.2642...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9872...  Training loss: 4.3022...  0.3403 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9873...  Training loss: 4.4482...  0.3441 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9874...  Training loss: 4.3929...  0.3408 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9875...  Training loss: 4.3966...  0.3398 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9876...  Training loss: 4.3212...  0.3417 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9877...  Training loss: 4.3288...  0.3409 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9878...  Training loss: 4.3496...  0.3409 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9879...  Training loss: 4.3734...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9880...  Training loss: 4.3627...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9881...  Training loss: 4.4452...  0.3436 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9882...  Training loss: 4.3926...  0.3441 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9883...  Training loss: 4.3950...  0.3425 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9884...  Training loss: 4.3560...  0.3447 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9885...  Training loss: 4.3904...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9886...  Training loss: 4.3126...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9887...  Training loss: 4.3998...  0.3406 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9888...  Training loss: 4.3051...  0.3446 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9889...  Training loss: 4.3529...  0.3445 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9890...  Training loss: 4.3266...  0.3449 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9891...  Training loss: 4.3883...  0.3435 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9892...  Training loss: 4.3015...  0.3455 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9893...  Training loss: 4.4098...  0.3437 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9894...  Training loss: 4.3717...  0.3447 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9895...  Training loss: 4.2785...  0.3448 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9896...  Training loss: 4.2342...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9897...  Training loss: 4.2758...  0.3456 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9898...  Training loss: 4.2905...  0.3415 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9899...  Training loss: 4.3347...  0.3429 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9900...  Training loss: 4.3462...  0.3441 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9901...  Training loss: 4.3675...  0.3441 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9902...  Training loss: 4.2805...  0.3436 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9903...  Training loss: 4.3092...  0.3413 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9904...  Training loss: 4.2512...  0.3407 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9905...  Training loss: 4.3362...  0.3406 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9906...  Training loss: 4.2968...  0.3417 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9907...  Training loss: 4.3518...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9908...  Training loss: 4.3485...  0.3438 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9909...  Training loss: 4.3642...  0.3429 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9910...  Training loss: 4.4214...  0.3412 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9911...  Training loss: 4.2796...  0.3443 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9912...  Training loss: 4.3144...  0.3453 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9913...  Training loss: 4.3382...  0.3419 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9914...  Training loss: 4.3384...  0.3414 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9915...  Training loss: 4.2947...  0.3459 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9916...  Training loss: 4.2698...  0.3434 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9917...  Training loss: 4.3535...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9918...  Training loss: 4.3377...  0.3440 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9919...  Training loss: 4.2972...  0.3430 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9920...  Training loss: 4.2828...  0.3442 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9921...  Training loss: 4.3333...  0.3407 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9922...  Training loss: 4.3324...  0.3422 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9923...  Training loss: 4.2986...  0.3453 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/100...  Training Step: 9924...  Training loss: 4.2544...  0.3403 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9925...  Training loss: 4.2690...  0.3441 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9926...  Training loss: 4.2569...  0.3439 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9927...  Training loss: 4.1841...  0.3424 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9928...  Training loss: 4.2730...  0.3466 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9929...  Training loss: 4.2799...  0.3412 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9930...  Training loss: 4.2811...  0.3428 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9931...  Training loss: 4.2738...  0.3453 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9932...  Training loss: 4.2587...  0.3434 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9933...  Training loss: 4.3085...  0.3400 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9934...  Training loss: 4.3045...  0.3418 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9935...  Training loss: 4.2857...  0.3454 sec/batch\n",
      "Epoch: 54/100...  Training Step: 9936...  Training loss: 4.3307...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9937...  Training loss: 4.3103...  0.3436 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9938...  Training loss: 4.1458...  0.3438 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9939...  Training loss: 4.0883...  0.3411 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9940...  Training loss: 4.2040...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9941...  Training loss: 4.1799...  0.3439 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9942...  Training loss: 4.1192...  0.3431 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9943...  Training loss: 4.2963...  0.3452 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9944...  Training loss: 4.3055...  0.3451 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9945...  Training loss: 4.3209...  0.3413 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9946...  Training loss: 4.3471...  0.3438 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9947...  Training loss: 4.3161...  0.3446 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9948...  Training loss: 4.2861...  0.3395 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9949...  Training loss: 4.2781...  0.3443 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9950...  Training loss: 4.3017...  0.3422 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9951...  Training loss: 4.3338...  0.3418 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9952...  Training loss: 4.2904...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9953...  Training loss: 4.3440...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9954...  Training loss: 4.2527...  0.3452 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9955...  Training loss: 4.3557...  0.3446 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9956...  Training loss: 4.3371...  0.3411 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9957...  Training loss: 4.2813...  0.3406 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9958...  Training loss: 4.1714...  0.3415 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9959...  Training loss: 4.2308...  0.3414 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9960...  Training loss: 4.2125...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9961...  Training loss: 4.3057...  0.3445 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9962...  Training loss: 4.2685...  0.3440 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9963...  Training loss: 4.2868...  0.3454 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9964...  Training loss: 4.2562...  0.3404 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9965...  Training loss: 4.3133...  0.3407 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9966...  Training loss: 4.3736...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9967...  Training loss: 4.3317...  0.3422 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9968...  Training loss: 4.3164...  0.3407 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9969...  Training loss: 4.1568...  0.3446 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9970...  Training loss: 4.1875...  0.3427 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9971...  Training loss: 4.3263...  0.3425 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9972...  Training loss: 4.3913...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9973...  Training loss: 4.3924...  0.3405 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9974...  Training loss: 4.3848...  0.3447 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9975...  Training loss: 4.3583...  0.3441 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9976...  Training loss: 4.3926...  0.3428 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9977...  Training loss: 4.3816...  0.3430 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9978...  Training loss: 4.4409...  0.3411 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9979...  Training loss: 4.4753...  0.3428 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9980...  Training loss: 4.4103...  0.3423 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9981...  Training loss: 4.3904...  0.3401 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9982...  Training loss: 4.3317...  0.3408 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9983...  Training loss: 4.4024...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9984...  Training loss: 4.3029...  0.3441 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9985...  Training loss: 4.3192...  0.3427 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9986...  Training loss: 4.3713...  0.3429 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9987...  Training loss: 4.3726...  0.3450 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9988...  Training loss: 4.3741...  0.3424 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9989...  Training loss: 4.3616...  0.3439 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9990...  Training loss: 4.4068...  0.3443 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9991...  Training loss: 4.3935...  0.3437 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9992...  Training loss: 4.4111...  0.3412 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9993...  Training loss: 4.3192...  0.3410 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9994...  Training loss: 4.3431...  0.3434 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9995...  Training loss: 4.2660...  0.3428 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9996...  Training loss: 4.2554...  0.3434 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9997...  Training loss: 4.3002...  0.3451 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9998...  Training loss: 4.3366...  0.3448 sec/batch\n",
      "Epoch: 55/100...  Training Step: 9999...  Training loss: 4.3407...  0.3459 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10000...  Training loss: 4.3561...  0.3423 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10001...  Training loss: 4.2949...  0.3922 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10002...  Training loss: 4.3115...  0.3476 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10003...  Training loss: 4.3031...  0.3450 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10004...  Training loss: 4.2660...  0.3415 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10005...  Training loss: 4.3559...  0.3447 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10006...  Training loss: 4.3306...  0.3461 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10007...  Training loss: 4.3176...  0.3403 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10008...  Training loss: 4.2128...  0.3411 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10009...  Training loss: 4.2644...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10010...  Training loss: 4.2674...  0.3423 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10011...  Training loss: 4.2369...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10012...  Training loss: 4.2751...  0.3436 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10013...  Training loss: 4.2987...  0.3440 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10014...  Training loss: 4.2532...  0.3452 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10015...  Training loss: 4.2864...  0.3430 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10016...  Training loss: 4.2687...  0.3450 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10017...  Training loss: 4.3285...  0.3433 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10018...  Training loss: 4.2658...  0.3444 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10019...  Training loss: 4.2958...  0.3440 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10020...  Training loss: 4.2795...  0.3458 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100...  Training Step: 10021...  Training loss: 4.2839...  0.3408 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10022...  Training loss: 4.2998...  0.3435 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10023...  Training loss: 4.2521...  0.3437 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10024...  Training loss: 4.2180...  0.3445 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10025...  Training loss: 4.2962...  0.3443 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10026...  Training loss: 4.3667...  0.3441 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10027...  Training loss: 4.3015...  0.3413 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10028...  Training loss: 4.2159...  0.3467 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10029...  Training loss: 4.2977...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10030...  Training loss: 4.2651...  0.3407 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10031...  Training loss: 4.3409...  0.3425 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10032...  Training loss: 4.3032...  0.3401 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10033...  Training loss: 4.2381...  0.3445 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10034...  Training loss: 4.2204...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10035...  Training loss: 4.2339...  0.3450 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10036...  Training loss: 4.2698...  0.3431 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10037...  Training loss: 4.2946...  0.3455 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10038...  Training loss: 4.2625...  0.3412 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10039...  Training loss: 4.3361...  0.3448 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10040...  Training loss: 4.2195...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10041...  Training loss: 4.3331...  0.3402 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10042...  Training loss: 4.3161...  0.3464 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10043...  Training loss: 4.2900...  0.3434 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10044...  Training loss: 4.2420...  0.3439 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10045...  Training loss: 4.2050...  0.3406 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10046...  Training loss: 4.2255...  0.3451 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10047...  Training loss: 4.2989...  0.3447 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10048...  Training loss: 4.2562...  0.3459 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10049...  Training loss: 4.2480...  0.3422 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10050...  Training loss: 4.1685...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10051...  Training loss: 4.2839...  0.3451 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10052...  Training loss: 4.2258...  0.3439 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10053...  Training loss: 4.2828...  0.3399 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10054...  Training loss: 4.2205...  0.3467 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10055...  Training loss: 4.2733...  0.3436 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10056...  Training loss: 4.3018...  0.3445 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10057...  Training loss: 4.4128...  0.3425 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10058...  Training loss: 4.3529...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10059...  Training loss: 4.3268...  0.3453 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10060...  Training loss: 4.2664...  0.3405 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10061...  Training loss: 4.2801...  0.3429 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10062...  Training loss: 4.3085...  0.3433 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10063...  Training loss: 4.3186...  0.3429 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10064...  Training loss: 4.3316...  0.3449 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10065...  Training loss: 4.4261...  0.3448 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10066...  Training loss: 4.3799...  0.3453 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10067...  Training loss: 4.4020...  0.3413 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10068...  Training loss: 4.3483...  0.3451 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10069...  Training loss: 4.3887...  0.3415 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10070...  Training loss: 4.2856...  0.3411 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10071...  Training loss: 4.3372...  0.3449 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10072...  Training loss: 4.2831...  0.3449 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10073...  Training loss: 4.3164...  0.3406 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10074...  Training loss: 4.3108...  0.3443 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10075...  Training loss: 4.3832...  0.3446 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10076...  Training loss: 4.3218...  0.3429 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10077...  Training loss: 4.4005...  0.3428 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10078...  Training loss: 4.3721...  0.3441 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10079...  Training loss: 4.2694...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10080...  Training loss: 4.2247...  0.3436 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10081...  Training loss: 4.2756...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10082...  Training loss: 4.2679...  0.3465 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10083...  Training loss: 4.3188...  0.3437 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10084...  Training loss: 4.3176...  0.3426 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10085...  Training loss: 4.3348...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10086...  Training loss: 4.2535...  0.3460 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10087...  Training loss: 4.3360...  0.3427 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10088...  Training loss: 4.2194...  0.3418 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10089...  Training loss: 4.3036...  0.3402 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10090...  Training loss: 4.2859...  0.3414 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10091...  Training loss: 4.3413...  0.3421 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10092...  Training loss: 4.3171...  0.3410 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10093...  Training loss: 4.3342...  0.3449 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10094...  Training loss: 4.3903...  0.3429 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10095...  Training loss: 4.2567...  0.3445 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10096...  Training loss: 4.3127...  0.3423 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10097...  Training loss: 4.3115...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10098...  Training loss: 4.3469...  0.3416 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10099...  Training loss: 4.2854...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10100...  Training loss: 4.2726...  0.3442 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10101...  Training loss: 4.3543...  0.3439 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10102...  Training loss: 4.3095...  0.3427 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10103...  Training loss: 4.3013...  0.3436 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10104...  Training loss: 4.2919...  0.3427 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10105...  Training loss: 4.3336...  0.3418 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10106...  Training loss: 4.3422...  0.3433 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10107...  Training loss: 4.2867...  0.3440 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10108...  Training loss: 4.2592...  0.3414 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10109...  Training loss: 4.2401...  0.3441 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10110...  Training loss: 4.2440...  0.3423 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10111...  Training loss: 4.1965...  0.3404 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10112...  Training loss: 4.2578...  0.3403 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10113...  Training loss: 4.2905...  0.3420 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10114...  Training loss: 4.2623...  0.3450 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10115...  Training loss: 4.2545...  0.3422 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10116...  Training loss: 4.2379...  0.3415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55/100...  Training Step: 10117...  Training loss: 4.3096...  0.3418 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10118...  Training loss: 4.2687...  0.3448 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10119...  Training loss: 4.2786...  0.3410 sec/batch\n",
      "Epoch: 55/100...  Training Step: 10120...  Training loss: 4.3202...  0.3439 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10121...  Training loss: 4.2580...  0.3447 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10122...  Training loss: 4.1161...  0.3419 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10123...  Training loss: 4.0917...  0.3448 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10124...  Training loss: 4.1664...  0.3417 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10125...  Training loss: 4.1530...  0.3414 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10126...  Training loss: 4.1078...  0.3410 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10127...  Training loss: 4.3068...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10128...  Training loss: 4.3003...  0.3427 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10129...  Training loss: 4.2597...  0.3457 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10130...  Training loss: 4.3085...  0.3411 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10131...  Training loss: 4.2603...  0.3447 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10132...  Training loss: 4.2344...  0.3425 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10133...  Training loss: 4.2419...  0.3431 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10134...  Training loss: 4.2924...  0.3446 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10135...  Training loss: 4.2985...  0.3450 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10136...  Training loss: 4.2648...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10137...  Training loss: 4.3266...  0.3413 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10138...  Training loss: 4.2472...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10139...  Training loss: 4.3339...  0.3465 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10140...  Training loss: 4.2931...  0.3403 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10141...  Training loss: 4.2652...  0.3430 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10142...  Training loss: 4.1620...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10143...  Training loss: 4.1993...  0.3425 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10144...  Training loss: 4.1890...  0.3428 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10145...  Training loss: 4.2501...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10146...  Training loss: 4.2386...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10147...  Training loss: 4.2658...  0.3413 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10148...  Training loss: 4.2447...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10149...  Training loss: 4.2768...  0.3433 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10150...  Training loss: 4.3510...  0.3458 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10151...  Training loss: 4.3100...  0.3446 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10152...  Training loss: 4.2876...  0.3444 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10153...  Training loss: 4.1270...  0.3404 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10154...  Training loss: 4.1833...  0.3427 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10155...  Training loss: 4.3141...  0.3427 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10156...  Training loss: 4.3868...  0.3441 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10157...  Training loss: 4.3686...  0.3443 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10158...  Training loss: 4.3716...  0.3444 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10159...  Training loss: 4.3383...  0.3441 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10160...  Training loss: 4.3329...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10161...  Training loss: 4.3590...  0.3452 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10162...  Training loss: 4.3928...  0.3441 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10163...  Training loss: 4.4554...  0.3422 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10164...  Training loss: 4.4029...  0.3434 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10165...  Training loss: 4.3794...  0.3411 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10166...  Training loss: 4.2910...  0.3416 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10167...  Training loss: 4.3891...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10168...  Training loss: 4.2650...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10169...  Training loss: 4.3132...  0.3416 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10170...  Training loss: 4.3448...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10171...  Training loss: 4.3465...  0.3433 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10172...  Training loss: 4.3449...  0.3436 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10173...  Training loss: 4.3493...  0.3430 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10174...  Training loss: 4.3535...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10175...  Training loss: 4.3545...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10176...  Training loss: 4.3430...  0.3408 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10177...  Training loss: 4.2700...  0.3432 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10178...  Training loss: 4.3184...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10179...  Training loss: 4.2289...  0.3457 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10180...  Training loss: 4.2052...  0.3450 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10181...  Training loss: 4.2703...  0.3430 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10182...  Training loss: 4.2911...  0.3444 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10183...  Training loss: 4.2896...  0.3429 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10184...  Training loss: 4.3559...  0.3416 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10185...  Training loss: 4.2888...  0.3469 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10186...  Training loss: 4.2615...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10187...  Training loss: 4.2851...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10188...  Training loss: 4.2066...  0.3441 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10189...  Training loss: 4.3228...  0.3441 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10190...  Training loss: 4.3397...  0.3432 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10191...  Training loss: 4.2761...  0.3402 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10192...  Training loss: 4.1847...  0.3412 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10193...  Training loss: 4.2402...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10194...  Training loss: 4.2448...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10195...  Training loss: 4.2133...  0.3417 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10196...  Training loss: 4.2507...  0.3416 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10197...  Training loss: 4.2818...  0.3449 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10198...  Training loss: 4.2038...  0.3442 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10199...  Training loss: 4.2649...  0.3473 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10200...  Training loss: 4.2456...  0.3406 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10201...  Training loss: 4.2985...  0.3425 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10202...  Training loss: 4.2668...  0.3430 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10203...  Training loss: 4.2725...  0.3414 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10204...  Training loss: 4.2551...  0.3421 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10205...  Training loss: 4.2532...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10206...  Training loss: 4.2788...  0.3404 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10207...  Training loss: 4.2270...  0.3412 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10208...  Training loss: 4.2211...  0.3403 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10209...  Training loss: 4.3057...  0.3415 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10210...  Training loss: 4.3530...  0.3421 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10211...  Training loss: 4.2689...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10212...  Training loss: 4.1889...  0.3460 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56/100...  Training Step: 10213...  Training loss: 4.2619...  0.3419 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10214...  Training loss: 4.2525...  0.3400 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10215...  Training loss: 4.2800...  0.3448 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10216...  Training loss: 4.2884...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10217...  Training loss: 4.2307...  0.3449 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10218...  Training loss: 4.2041...  0.3461 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10219...  Training loss: 4.2405...  0.3419 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10220...  Training loss: 4.2366...  0.3421 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10221...  Training loss: 4.2901...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10222...  Training loss: 4.2267...  0.3410 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10223...  Training loss: 4.2797...  0.3417 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10224...  Training loss: 4.1753...  0.3396 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10225...  Training loss: 4.2907...  0.3429 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10226...  Training loss: 4.2740...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10227...  Training loss: 4.2649...  0.3410 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10228...  Training loss: 4.2137...  0.3462 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10229...  Training loss: 4.1740...  0.3400 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10230...  Training loss: 4.2280...  0.3426 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10231...  Training loss: 4.2769...  0.3444 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10232...  Training loss: 4.2141...  0.3449 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10233...  Training loss: 4.2444...  0.3432 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10234...  Training loss: 4.1887...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10235...  Training loss: 4.2597...  0.3431 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10236...  Training loss: 4.2354...  0.3412 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10237...  Training loss: 4.2389...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10238...  Training loss: 4.1818...  0.3406 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10239...  Training loss: 4.2199...  0.3454 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10240...  Training loss: 4.2520...  0.3439 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10241...  Training loss: 4.3912...  0.3434 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10242...  Training loss: 4.3556...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10243...  Training loss: 4.3170...  0.3433 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10244...  Training loss: 4.2473...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10245...  Training loss: 4.2524...  0.3454 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10246...  Training loss: 4.2627...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10247...  Training loss: 4.2875...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10248...  Training loss: 4.2761...  0.3452 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10249...  Training loss: 4.3853...  0.3457 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10250...  Training loss: 4.3665...  0.3454 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10251...  Training loss: 4.3672...  0.3418 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10252...  Training loss: 4.3405...  0.3420 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10253...  Training loss: 4.3733...  0.3433 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10254...  Training loss: 4.2570...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10255...  Training loss: 4.3114...  0.3454 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10256...  Training loss: 4.2417...  0.3447 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10257...  Training loss: 4.3190...  0.3460 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10258...  Training loss: 4.3088...  0.3426 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10259...  Training loss: 4.3432...  0.3404 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10260...  Training loss: 4.3060...  0.3420 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10261...  Training loss: 4.3424...  0.3406 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10262...  Training loss: 4.3293...  0.3433 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10263...  Training loss: 4.2536...  0.3465 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10264...  Training loss: 4.2053...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10265...  Training loss: 4.2651...  0.3411 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10266...  Training loss: 4.2613...  0.3416 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10267...  Training loss: 4.2811...  0.3477 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10268...  Training loss: 4.3111...  0.3466 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10269...  Training loss: 4.3218...  0.3456 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10270...  Training loss: 4.2209...  0.3438 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10271...  Training loss: 4.3038...  0.3414 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10272...  Training loss: 4.2013...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10273...  Training loss: 4.3043...  0.3448 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10274...  Training loss: 4.2989...  0.3445 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10275...  Training loss: 4.3193...  0.3422 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10276...  Training loss: 4.2965...  0.3442 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10277...  Training loss: 4.3153...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10278...  Training loss: 4.3591...  0.3410 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10279...  Training loss: 4.2544...  0.3450 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10280...  Training loss: 4.2688...  0.3458 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10281...  Training loss: 4.3059...  0.3451 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10282...  Training loss: 4.3119...  0.3450 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10283...  Training loss: 4.2707...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10284...  Training loss: 4.2396...  0.3427 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10285...  Training loss: 4.3476...  0.3434 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10286...  Training loss: 4.2776...  0.3422 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10287...  Training loss: 4.2589...  0.3443 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10288...  Training loss: 4.2861...  0.3450 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10289...  Training loss: 4.3062...  0.3414 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10290...  Training loss: 4.2887...  0.3455 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10291...  Training loss: 4.2772...  0.3443 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10292...  Training loss: 4.2214...  0.3445 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10293...  Training loss: 4.2253...  0.3428 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10294...  Training loss: 4.2153...  0.3423 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10295...  Training loss: 4.1659...  0.3454 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10296...  Training loss: 4.2450...  0.3453 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10297...  Training loss: 4.2475...  0.3440 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10298...  Training loss: 4.2529...  0.3412 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10299...  Training loss: 4.2569...  0.3437 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10300...  Training loss: 4.2265...  0.3431 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10301...  Training loss: 4.2772...  0.3437 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10302...  Training loss: 4.2491...  0.3421 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10303...  Training loss: 4.2429...  0.3424 sec/batch\n",
      "Epoch: 56/100...  Training Step: 10304...  Training loss: 4.2993...  0.3413 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10305...  Training loss: 4.2275...  0.3422 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10306...  Training loss: 4.1049...  0.3438 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10307...  Training loss: 4.0906...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10308...  Training loss: 4.1750...  0.3459 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/100...  Training Step: 10309...  Training loss: 4.1282...  0.3458 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10310...  Training loss: 4.0685...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10311...  Training loss: 4.2859...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10312...  Training loss: 4.2374...  0.3435 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10313...  Training loss: 4.2544...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10314...  Training loss: 4.3022...  0.3412 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10315...  Training loss: 4.2336...  0.3429 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10316...  Training loss: 4.2291...  0.3396 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10317...  Training loss: 4.2097...  0.3410 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10318...  Training loss: 4.2285...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10319...  Training loss: 4.2818...  0.3473 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10320...  Training loss: 4.2390...  0.3462 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10321...  Training loss: 4.3059...  0.3451 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10322...  Training loss: 4.2169...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10323...  Training loss: 4.3353...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10324...  Training loss: 4.2805...  0.3452 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10325...  Training loss: 4.2457...  0.3452 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10326...  Training loss: 4.1455...  0.3435 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10327...  Training loss: 4.1965...  0.3412 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10328...  Training loss: 4.1925...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10329...  Training loss: 4.2557...  0.3439 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10330...  Training loss: 4.2332...  0.3435 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10331...  Training loss: 4.2447...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10332...  Training loss: 4.2329...  0.3425 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10333...  Training loss: 4.2712...  0.3446 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10334...  Training loss: 4.3284...  0.3422 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10335...  Training loss: 4.2943...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10336...  Training loss: 4.2544...  0.3465 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10337...  Training loss: 4.1093...  0.3410 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10338...  Training loss: 4.1566...  0.3453 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10339...  Training loss: 4.2927...  0.3446 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10340...  Training loss: 4.3550...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10341...  Training loss: 4.3527...  0.3421 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10342...  Training loss: 4.3166...  0.3420 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10343...  Training loss: 4.2852...  0.3425 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10344...  Training loss: 4.3025...  0.3459 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10345...  Training loss: 4.3488...  0.3421 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10346...  Training loss: 4.3956...  0.3432 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10347...  Training loss: 4.4300...  0.3449 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10348...  Training loss: 4.3482...  0.3453 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10349...  Training loss: 4.3482...  0.3457 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10350...  Training loss: 4.2889...  0.3423 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10351...  Training loss: 4.3594...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10352...  Training loss: 4.2552...  0.3403 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10353...  Training loss: 4.3022...  0.3454 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10354...  Training loss: 4.3460...  0.3436 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10355...  Training loss: 4.3294...  0.3451 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10356...  Training loss: 4.3219...  0.3420 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10357...  Training loss: 4.3294...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10358...  Training loss: 4.3247...  0.3459 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10359...  Training loss: 4.3178...  0.3441 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10360...  Training loss: 4.3265...  0.3414 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10361...  Training loss: 4.2621...  0.3428 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10362...  Training loss: 4.3007...  0.3425 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10363...  Training loss: 4.2199...  0.3437 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10364...  Training loss: 4.2111...  0.3457 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10365...  Training loss: 4.2691...  0.3457 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10366...  Training loss: 4.2769...  0.3406 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10367...  Training loss: 4.2912...  0.3449 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10368...  Training loss: 4.3267...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10369...  Training loss: 4.2791...  0.3449 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10370...  Training loss: 4.2264...  0.3422 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10371...  Training loss: 4.2558...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10372...  Training loss: 4.2013...  0.3429 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10373...  Training loss: 4.3027...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10374...  Training loss: 4.2883...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10375...  Training loss: 4.2558...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10376...  Training loss: 4.1615...  0.3416 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10377...  Training loss: 4.2320...  0.3415 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10378...  Training loss: 4.2250...  0.3427 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10379...  Training loss: 4.1948...  0.3437 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10380...  Training loss: 4.2637...  0.3445 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10381...  Training loss: 4.2453...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10382...  Training loss: 4.1881...  0.3449 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10383...  Training loss: 4.2316...  0.3450 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10384...  Training loss: 4.2067...  0.3417 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10385...  Training loss: 4.3016...  0.3407 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10386...  Training loss: 4.2375...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10387...  Training loss: 4.2496...  0.3454 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10388...  Training loss: 4.2341...  0.3450 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10389...  Training loss: 4.2490...  0.3406 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10390...  Training loss: 4.2804...  0.3417 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10391...  Training loss: 4.2155...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10392...  Training loss: 4.1930...  0.3410 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10393...  Training loss: 4.2665...  0.3417 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10394...  Training loss: 4.3503...  0.3452 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10395...  Training loss: 4.2455...  0.3427 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10396...  Training loss: 4.1756...  0.3418 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10397...  Training loss: 4.2656...  0.3431 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10398...  Training loss: 4.2467...  0.3424 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10399...  Training loss: 4.2805...  0.3452 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10400...  Training loss: 4.2870...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10401...  Training loss: 4.2014...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10402...  Training loss: 4.1844...  0.3463 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10403...  Training loss: 4.2098...  0.3449 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10404...  Training loss: 4.2042...  0.3453 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57/100...  Training Step: 10405...  Training loss: 4.2424...  0.3432 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10406...  Training loss: 4.2241...  0.3421 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10407...  Training loss: 4.2620...  0.3432 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10408...  Training loss: 4.1501...  0.3459 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10409...  Training loss: 4.2708...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10410...  Training loss: 4.2645...  0.3451 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10411...  Training loss: 4.2248...  0.3463 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10412...  Training loss: 4.1899...  0.3467 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10413...  Training loss: 4.1526...  0.3455 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10414...  Training loss: 4.2328...  0.3459 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10415...  Training loss: 4.2775...  0.3448 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10416...  Training loss: 4.2523...  0.3441 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10417...  Training loss: 4.2393...  0.3424 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10418...  Training loss: 4.1504...  0.3434 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10419...  Training loss: 4.2577...  0.3417 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10420...  Training loss: 4.1957...  0.3396 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10421...  Training loss: 4.2353...  0.3427 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10422...  Training loss: 4.1627...  0.3418 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10423...  Training loss: 4.2165...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10424...  Training loss: 4.2367...  0.3441 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10425...  Training loss: 4.3587...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10426...  Training loss: 4.3381...  0.3428 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10427...  Training loss: 4.3124...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10428...  Training loss: 4.2478...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10429...  Training loss: 4.2563...  0.3415 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10430...  Training loss: 4.2433...  0.3419 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10431...  Training loss: 4.2641...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10432...  Training loss: 4.2601...  0.3445 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10433...  Training loss: 4.3442...  0.3446 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10434...  Training loss: 4.3348...  0.3417 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10435...  Training loss: 4.3511...  0.3431 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10436...  Training loss: 4.3070...  0.3436 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10437...  Training loss: 4.3276...  0.3429 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10438...  Training loss: 4.2480...  0.3438 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10439...  Training loss: 4.3178...  0.3445 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10440...  Training loss: 4.2275...  0.3424 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10441...  Training loss: 4.2661...  0.3424 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10442...  Training loss: 4.2601...  0.3427 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10443...  Training loss: 4.3267...  0.3403 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10444...  Training loss: 4.2455...  0.3407 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10445...  Training loss: 4.3292...  0.3430 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10446...  Training loss: 4.3269...  0.3452 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10447...  Training loss: 4.2699...  0.3455 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10448...  Training loss: 4.2219...  0.3419 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10449...  Training loss: 4.2376...  0.3422 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10450...  Training loss: 4.2423...  0.3400 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10451...  Training loss: 4.2624...  0.3426 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10452...  Training loss: 4.2746...  0.3444 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10453...  Training loss: 4.2833...  0.3450 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10454...  Training loss: 4.1924...  0.3416 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10455...  Training loss: 4.2759...  0.3431 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10456...  Training loss: 4.1972...  0.3438 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10457...  Training loss: 4.2786...  0.3440 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10458...  Training loss: 4.2602...  0.3442 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10459...  Training loss: 4.3193...  0.3441 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10460...  Training loss: 4.2776...  0.3456 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10461...  Training loss: 4.2974...  0.3420 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10462...  Training loss: 4.3401...  0.3427 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10463...  Training loss: 4.1844...  0.3445 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10464...  Training loss: 4.2239...  0.3441 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10465...  Training loss: 4.2737...  0.3412 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10466...  Training loss: 4.3049...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10467...  Training loss: 4.2570...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10468...  Training loss: 4.2050...  0.3413 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10469...  Training loss: 4.2932...  0.3448 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10470...  Training loss: 4.2377...  0.3419 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10471...  Training loss: 4.2316...  0.3406 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10472...  Training loss: 4.2353...  0.3448 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10473...  Training loss: 4.3069...  0.3412 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10474...  Training loss: 4.2925...  0.3400 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10475...  Training loss: 4.2564...  0.3477 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10476...  Training loss: 4.2234...  0.3461 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10477...  Training loss: 4.2198...  0.3459 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10478...  Training loss: 4.2112...  0.3403 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10479...  Training loss: 4.1343...  0.3411 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10480...  Training loss: 4.2184...  0.3410 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10481...  Training loss: 4.2418...  0.3453 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10482...  Training loss: 4.2117...  0.3454 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10483...  Training loss: 4.2271...  0.3405 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10484...  Training loss: 4.1945...  0.3395 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10485...  Training loss: 4.2774...  0.3403 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10486...  Training loss: 4.2431...  0.3447 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10487...  Training loss: 4.2471...  0.3468 sec/batch\n",
      "Epoch: 57/100...  Training Step: 10488...  Training loss: 4.2712...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10489...  Training loss: 4.1916...  0.3452 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10490...  Training loss: 4.0924...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10491...  Training loss: 4.0684...  0.3406 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10492...  Training loss: 4.1464...  0.3434 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10493...  Training loss: 4.0980...  0.3435 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10494...  Training loss: 4.0414...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10495...  Training loss: 4.2358...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10496...  Training loss: 4.2297...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10497...  Training loss: 4.2364...  0.3391 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10498...  Training loss: 4.2490...  0.3437 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10499...  Training loss: 4.2512...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10500...  Training loss: 4.2034...  0.3462 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100...  Training Step: 10501...  Training loss: 4.2163...  0.3419 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10502...  Training loss: 4.2048...  0.3421 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10503...  Training loss: 4.2675...  0.3441 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10504...  Training loss: 4.2336...  0.3456 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10505...  Training loss: 4.2894...  0.3440 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10506...  Training loss: 4.2034...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10507...  Training loss: 4.2800...  0.3441 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10508...  Training loss: 4.2607...  0.3436 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10509...  Training loss: 4.2096...  0.3409 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10510...  Training loss: 4.1491...  0.3414 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10511...  Training loss: 4.1563...  0.3459 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10512...  Training loss: 4.1709...  0.3404 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10513...  Training loss: 4.2263...  0.3417 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10514...  Training loss: 4.2196...  0.3428 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10515...  Training loss: 4.2505...  0.3426 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10516...  Training loss: 4.2137...  0.3457 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10517...  Training loss: 4.2451...  0.3454 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10518...  Training loss: 4.3025...  0.3397 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10519...  Training loss: 4.2764...  0.3433 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10520...  Training loss: 4.2327...  0.3408 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10521...  Training loss: 4.0908...  0.3440 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10522...  Training loss: 4.1557...  0.3412 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10523...  Training loss: 4.2723...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10524...  Training loss: 4.3171...  0.3455 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10525...  Training loss: 4.3537...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10526...  Training loss: 4.3422...  0.3408 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10527...  Training loss: 4.3025...  0.3434 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10528...  Training loss: 4.3039...  0.3453 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10529...  Training loss: 4.3273...  0.3450 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10530...  Training loss: 4.3545...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10531...  Training loss: 4.3995...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10532...  Training loss: 4.3139...  0.3446 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10533...  Training loss: 4.3003...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10534...  Training loss: 4.2831...  0.3447 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10535...  Training loss: 4.3341...  0.3450 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10536...  Training loss: 4.2308...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10537...  Training loss: 4.2849...  0.3411 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10538...  Training loss: 4.2964...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10539...  Training loss: 4.3016...  0.3402 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10540...  Training loss: 4.3127...  0.3442 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10541...  Training loss: 4.2952...  0.3404 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10542...  Training loss: 4.3109...  0.3430 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10543...  Training loss: 4.2894...  0.3425 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10544...  Training loss: 4.2896...  0.3427 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10545...  Training loss: 4.2284...  0.3432 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10546...  Training loss: 4.2716...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10547...  Training loss: 4.1712...  0.3466 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10548...  Training loss: 4.1567...  0.3446 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10549...  Training loss: 4.2191...  0.3441 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10550...  Training loss: 4.2797...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10551...  Training loss: 4.2427...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10552...  Training loss: 4.3329...  0.3447 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10553...  Training loss: 4.2437...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10554...  Training loss: 4.2481...  0.3410 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10555...  Training loss: 4.2293...  0.3453 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10556...  Training loss: 4.2032...  0.3419 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10557...  Training loss: 4.2767...  0.3432 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10558...  Training loss: 4.2833...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10559...  Training loss: 4.2599...  0.3442 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10560...  Training loss: 4.1401...  0.3403 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10561...  Training loss: 4.2041...  0.3455 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10562...  Training loss: 4.1952...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10563...  Training loss: 4.1763...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10564...  Training loss: 4.2312...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10565...  Training loss: 4.2404...  0.3451 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10566...  Training loss: 4.1725...  0.3409 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10567...  Training loss: 4.2396...  0.3415 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10568...  Training loss: 4.1848...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10569...  Training loss: 4.2899...  0.3419 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10570...  Training loss: 4.2661...  0.3426 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10571...  Training loss: 4.2601...  0.3427 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10572...  Training loss: 4.2084...  0.3420 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10573...  Training loss: 4.2457...  0.3433 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10574...  Training loss: 4.2714...  0.3430 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10575...  Training loss: 4.2122...  0.3417 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10576...  Training loss: 4.1801...  0.3452 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10577...  Training loss: 4.2655...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10578...  Training loss: 4.3460...  0.3405 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10579...  Training loss: 4.2246...  0.3434 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10580...  Training loss: 4.1566...  0.3427 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10581...  Training loss: 4.2800...  0.3446 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10582...  Training loss: 4.2594...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10583...  Training loss: 4.2874...  0.3437 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10584...  Training loss: 4.2993...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10585...  Training loss: 4.2283...  0.3403 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10586...  Training loss: 4.2133...  0.3458 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10587...  Training loss: 4.1913...  0.3433 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10588...  Training loss: 4.2015...  0.3441 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10589...  Training loss: 4.2422...  0.3422 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10590...  Training loss: 4.1873...  0.3436 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10591...  Training loss: 4.2495...  0.3442 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10592...  Training loss: 4.1157...  0.3462 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10593...  Training loss: 4.2517...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10594...  Training loss: 4.2513...  0.3410 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10595...  Training loss: 4.2175...  0.3427 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10596...  Training loss: 4.1874...  0.3420 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/100...  Training Step: 10597...  Training loss: 4.1610...  0.3437 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10598...  Training loss: 4.1803...  0.3457 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10599...  Training loss: 4.2371...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10600...  Training loss: 4.2333...  0.3408 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10601...  Training loss: 4.2439...  0.3426 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10602...  Training loss: 4.1913...  0.3409 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10603...  Training loss: 4.2516...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10604...  Training loss: 4.2128...  0.3439 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10605...  Training loss: 4.2484...  0.3454 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10606...  Training loss: 4.1577...  0.3426 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10607...  Training loss: 4.2273...  0.3426 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10608...  Training loss: 4.2140...  0.3437 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10609...  Training loss: 4.3552...  0.3428 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10610...  Training loss: 4.3290...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10611...  Training loss: 4.2982...  0.3412 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10612...  Training loss: 4.2269...  0.3422 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10613...  Training loss: 4.2536...  0.3452 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10614...  Training loss: 4.2512...  0.3429 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10615...  Training loss: 4.2482...  0.3469 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10616...  Training loss: 4.2512...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10617...  Training loss: 4.3106...  0.3454 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10618...  Training loss: 4.3010...  0.3449 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10619...  Training loss: 4.3228...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10620...  Training loss: 4.2787...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10621...  Training loss: 4.2913...  0.3460 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10622...  Training loss: 4.2190...  0.3456 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10623...  Training loss: 4.3008...  0.3420 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10624...  Training loss: 4.2012...  0.3401 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10625...  Training loss: 4.2489...  0.3445 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10626...  Training loss: 4.2292...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10627...  Training loss: 4.2774...  0.3451 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10628...  Training loss: 4.2056...  0.3431 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10629...  Training loss: 4.2991...  0.3438 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10630...  Training loss: 4.2968...  0.3417 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10631...  Training loss: 4.2235...  0.3435 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10632...  Training loss: 4.1982...  0.3423 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10633...  Training loss: 4.2356...  0.3461 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10634...  Training loss: 4.2141...  0.3452 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10635...  Training loss: 4.2393...  0.3454 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10636...  Training loss: 4.2411...  0.3435 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10637...  Training loss: 4.2697...  0.3442 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10638...  Training loss: 4.1770...  0.3451 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10639...  Training loss: 4.2814...  0.3424 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10640...  Training loss: 4.1890...  0.3411 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10641...  Training loss: 4.2741...  0.3423 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10642...  Training loss: 4.2427...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10643...  Training loss: 4.2811...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10644...  Training loss: 4.2624...  0.3447 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10645...  Training loss: 4.2897...  0.3413 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10646...  Training loss: 4.3216...  0.3436 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10647...  Training loss: 4.1541...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10648...  Training loss: 4.2129...  0.3446 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10649...  Training loss: 4.2545...  0.3410 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10650...  Training loss: 4.2821...  0.3417 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10651...  Training loss: 4.2455...  0.3423 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10652...  Training loss: 4.2194...  0.3410 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10653...  Training loss: 4.2901...  0.3414 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10654...  Training loss: 4.2057...  0.3414 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10655...  Training loss: 4.2383...  0.3430 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10656...  Training loss: 4.2262...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10657...  Training loss: 4.2961...  0.3412 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10658...  Training loss: 4.2835...  0.3443 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10659...  Training loss: 4.2413...  0.3451 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10660...  Training loss: 4.2030...  0.3444 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10661...  Training loss: 4.2010...  0.3433 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10662...  Training loss: 4.1952...  0.3405 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10663...  Training loss: 4.1549...  0.3407 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10664...  Training loss: 4.2064...  0.3419 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10665...  Training loss: 4.2338...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10666...  Training loss: 4.2165...  0.3427 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10667...  Training loss: 4.1792...  0.3429 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10668...  Training loss: 4.1744...  0.3416 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10669...  Training loss: 4.2288...  0.3415 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10670...  Training loss: 4.2210...  0.3448 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10671...  Training loss: 4.2551...  0.3449 sec/batch\n",
      "Epoch: 58/100...  Training Step: 10672...  Training loss: 4.2717...  0.3446 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10673...  Training loss: 4.1923...  0.3438 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10674...  Training loss: 4.0774...  0.3434 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10675...  Training loss: 4.0299...  0.3430 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10676...  Training loss: 4.1500...  0.3460 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10677...  Training loss: 4.0891...  0.3443 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10678...  Training loss: 4.0602...  0.3433 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10679...  Training loss: 4.2451...  0.3435 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10680...  Training loss: 4.2091...  0.3458 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10681...  Training loss: 4.2352...  0.3414 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10682...  Training loss: 4.2401...  0.3447 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10683...  Training loss: 4.2132...  0.3444 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10684...  Training loss: 4.1564...  0.3446 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10685...  Training loss: 4.2071...  0.3444 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10686...  Training loss: 4.1871...  0.3402 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10687...  Training loss: 4.2457...  0.3417 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10688...  Training loss: 4.1888...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10689...  Training loss: 4.2440...  0.3437 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10690...  Training loss: 4.2106...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10691...  Training loss: 4.2839...  0.3441 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10692...  Training loss: 4.2596...  0.3424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100...  Training Step: 10693...  Training loss: 4.1902...  0.3433 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10694...  Training loss: 4.1328...  0.3430 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10695...  Training loss: 4.1442...  0.3424 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10696...  Training loss: 4.1604...  0.3428 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10697...  Training loss: 4.2324...  0.3428 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10698...  Training loss: 4.2349...  0.3445 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10699...  Training loss: 4.2316...  0.3440 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10700...  Training loss: 4.2205...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10701...  Training loss: 4.2619...  0.3446 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10702...  Training loss: 4.3030...  0.3414 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10703...  Training loss: 4.2762...  0.3432 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10704...  Training loss: 4.2321...  0.3414 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10705...  Training loss: 4.1006...  0.3451 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10706...  Training loss: 4.1505...  0.3423 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10707...  Training loss: 4.2518...  0.3443 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10708...  Training loss: 4.3160...  0.3441 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10709...  Training loss: 4.3150...  0.3410 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10710...  Training loss: 4.2807...  0.3414 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10711...  Training loss: 4.2726...  0.3433 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10712...  Training loss: 4.2940...  0.3442 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10713...  Training loss: 4.3204...  0.3429 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10714...  Training loss: 4.3314...  0.3421 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10715...  Training loss: 4.3844...  0.3430 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10716...  Training loss: 4.3189...  0.3418 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10717...  Training loss: 4.2854...  0.3463 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10718...  Training loss: 4.2398...  0.3442 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10719...  Training loss: 4.3130...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10720...  Training loss: 4.2434...  0.3450 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10721...  Training loss: 4.2678...  0.3459 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10722...  Training loss: 4.2890...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10723...  Training loss: 4.2893...  0.3436 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10724...  Training loss: 4.2749...  0.3456 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10725...  Training loss: 4.2580...  0.3412 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10726...  Training loss: 4.2797...  0.3458 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10727...  Training loss: 4.2910...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10728...  Training loss: 4.2868...  0.3411 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10729...  Training loss: 4.2104...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10730...  Training loss: 4.2455...  0.3453 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10731...  Training loss: 4.1718...  0.3441 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10732...  Training loss: 4.1485...  0.3408 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10733...  Training loss: 4.2108...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10734...  Training loss: 4.2314...  0.3437 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10735...  Training loss: 4.2277...  0.3447 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10736...  Training loss: 4.2942...  0.3429 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10737...  Training loss: 4.2205...  0.3428 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10738...  Training loss: 4.1857...  0.3421 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10739...  Training loss: 4.2043...  0.3434 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10740...  Training loss: 4.1772...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10741...  Training loss: 4.2740...  0.3442 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10742...  Training loss: 4.2655...  0.3449 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10743...  Training loss: 4.2536...  0.3468 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10744...  Training loss: 4.1344...  0.3431 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10745...  Training loss: 4.1957...  0.3456 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10746...  Training loss: 4.1866...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10747...  Training loss: 4.1550...  0.3454 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10748...  Training loss: 4.1887...  0.3435 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10749...  Training loss: 4.2092...  0.3444 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10750...  Training loss: 4.1353...  0.3450 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10751...  Training loss: 4.2078...  0.3403 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10752...  Training loss: 4.1350...  0.3400 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10753...  Training loss: 4.2325...  0.3446 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10754...  Training loss: 4.1787...  0.3420 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10755...  Training loss: 4.1766...  0.3424 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10756...  Training loss: 4.1547...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10757...  Training loss: 4.2286...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10758...  Training loss: 4.2209...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10759...  Training loss: 4.2022...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10760...  Training loss: 4.1626...  0.3417 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10761...  Training loss: 4.2232...  0.3422 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10762...  Training loss: 4.2823...  0.3429 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10763...  Training loss: 4.1809...  0.3459 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10764...  Training loss: 4.1240...  0.3404 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10765...  Training loss: 4.2086...  0.3394 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10766...  Training loss: 4.1949...  0.3427 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10767...  Training loss: 4.2404...  0.3418 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10768...  Training loss: 4.2543...  0.3453 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10769...  Training loss: 4.1773...  0.3444 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10770...  Training loss: 4.1600...  0.3424 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10771...  Training loss: 4.1836...  0.3419 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10772...  Training loss: 4.1891...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10773...  Training loss: 4.2228...  0.3425 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10774...  Training loss: 4.1881...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10775...  Training loss: 4.2290...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10776...  Training loss: 4.1211...  0.3445 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10777...  Training loss: 4.2308...  0.3458 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10778...  Training loss: 4.2024...  0.3459 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10779...  Training loss: 4.1881...  0.3448 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10780...  Training loss: 4.1636...  0.3442 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10781...  Training loss: 4.1404...  0.3434 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10782...  Training loss: 4.1557...  0.3456 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10783...  Training loss: 4.2116...  0.3451 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10784...  Training loss: 4.1827...  0.3409 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10785...  Training loss: 4.1700...  0.3427 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10786...  Training loss: 4.1524...  0.3430 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10787...  Training loss: 4.1966...  0.3413 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10788...  Training loss: 4.1956...  0.3419 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59/100...  Training Step: 10789...  Training loss: 4.2355...  0.3448 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10790...  Training loss: 4.1572...  0.3446 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10791...  Training loss: 4.2270...  0.3455 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10792...  Training loss: 4.2163...  0.3466 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10793...  Training loss: 4.3492...  0.3443 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10794...  Training loss: 4.3050...  0.3459 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10795...  Training loss: 4.2936...  0.3425 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10796...  Training loss: 4.2300...  0.3438 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10797...  Training loss: 4.2764...  0.3436 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10798...  Training loss: 4.2693...  0.3435 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10799...  Training loss: 4.2438...  0.3440 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10800...  Training loss: 4.2629...  0.3453 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10801...  Training loss: 4.3162...  0.3429 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10802...  Training loss: 4.2877...  0.3413 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10803...  Training loss: 4.3018...  0.3421 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10804...  Training loss: 4.2492...  0.3451 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10805...  Training loss: 4.2881...  0.3428 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10806...  Training loss: 4.1905...  0.3407 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10807...  Training loss: 4.2683...  0.3436 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10808...  Training loss: 4.2127...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10809...  Training loss: 4.2376...  0.3436 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10810...  Training loss: 4.2164...  0.3405 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10811...  Training loss: 4.2866...  0.3465 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10812...  Training loss: 4.1582...  0.3432 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10813...  Training loss: 4.2782...  0.3385 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10814...  Training loss: 4.2645...  0.3395 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10815...  Training loss: 4.2295...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10816...  Training loss: 4.2169...  0.3410 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10817...  Training loss: 4.2197...  0.3392 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10818...  Training loss: 4.2104...  0.3400 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10819...  Training loss: 4.2218...  0.3411 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10820...  Training loss: 4.2137...  0.3412 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10821...  Training loss: 4.2455...  0.3390 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10822...  Training loss: 4.1633...  0.3421 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10823...  Training loss: 4.2085...  0.3410 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10824...  Training loss: 4.1756...  0.3406 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10825...  Training loss: 4.2285...  0.3418 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10826...  Training loss: 4.2295...  0.3412 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10827...  Training loss: 4.2618...  0.3405 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10828...  Training loss: 4.2449...  0.3418 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10829...  Training loss: 4.2556...  0.3425 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10830...  Training loss: 4.3183...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10831...  Training loss: 4.1545...  0.3440 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10832...  Training loss: 4.2240...  0.3415 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10833...  Training loss: 4.2254...  0.3432 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10834...  Training loss: 4.2594...  0.3434 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10835...  Training loss: 4.2192...  0.3444 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10836...  Training loss: 4.1840...  0.3408 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10837...  Training loss: 4.2704...  0.3428 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10838...  Training loss: 4.2255...  0.3453 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10839...  Training loss: 4.2109...  0.3461 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10840...  Training loss: 4.2234...  0.3443 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10841...  Training loss: 4.2591...  0.3442 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10842...  Training loss: 4.2856...  0.3452 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10843...  Training loss: 4.2317...  0.3440 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10844...  Training loss: 4.1805...  0.3416 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10845...  Training loss: 4.1929...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10846...  Training loss: 4.1870...  0.3424 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10847...  Training loss: 4.1541...  0.3447 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10848...  Training loss: 4.2073...  0.3408 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10849...  Training loss: 4.2365...  0.3449 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10850...  Training loss: 4.1901...  0.3456 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10851...  Training loss: 4.1876...  0.3432 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10852...  Training loss: 4.1717...  0.3443 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10853...  Training loss: 4.1925...  0.3426 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10854...  Training loss: 4.2016...  0.3420 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10855...  Training loss: 4.1914...  0.3438 sec/batch\n",
      "Epoch: 59/100...  Training Step: 10856...  Training loss: 4.2559...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10857...  Training loss: 4.1731...  0.3429 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10858...  Training loss: 4.0069...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10859...  Training loss: 4.0304...  0.3412 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10860...  Training loss: 4.1014...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10861...  Training loss: 4.0760...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10862...  Training loss: 4.0335...  0.3452 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10863...  Training loss: 4.1733...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10864...  Training loss: 4.2020...  0.3406 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10865...  Training loss: 4.2108...  0.3417 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10866...  Training loss: 4.2618...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10867...  Training loss: 4.1866...  0.3445 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10868...  Training loss: 4.1605...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10869...  Training loss: 4.1643...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10870...  Training loss: 4.1845...  0.3410 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10871...  Training loss: 4.2211...  0.3427 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10872...  Training loss: 4.1984...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10873...  Training loss: 4.2613...  0.3417 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10874...  Training loss: 4.1736...  0.3437 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10875...  Training loss: 4.2588...  0.3445 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10876...  Training loss: 4.2427...  0.3419 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10877...  Training loss: 4.2026...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10878...  Training loss: 4.1106...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10879...  Training loss: 4.1275...  0.3420 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10880...  Training loss: 4.1560...  0.3421 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10881...  Training loss: 4.1842...  0.3439 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10882...  Training loss: 4.1938...  0.3447 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10883...  Training loss: 4.1997...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10884...  Training loss: 4.1847...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/100...  Training Step: 10885...  Training loss: 4.2262...  0.3409 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10886...  Training loss: 4.2545...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10887...  Training loss: 4.2471...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10888...  Training loss: 4.2272...  0.3451 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10889...  Training loss: 4.0685...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10890...  Training loss: 4.1401...  0.3397 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10891...  Training loss: 4.2388...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10892...  Training loss: 4.2643...  0.3428 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10893...  Training loss: 4.2907...  0.3426 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10894...  Training loss: 4.2555...  0.3404 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10895...  Training loss: 4.2521...  0.3450 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10896...  Training loss: 4.2669...  0.3448 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10897...  Training loss: 4.2800...  0.3410 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10898...  Training loss: 4.3091...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10899...  Training loss: 4.3571...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10900...  Training loss: 4.2628...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10901...  Training loss: 4.2738...  0.3411 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10902...  Training loss: 4.2311...  0.3405 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10903...  Training loss: 4.2785...  0.3416 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10904...  Training loss: 4.1897...  0.3404 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10905...  Training loss: 4.2588...  0.3409 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10906...  Training loss: 4.2707...  0.3416 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10907...  Training loss: 4.2718...  0.3421 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10908...  Training loss: 4.2836...  0.3406 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10909...  Training loss: 4.2640...  0.3450 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10910...  Training loss: 4.2487...  0.3426 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10911...  Training loss: 4.2660...  0.3430 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10912...  Training loss: 4.2601...  0.3447 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10913...  Training loss: 4.1836...  0.3432 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10914...  Training loss: 4.2378...  0.3401 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10915...  Training loss: 4.1557...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10916...  Training loss: 4.1570...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10917...  Training loss: 4.1924...  0.3438 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10918...  Training loss: 4.2137...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10919...  Training loss: 4.2198...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10920...  Training loss: 4.2498...  0.3421 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10921...  Training loss: 4.1891...  0.3395 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10922...  Training loss: 4.1943...  0.3422 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10923...  Training loss: 4.2020...  0.3435 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10924...  Training loss: 4.1645...  0.3434 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10925...  Training loss: 4.2610...  0.3442 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10926...  Training loss: 4.2755...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10927...  Training loss: 4.2200...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10928...  Training loss: 4.1200...  0.3446 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10929...  Training loss: 4.1656...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10930...  Training loss: 4.1586...  0.3437 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10931...  Training loss: 4.1466...  0.3439 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10932...  Training loss: 4.1703...  0.3420 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10933...  Training loss: 4.2105...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10934...  Training loss: 4.1248...  0.3428 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10935...  Training loss: 4.1823...  0.3426 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10936...  Training loss: 4.1480...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10937...  Training loss: 4.2065...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10938...  Training loss: 4.1860...  0.3450 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10939...  Training loss: 4.1615...  0.3434 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10940...  Training loss: 4.1579...  0.3420 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10941...  Training loss: 4.1689...  0.3400 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10942...  Training loss: 4.1761...  0.3425 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10943...  Training loss: 4.1466...  0.3440 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10944...  Training loss: 4.1082...  0.3418 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10945...  Training loss: 4.2140...  0.3403 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10946...  Training loss: 4.2821...  0.3457 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10947...  Training loss: 4.2141...  0.3434 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10948...  Training loss: 4.1126...  0.3428 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10949...  Training loss: 4.2087...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10950...  Training loss: 4.1757...  0.3428 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10951...  Training loss: 4.2164...  0.3439 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10952...  Training loss: 4.2413...  0.3453 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10953...  Training loss: 4.1751...  0.3457 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10954...  Training loss: 4.1450...  0.3447 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10955...  Training loss: 4.1787...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10956...  Training loss: 4.1662...  0.3452 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10957...  Training loss: 4.1924...  0.3414 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10958...  Training loss: 4.1520...  0.3422 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10959...  Training loss: 4.1878...  0.3433 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10960...  Training loss: 4.0923...  0.3411 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10961...  Training loss: 4.2003...  0.3412 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10962...  Training loss: 4.1919...  0.3438 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10963...  Training loss: 4.1566...  0.3412 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10964...  Training loss: 4.1415...  0.3447 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10965...  Training loss: 4.1079...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10966...  Training loss: 4.1389...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10967...  Training loss: 4.1695...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10968...  Training loss: 4.1444...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10969...  Training loss: 4.1603...  0.3437 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10970...  Training loss: 4.1179...  0.3422 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10971...  Training loss: 4.2011...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10972...  Training loss: 4.1687...  0.3419 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10973...  Training loss: 4.2194...  0.3457 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10974...  Training loss: 4.1438...  0.3439 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10975...  Training loss: 4.2198...  0.3410 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10976...  Training loss: 4.1757...  0.3406 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10977...  Training loss: 4.3225...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10978...  Training loss: 4.2877...  0.3456 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10979...  Training loss: 4.2706...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10980...  Training loss: 4.1850...  0.3407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/100...  Training Step: 10981...  Training loss: 4.2246...  0.3446 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10982...  Training loss: 4.2397...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10983...  Training loss: 4.2463...  0.3438 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10984...  Training loss: 4.2366...  0.3410 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10985...  Training loss: 4.3072...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10986...  Training loss: 4.2471...  0.3434 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10987...  Training loss: 4.2735...  0.3413 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10988...  Training loss: 4.2388...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10989...  Training loss: 4.2479...  0.3458 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10990...  Training loss: 4.2031...  0.3412 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10991...  Training loss: 4.2696...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10992...  Training loss: 4.2044...  0.3438 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10993...  Training loss: 4.2572...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10994...  Training loss: 4.2092...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10995...  Training loss: 4.2715...  0.3453 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10996...  Training loss: 4.1617...  0.3424 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10997...  Training loss: 4.2470...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10998...  Training loss: 4.2460...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 10999...  Training loss: 4.1898...  0.3457 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11000...  Training loss: 4.1402...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11001...  Training loss: 4.1980...  0.3913 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11002...  Training loss: 4.1988...  0.3496 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11003...  Training loss: 4.2144...  0.3396 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11004...  Training loss: 4.2152...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11005...  Training loss: 4.2293...  0.3409 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11006...  Training loss: 4.1203...  0.3420 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11007...  Training loss: 4.1865...  0.3421 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11008...  Training loss: 4.1399...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11009...  Training loss: 4.1930...  0.3435 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11010...  Training loss: 4.2107...  0.3427 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11011...  Training loss: 4.2471...  0.3459 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11012...  Training loss: 4.2269...  0.3450 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11013...  Training loss: 4.2703...  0.3444 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11014...  Training loss: 4.3027...  0.3441 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11015...  Training loss: 4.1533...  0.3410 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11016...  Training loss: 4.2006...  0.3403 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11017...  Training loss: 4.2087...  0.3416 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11018...  Training loss: 4.2279...  0.3449 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11019...  Training loss: 4.1754...  0.3428 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11020...  Training loss: 4.1867...  0.3416 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11021...  Training loss: 4.2292...  0.3405 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11022...  Training loss: 4.2132...  0.3448 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11023...  Training loss: 4.1918...  0.3416 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11024...  Training loss: 4.1997...  0.3409 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11025...  Training loss: 4.2570...  0.3431 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11026...  Training loss: 4.2573...  0.3458 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11027...  Training loss: 4.2056...  0.3407 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11028...  Training loss: 4.1721...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11029...  Training loss: 4.1369...  0.3443 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11030...  Training loss: 4.1499...  0.3436 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11031...  Training loss: 4.1146...  0.3415 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11032...  Training loss: 4.1954...  0.3429 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11033...  Training loss: 4.2268...  0.3465 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11034...  Training loss: 4.1806...  0.3433 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11035...  Training loss: 4.1769...  0.3409 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11036...  Training loss: 4.1375...  0.3423 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11037...  Training loss: 4.2027...  0.3442 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11038...  Training loss: 4.1534...  0.3461 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11039...  Training loss: 4.2024...  0.3420 sec/batch\n",
      "Epoch: 60/100...  Training Step: 11040...  Training loss: 4.1896...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11041...  Training loss: 4.1431...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11042...  Training loss: 3.9900...  0.3401 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11043...  Training loss: 4.0117...  0.3439 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11044...  Training loss: 4.1104...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11045...  Training loss: 4.0710...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11046...  Training loss: 4.0362...  0.3457 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11047...  Training loss: 4.1681...  0.3448 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11048...  Training loss: 4.1799...  0.3441 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11049...  Training loss: 4.1955...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11050...  Training loss: 4.2171...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11051...  Training loss: 4.1768...  0.3431 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11052...  Training loss: 4.1561...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11053...  Training loss: 4.1747...  0.3430 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11054...  Training loss: 4.1882...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11055...  Training loss: 4.2195...  0.3449 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11056...  Training loss: 4.1692...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11057...  Training loss: 4.2191...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11058...  Training loss: 4.1662...  0.3431 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11059...  Training loss: 4.2246...  0.3453 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11060...  Training loss: 4.1965...  0.3454 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11061...  Training loss: 4.1580...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11062...  Training loss: 4.0746...  0.3430 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11063...  Training loss: 4.1225...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11064...  Training loss: 4.1310...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11065...  Training loss: 4.1936...  0.3444 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11066...  Training loss: 4.2069...  0.3456 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11067...  Training loss: 4.2144...  0.3427 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11068...  Training loss: 4.1829...  0.3412 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11069...  Training loss: 4.2153...  0.3403 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11070...  Training loss: 4.2500...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11071...  Training loss: 4.2356...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11072...  Training loss: 4.2043...  0.3429 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11073...  Training loss: 4.0501...  0.3450 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11074...  Training loss: 4.1184...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11075...  Training loss: 4.1972...  0.3428 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11076...  Training loss: 4.2832...  0.3418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100...  Training Step: 11077...  Training loss: 4.2859...  0.3422 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11078...  Training loss: 4.2596...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11079...  Training loss: 4.2466...  0.3426 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11080...  Training loss: 4.2471...  0.3454 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11081...  Training loss: 4.2616...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11082...  Training loss: 4.3224...  0.3444 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11083...  Training loss: 4.3672...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11084...  Training loss: 4.2851...  0.3441 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11085...  Training loss: 4.2746...  0.3427 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11086...  Training loss: 4.2240...  0.3432 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11087...  Training loss: 4.2883...  0.3415 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11088...  Training loss: 4.1839...  0.3437 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11089...  Training loss: 4.2379...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11090...  Training loss: 4.2452...  0.3420 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11091...  Training loss: 4.2702...  0.3428 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11092...  Training loss: 4.2827...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11093...  Training loss: 4.2363...  0.3445 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11094...  Training loss: 4.2603...  0.3428 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11095...  Training loss: 4.2389...  0.3408 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11096...  Training loss: 4.2344...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11097...  Training loss: 4.1798...  0.3426 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11098...  Training loss: 4.2045...  0.3439 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11099...  Training loss: 4.1383...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11100...  Training loss: 4.1326...  0.3426 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11101...  Training loss: 4.2072...  0.3418 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11102...  Training loss: 4.2184...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11103...  Training loss: 4.1824...  0.3404 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11104...  Training loss: 4.2456...  0.3479 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11105...  Training loss: 4.1679...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11106...  Training loss: 4.1696...  0.3451 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11107...  Training loss: 4.1735...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11108...  Training loss: 4.1337...  0.3414 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11109...  Training loss: 4.2491...  0.3456 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11110...  Training loss: 4.2537...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11111...  Training loss: 4.1940...  0.3427 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11112...  Training loss: 4.1038...  0.3416 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11113...  Training loss: 4.2018...  0.3418 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11114...  Training loss: 4.1677...  0.3407 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11115...  Training loss: 4.1763...  0.3431 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11116...  Training loss: 4.1841...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11117...  Training loss: 4.1864...  0.3450 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11118...  Training loss: 4.1206...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11119...  Training loss: 4.1819...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11120...  Training loss: 4.1440...  0.3451 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11121...  Training loss: 4.2074...  0.3452 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11122...  Training loss: 4.1711...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11123...  Training loss: 4.1579...  0.3413 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11124...  Training loss: 4.1490...  0.3450 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11125...  Training loss: 4.1729...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11126...  Training loss: 4.1735...  0.3455 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11127...  Training loss: 4.1428...  0.3419 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11128...  Training loss: 4.1089...  0.3460 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11129...  Training loss: 4.1912...  0.3412 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11130...  Training loss: 4.2510...  0.3441 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11131...  Training loss: 4.1718...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11132...  Training loss: 4.1019...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11133...  Training loss: 4.1520...  0.3437 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11134...  Training loss: 4.1564...  0.3439 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11135...  Training loss: 4.2403...  0.3432 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11136...  Training loss: 4.2287...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11137...  Training loss: 4.1448...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11138...  Training loss: 4.1477...  0.3429 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11139...  Training loss: 4.1569...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11140...  Training loss: 4.1598...  0.3444 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11141...  Training loss: 4.1891...  0.3417 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11142...  Training loss: 4.1453...  0.3455 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11143...  Training loss: 4.1829...  0.3429 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11144...  Training loss: 4.0758...  0.3453 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11145...  Training loss: 4.1802...  0.3430 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11146...  Training loss: 4.1819...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11147...  Training loss: 4.1418...  0.3434 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11148...  Training loss: 4.1414...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11149...  Training loss: 4.1177...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11150...  Training loss: 4.1092...  0.3440 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11151...  Training loss: 4.1709...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11152...  Training loss: 4.1112...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11153...  Training loss: 4.1197...  0.3454 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11154...  Training loss: 4.0767...  0.3416 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11155...  Training loss: 4.1615...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11156...  Training loss: 4.1342...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11157...  Training loss: 4.1758...  0.3418 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11158...  Training loss: 4.1223...  0.3450 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11159...  Training loss: 4.2215...  0.3416 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11160...  Training loss: 4.1983...  0.3408 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11161...  Training loss: 4.3386...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11162...  Training loss: 4.2960...  0.3495 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11163...  Training loss: 4.2663...  0.3446 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11164...  Training loss: 4.1783...  0.3430 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11165...  Training loss: 4.2063...  0.3427 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11166...  Training loss: 4.2168...  0.3422 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11167...  Training loss: 4.2417...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11168...  Training loss: 4.2203...  0.3447 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11169...  Training loss: 4.2894...  0.3454 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11170...  Training loss: 4.2615...  0.3458 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11171...  Training loss: 4.2677...  0.3439 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11172...  Training loss: 4.2234...  0.3465 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/100...  Training Step: 11173...  Training loss: 4.2662...  0.3442 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11174...  Training loss: 4.1596...  0.3408 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11175...  Training loss: 4.2369...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11176...  Training loss: 4.1505...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11177...  Training loss: 4.2132...  0.3449 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11178...  Training loss: 4.2102...  0.3419 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11179...  Training loss: 4.2555...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11180...  Training loss: 4.1749...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11181...  Training loss: 4.2623...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11182...  Training loss: 4.2279...  0.3418 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11183...  Training loss: 4.1883...  0.3425 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11184...  Training loss: 4.1331...  0.3411 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11185...  Training loss: 4.1775...  0.3403 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11186...  Training loss: 4.1859...  0.3403 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11187...  Training loss: 4.1888...  0.3436 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11188...  Training loss: 4.2013...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11189...  Training loss: 4.2445...  0.3416 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11190...  Training loss: 4.1490...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11191...  Training loss: 4.2039...  0.3443 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11192...  Training loss: 4.1482...  0.3435 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11193...  Training loss: 4.1995...  0.3408 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11194...  Training loss: 4.2103...  0.3415 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11195...  Training loss: 4.2342...  0.3419 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11196...  Training loss: 4.2087...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11197...  Training loss: 4.2335...  0.3421 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11198...  Training loss: 4.2830...  0.3388 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11199...  Training loss: 4.1510...  0.3418 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11200...  Training loss: 4.2311...  0.3438 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11201...  Training loss: 4.2155...  0.3424 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11202...  Training loss: 4.2521...  0.3420 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11203...  Training loss: 4.1834...  0.3422 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11204...  Training loss: 4.1531...  0.3432 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11205...  Training loss: 4.2348...  0.3410 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11206...  Training loss: 4.1738...  0.3426 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11207...  Training loss: 4.1742...  0.3394 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11208...  Training loss: 4.2079...  0.3416 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11209...  Training loss: 4.2527...  0.3450 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11210...  Training loss: 4.2452...  0.3386 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11211...  Training loss: 4.2064...  0.3427 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11212...  Training loss: 4.1330...  0.3401 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11213...  Training loss: 4.1561...  0.3405 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11214...  Training loss: 4.1389...  0.3423 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11215...  Training loss: 4.1067...  0.3420 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11216...  Training loss: 4.1774...  0.3414 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11217...  Training loss: 4.2074...  0.3406 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11218...  Training loss: 4.1795...  0.3384 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11219...  Training loss: 4.1750...  0.3408 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11220...  Training loss: 4.1623...  0.3433 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11221...  Training loss: 4.1862...  0.3403 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11222...  Training loss: 4.1409...  0.3422 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11223...  Training loss: 4.1434...  0.3410 sec/batch\n",
      "Epoch: 61/100...  Training Step: 11224...  Training loss: 4.1866...  0.3427 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11225...  Training loss: 4.1083...  0.3443 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11226...  Training loss: 3.9434...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11227...  Training loss: 3.9787...  0.3417 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11228...  Training loss: 4.0862...  0.3400 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11229...  Training loss: 4.0377...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11230...  Training loss: 4.0056...  0.3411 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11231...  Training loss: 4.1406...  0.3409 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11232...  Training loss: 4.2119...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11233...  Training loss: 4.1646...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11234...  Training loss: 4.2344...  0.3407 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11235...  Training loss: 4.1739...  0.3432 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11236...  Training loss: 4.1547...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11237...  Training loss: 4.1443...  0.3409 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11238...  Training loss: 4.1791...  0.3423 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11239...  Training loss: 4.1875...  0.3404 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11240...  Training loss: 4.1510...  0.3391 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11241...  Training loss: 4.2426...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11242...  Training loss: 4.1725...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11243...  Training loss: 4.2186...  0.3420 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11244...  Training loss: 4.2270...  0.3435 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11245...  Training loss: 4.1654...  0.3382 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11246...  Training loss: 4.0471...  0.3401 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11247...  Training loss: 4.0872...  0.3420 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11248...  Training loss: 4.1025...  0.3410 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11249...  Training loss: 4.1825...  0.3407 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11250...  Training loss: 4.1557...  0.3408 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11251...  Training loss: 4.2018...  0.3392 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11252...  Training loss: 4.1769...  0.3392 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11253...  Training loss: 4.1896...  0.3425 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11254...  Training loss: 4.2428...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11255...  Training loss: 4.2078...  0.3423 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11256...  Training loss: 4.1633...  0.3392 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11257...  Training loss: 4.0209...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11258...  Training loss: 4.0869...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11259...  Training loss: 4.1751...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11260...  Training loss: 4.2504...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11261...  Training loss: 4.2990...  0.3397 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11262...  Training loss: 4.2681...  0.3395 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11263...  Training loss: 4.2169...  0.3413 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11264...  Training loss: 4.2452...  0.3439 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11265...  Training loss: 4.2481...  0.3425 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11266...  Training loss: 4.2921...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11267...  Training loss: 4.3205...  0.3488 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11268...  Training loss: 4.2579...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100...  Training Step: 11269...  Training loss: 4.2667...  0.3424 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11270...  Training loss: 4.2030...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11271...  Training loss: 4.2571...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11272...  Training loss: 4.1834...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11273...  Training loss: 4.2112...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11274...  Training loss: 4.2617...  0.3410 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11275...  Training loss: 4.2541...  0.3398 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11276...  Training loss: 4.2582...  0.3398 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11277...  Training loss: 4.2480...  0.3420 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11278...  Training loss: 4.2760...  0.3411 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11279...  Training loss: 4.2622...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11280...  Training loss: 4.2538...  0.3404 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11281...  Training loss: 4.1732...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11282...  Training loss: 4.2205...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11283...  Training loss: 4.1030...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11284...  Training loss: 4.1112...  0.3410 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11285...  Training loss: 4.1666...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11286...  Training loss: 4.2093...  0.3400 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11287...  Training loss: 4.2071...  0.3410 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11288...  Training loss: 4.2815...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11289...  Training loss: 4.1804...  0.3417 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11290...  Training loss: 4.1648...  0.3400 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11291...  Training loss: 4.1641...  0.3433 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11292...  Training loss: 4.0863...  0.3444 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11293...  Training loss: 4.2038...  0.3440 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11294...  Training loss: 4.2155...  0.3440 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11295...  Training loss: 4.1857...  0.3416 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11296...  Training loss: 4.1119...  0.3411 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11297...  Training loss: 4.1651...  0.3422 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11298...  Training loss: 4.1458...  0.3436 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11299...  Training loss: 4.1653...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11300...  Training loss: 4.1614...  0.3385 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11301...  Training loss: 4.1717...  0.3410 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11302...  Training loss: 4.1016...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11303...  Training loss: 4.1563...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11304...  Training loss: 4.1010...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11305...  Training loss: 4.1944...  0.3398 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11306...  Training loss: 4.1878...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11307...  Training loss: 4.1529...  0.3423 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11308...  Training loss: 4.1414...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11309...  Training loss: 4.1635...  0.3422 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11310...  Training loss: 4.1971...  0.3406 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11311...  Training loss: 4.1004...  0.3449 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11312...  Training loss: 4.0590...  0.3401 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11313...  Training loss: 4.1630...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11314...  Training loss: 4.2332...  0.3408 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11315...  Training loss: 4.1576...  0.3401 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11316...  Training loss: 4.1045...  0.3398 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11317...  Training loss: 4.1380...  0.3429 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11318...  Training loss: 4.1507...  0.3398 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11319...  Training loss: 4.2023...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11320...  Training loss: 4.1794...  0.3408 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11321...  Training loss: 4.1258...  0.3442 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11322...  Training loss: 4.1068...  0.3387 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11323...  Training loss: 4.1409...  0.3432 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11324...  Training loss: 4.1541...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11325...  Training loss: 4.2121...  0.3390 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11326...  Training loss: 4.1463...  0.3421 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11327...  Training loss: 4.1774...  0.3404 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11328...  Training loss: 4.0985...  0.3420 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11329...  Training loss: 4.2043...  0.3387 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11330...  Training loss: 4.1983...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11331...  Training loss: 4.1557...  0.3400 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11332...  Training loss: 4.1370...  0.3413 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11333...  Training loss: 4.1294...  0.3428 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11334...  Training loss: 4.1613...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11335...  Training loss: 4.1852...  0.3441 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11336...  Training loss: 4.1461...  0.3443 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11337...  Training loss: 4.1343...  0.3446 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11338...  Training loss: 4.0540...  0.3435 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11339...  Training loss: 4.1289...  0.3425 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11340...  Training loss: 4.0786...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11341...  Training loss: 4.1478...  0.3409 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11342...  Training loss: 4.0913...  0.3379 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11343...  Training loss: 4.1636...  0.3446 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11344...  Training loss: 4.1544...  0.3434 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11345...  Training loss: 4.3261...  0.3422 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11346...  Training loss: 4.2713...  0.3448 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11347...  Training loss: 4.2700...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11348...  Training loss: 4.1672...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11349...  Training loss: 4.2033...  0.3417 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11350...  Training loss: 4.1887...  0.3430 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11351...  Training loss: 4.2037...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11352...  Training loss: 4.2125...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11353...  Training loss: 4.2648...  0.3422 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11354...  Training loss: 4.2480...  0.3424 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11355...  Training loss: 4.2527...  0.3404 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11356...  Training loss: 4.1906...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11357...  Training loss: 4.2243...  0.3417 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11358...  Training loss: 4.1488...  0.3430 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11359...  Training loss: 4.1944...  0.3437 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11360...  Training loss: 4.1370...  0.3421 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11361...  Training loss: 4.2042...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11362...  Training loss: 4.1713...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11363...  Training loss: 4.2710...  0.3415 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11364...  Training loss: 4.1494...  0.3401 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62/100...  Training Step: 11365...  Training loss: 4.2670...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11366...  Training loss: 4.2182...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11367...  Training loss: 4.1871...  0.3425 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11368...  Training loss: 4.1389...  0.3442 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11369...  Training loss: 4.1732...  0.3417 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11370...  Training loss: 4.1655...  0.3434 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11371...  Training loss: 4.1571...  0.3430 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11372...  Training loss: 4.1909...  0.3402 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11373...  Training loss: 4.1977...  0.3437 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11374...  Training loss: 4.1431...  0.3434 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11375...  Training loss: 4.1917...  0.3407 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11376...  Training loss: 4.1566...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11377...  Training loss: 4.1852...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11378...  Training loss: 4.1832...  0.3425 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11379...  Training loss: 4.2424...  0.3430 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11380...  Training loss: 4.2017...  0.3416 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11381...  Training loss: 4.2265...  0.3403 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11382...  Training loss: 4.2624...  0.3424 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11383...  Training loss: 4.1267...  0.3399 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11384...  Training loss: 4.1894...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11385...  Training loss: 4.1924...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11386...  Training loss: 4.1932...  0.3446 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11387...  Training loss: 4.1896...  0.3424 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11388...  Training loss: 4.1498...  0.3400 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11389...  Training loss: 4.2012...  0.3430 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11390...  Training loss: 4.1621...  0.3391 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11391...  Training loss: 4.1508...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11392...  Training loss: 4.1725...  0.3416 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11393...  Training loss: 4.2351...  0.3392 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11394...  Training loss: 4.2226...  0.3405 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11395...  Training loss: 4.1952...  0.3402 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11396...  Training loss: 4.1560...  0.3401 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11397...  Training loss: 4.1263...  0.3412 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11398...  Training loss: 4.1458...  0.3411 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11399...  Training loss: 4.0875...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11400...  Training loss: 4.1546...  0.3414 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11401...  Training loss: 4.1917...  0.3409 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11402...  Training loss: 4.1727...  0.3419 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11403...  Training loss: 4.1673...  0.3426 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11404...  Training loss: 4.1576...  0.3424 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11405...  Training loss: 4.1705...  0.3418 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11406...  Training loss: 4.1461...  0.3416 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11407...  Training loss: 4.1482...  0.3423 sec/batch\n",
      "Epoch: 62/100...  Training Step: 11408...  Training loss: 4.2054...  0.3413 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11409...  Training loss: 4.1304...  0.3450 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11410...  Training loss: 3.9487...  0.3414 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11411...  Training loss: 3.9352...  0.3390 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11412...  Training loss: 4.0428...  0.3434 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11413...  Training loss: 4.0034...  0.3428 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11414...  Training loss: 3.9817...  0.3374 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11415...  Training loss: 4.1203...  0.3409 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11416...  Training loss: 4.1442...  0.3412 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11417...  Training loss: 4.1631...  0.3395 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11418...  Training loss: 4.2343...  0.3431 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11419...  Training loss: 4.1311...  0.3428 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11420...  Training loss: 4.1152...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11421...  Training loss: 4.0843...  0.3396 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11422...  Training loss: 4.1350...  0.3416 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11423...  Training loss: 4.1947...  0.3410 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11424...  Training loss: 4.1773...  0.3411 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11425...  Training loss: 4.2051...  0.3412 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11426...  Training loss: 4.1129...  0.3415 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11427...  Training loss: 4.1936...  0.3411 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11428...  Training loss: 4.1835...  0.3413 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11429...  Training loss: 4.1274...  0.3392 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11430...  Training loss: 4.0576...  0.3398 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11431...  Training loss: 4.0879...  0.3419 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11432...  Training loss: 4.0830...  0.3425 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11433...  Training loss: 4.2031...  0.3409 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11434...  Training loss: 4.1629...  0.3397 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11435...  Training loss: 4.1662...  0.3440 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11436...  Training loss: 4.1376...  0.3440 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11437...  Training loss: 4.1800...  0.3436 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11438...  Training loss: 4.2430...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11439...  Training loss: 4.2242...  0.3409 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11440...  Training loss: 4.1812...  0.3396 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11441...  Training loss: 4.0271...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11442...  Training loss: 4.0735...  0.3438 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11443...  Training loss: 4.1560...  0.3432 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11444...  Training loss: 4.2386...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11445...  Training loss: 4.2571...  0.3438 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11446...  Training loss: 4.2292...  0.3445 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11447...  Training loss: 4.2324...  0.3452 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11448...  Training loss: 4.2253...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11449...  Training loss: 4.2590...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11450...  Training loss: 4.2744...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11451...  Training loss: 4.2946...  0.3451 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11452...  Training loss: 4.2743...  0.3453 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11453...  Training loss: 4.2411...  0.3413 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11454...  Training loss: 4.1931...  0.3454 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11455...  Training loss: 4.2514...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11456...  Training loss: 4.1624...  0.3455 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11457...  Training loss: 4.1986...  0.3444 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11458...  Training loss: 4.2447...  0.3431 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11459...  Training loss: 4.2453...  0.3441 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11460...  Training loss: 4.2620...  0.3417 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100...  Training Step: 11461...  Training loss: 4.2527...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11462...  Training loss: 4.2492...  0.3415 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11463...  Training loss: 4.2482...  0.3419 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11464...  Training loss: 4.2480...  0.3412 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11465...  Training loss: 4.1721...  0.3414 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11466...  Training loss: 4.2060...  0.3413 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11467...  Training loss: 4.1144...  0.3449 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11468...  Training loss: 4.1098...  0.3450 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11469...  Training loss: 4.1258...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11470...  Training loss: 4.1877...  0.3410 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11471...  Training loss: 4.1624...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11472...  Training loss: 4.2417...  0.3423 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11473...  Training loss: 4.1530...  0.3444 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11474...  Training loss: 4.1838...  0.3463 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11475...  Training loss: 4.1797...  0.3447 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11476...  Training loss: 4.1055...  0.3456 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11477...  Training loss: 4.2093...  0.3483 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11478...  Training loss: 4.2069...  0.3457 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11479...  Training loss: 4.1673...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11480...  Training loss: 4.0788...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11481...  Training loss: 4.1416...  0.3461 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11482...  Training loss: 4.1348...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11483...  Training loss: 4.1448...  0.3426 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11484...  Training loss: 4.1652...  0.3408 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11485...  Training loss: 4.1778...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11486...  Training loss: 4.1459...  0.3468 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11487...  Training loss: 4.1666...  0.3439 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11488...  Training loss: 4.1115...  0.3450 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11489...  Training loss: 4.1701...  0.3438 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11490...  Training loss: 4.1628...  0.3405 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11491...  Training loss: 4.1510...  0.3463 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11492...  Training loss: 4.1550...  0.3427 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11493...  Training loss: 4.1591...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11494...  Training loss: 4.1960...  0.3431 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11495...  Training loss: 4.1023...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11496...  Training loss: 4.0744...  0.3444 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11497...  Training loss: 4.1694...  0.3467 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11498...  Training loss: 4.2420...  0.3442 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11499...  Training loss: 4.1558...  0.3421 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11500...  Training loss: 4.0854...  0.3423 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11501...  Training loss: 4.1413...  0.3427 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11502...  Training loss: 4.1212...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11503...  Training loss: 4.2005...  0.3397 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11504...  Training loss: 4.1903...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11505...  Training loss: 4.1046...  0.3408 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11506...  Training loss: 4.0676...  0.3410 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11507...  Training loss: 4.0741...  0.3454 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11508...  Training loss: 4.1306...  0.3420 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11509...  Training loss: 4.1514...  0.3415 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11510...  Training loss: 4.1048...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11511...  Training loss: 4.1827...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11512...  Training loss: 4.0855...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11513...  Training loss: 4.1714...  0.3419 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11514...  Training loss: 4.1632...  0.3403 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11515...  Training loss: 4.0787...  0.3445 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11516...  Training loss: 4.1090...  0.3435 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11517...  Training loss: 4.0869...  0.3424 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11518...  Training loss: 4.0874...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11519...  Training loss: 4.1314...  0.3452 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11520...  Training loss: 4.0964...  0.3431 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11521...  Training loss: 4.1097...  0.3417 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11522...  Training loss: 4.0603...  0.3441 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11523...  Training loss: 4.1604...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11524...  Training loss: 4.0724...  0.3429 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11525...  Training loss: 4.1172...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11526...  Training loss: 4.0618...  0.3457 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11527...  Training loss: 4.1444...  0.3451 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11528...  Training loss: 4.1221...  0.3454 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11529...  Training loss: 4.2502...  0.3447 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11530...  Training loss: 4.2293...  0.3460 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11531...  Training loss: 4.2171...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11532...  Training loss: 4.1353...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11533...  Training loss: 4.1601...  0.3424 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11534...  Training loss: 4.1901...  0.3417 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11535...  Training loss: 4.1947...  0.3507 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11536...  Training loss: 4.1828...  0.3464 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11537...  Training loss: 4.2527...  0.3450 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11538...  Training loss: 4.2420...  0.3425 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11539...  Training loss: 4.2563...  0.3456 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11540...  Training loss: 4.1915...  0.3405 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11541...  Training loss: 4.2272...  0.3418 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11542...  Training loss: 4.1642...  0.3440 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11543...  Training loss: 4.1784...  0.3411 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11544...  Training loss: 4.1228...  0.3396 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11545...  Training loss: 4.1507...  0.3445 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11546...  Training loss: 4.1677...  0.3419 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11547...  Training loss: 4.2298...  0.3417 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11548...  Training loss: 4.1491...  0.3431 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11549...  Training loss: 4.2216...  0.3449 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11550...  Training loss: 4.2083...  0.3430 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11551...  Training loss: 4.1457...  0.3441 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11552...  Training loss: 4.1409...  0.3414 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11553...  Training loss: 4.1702...  0.3424 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11554...  Training loss: 4.1871...  0.3452 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11555...  Training loss: 4.1686...  0.3443 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11556...  Training loss: 4.1865...  0.3450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63/100...  Training Step: 11557...  Training loss: 4.2040...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11558...  Training loss: 4.1142...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11559...  Training loss: 4.1577...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11560...  Training loss: 4.1178...  0.3448 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11561...  Training loss: 4.1779...  0.3427 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11562...  Training loss: 4.1608...  0.3461 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11563...  Training loss: 4.2031...  0.3434 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11564...  Training loss: 4.1930...  0.3418 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11565...  Training loss: 4.1883...  0.3440 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11566...  Training loss: 4.2311...  0.3449 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11567...  Training loss: 4.0923...  0.3450 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11568...  Training loss: 4.1638...  0.3417 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11569...  Training loss: 4.1574...  0.3415 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11570...  Training loss: 4.1879...  0.3419 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11571...  Training loss: 4.1472...  0.3444 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11572...  Training loss: 4.1208...  0.3440 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11573...  Training loss: 4.1819...  0.3438 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11574...  Training loss: 4.1104...  0.3463 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11575...  Training loss: 4.1557...  0.3438 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11576...  Training loss: 4.1702...  0.3433 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11577...  Training loss: 4.2198...  0.3441 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11578...  Training loss: 4.2269...  0.3437 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11579...  Training loss: 4.1624...  0.3444 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11580...  Training loss: 4.1300...  0.3406 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11581...  Training loss: 4.1392...  0.3428 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11582...  Training loss: 4.1346...  0.3426 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11583...  Training loss: 4.0888...  0.3430 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11584...  Training loss: 4.1677...  0.3448 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11585...  Training loss: 4.1776...  0.3442 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11586...  Training loss: 4.1330...  0.3453 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11587...  Training loss: 4.1655...  0.3448 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11588...  Training loss: 4.1290...  0.3446 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11589...  Training loss: 4.1690...  0.3442 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11590...  Training loss: 4.1574...  0.3417 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11591...  Training loss: 4.1155...  0.3434 sec/batch\n",
      "Epoch: 63/100...  Training Step: 11592...  Training loss: 4.1637...  0.3441 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11593...  Training loss: 4.1031...  0.3424 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11594...  Training loss: 3.9287...  0.3395 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11595...  Training loss: 3.9359...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11596...  Training loss: 4.0168...  0.3390 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11597...  Training loss: 3.9780...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11598...  Training loss: 3.9946...  0.3423 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11599...  Training loss: 4.1283...  0.3416 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11600...  Training loss: 4.1426...  0.3413 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11601...  Training loss: 4.1496...  0.3422 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11602...  Training loss: 4.1796...  0.3412 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11603...  Training loss: 4.1174...  0.3415 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11604...  Training loss: 4.0676...  0.3421 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11605...  Training loss: 4.0902...  0.3403 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11606...  Training loss: 4.1253...  0.3393 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11607...  Training loss: 4.1671...  0.3405 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11608...  Training loss: 4.1391...  0.3391 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11609...  Training loss: 4.2026...  0.3421 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11610...  Training loss: 4.1216...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11611...  Training loss: 4.1590...  0.3420 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11612...  Training loss: 4.1883...  0.3421 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11613...  Training loss: 4.1148...  0.3437 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11614...  Training loss: 4.0470...  0.3426 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11615...  Training loss: 4.0640...  0.3442 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11616...  Training loss: 4.0772...  0.3435 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11617...  Training loss: 4.1539...  0.3389 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11618...  Training loss: 4.1563...  0.3391 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11619...  Training loss: 4.1460...  0.3443 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11620...  Training loss: 4.1345...  0.3425 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11621...  Training loss: 4.1471...  0.3407 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11622...  Training loss: 4.2261...  0.3430 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11623...  Training loss: 4.2127...  0.3400 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11624...  Training loss: 4.1728...  0.3401 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11625...  Training loss: 4.0325...  0.3397 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11626...  Training loss: 4.0931...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11627...  Training loss: 4.1724...  0.3412 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11628...  Training loss: 4.2313...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11629...  Training loss: 4.2457...  0.3448 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11630...  Training loss: 4.2012...  0.3443 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11631...  Training loss: 4.2071...  0.3401 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11632...  Training loss: 4.2072...  0.3398 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11633...  Training loss: 4.2291...  0.3396 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11634...  Training loss: 4.2839...  0.3408 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11635...  Training loss: 4.2917...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11636...  Training loss: 4.2614...  0.3430 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11637...  Training loss: 4.2231...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11638...  Training loss: 4.2094...  0.3447 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11639...  Training loss: 4.2181...  0.3401 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11640...  Training loss: 4.1482...  0.3427 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11641...  Training loss: 4.1795...  0.3441 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11642...  Training loss: 4.2166...  0.3438 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11643...  Training loss: 4.2577...  0.3424 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11644...  Training loss: 4.2285...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11645...  Training loss: 4.2291...  0.3411 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11646...  Training loss: 4.2319...  0.3448 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11647...  Training loss: 4.2440...  0.3394 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11648...  Training loss: 4.2047...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11649...  Training loss: 4.1342...  0.3411 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11650...  Training loss: 4.2042...  0.3403 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11651...  Training loss: 4.0812...  0.3407 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11652...  Training loss: 4.0982...  0.3406 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/100...  Training Step: 11653...  Training loss: 4.1432...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11654...  Training loss: 4.1514...  0.3396 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11655...  Training loss: 4.1344...  0.3425 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11656...  Training loss: 4.2155...  0.3402 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11657...  Training loss: 4.1342...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11658...  Training loss: 4.1326...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11659...  Training loss: 4.1658...  0.3424 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11660...  Training loss: 4.1055...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11661...  Training loss: 4.1881...  0.3408 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11662...  Training loss: 4.2226...  0.3414 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11663...  Training loss: 4.1381...  0.3402 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11664...  Training loss: 4.0556...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11665...  Training loss: 4.0994...  0.3431 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11666...  Training loss: 4.1071...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11667...  Training loss: 4.1254...  0.3405 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11668...  Training loss: 4.1704...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11669...  Training loss: 4.1605...  0.3421 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11670...  Training loss: 4.1284...  0.3389 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11671...  Training loss: 4.1604...  0.3393 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11672...  Training loss: 4.0815...  0.3416 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11673...  Training loss: 4.1521...  0.3393 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11674...  Training loss: 4.1229...  0.3407 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11675...  Training loss: 4.1241...  0.3400 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11676...  Training loss: 4.1242...  0.3415 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11677...  Training loss: 4.1263...  0.3403 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11678...  Training loss: 4.1836...  0.3405 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11679...  Training loss: 4.0966...  0.3420 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11680...  Training loss: 4.0588...  0.3426 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11681...  Training loss: 4.1507...  0.3445 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11682...  Training loss: 4.2040...  0.3413 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11683...  Training loss: 4.1244...  0.3408 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11684...  Training loss: 4.0800...  0.3398 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11685...  Training loss: 4.1040...  0.3438 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11686...  Training loss: 4.1296...  0.3441 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11687...  Training loss: 4.1575...  0.3442 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11688...  Training loss: 4.1860...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11689...  Training loss: 4.0866...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11690...  Training loss: 4.0957...  0.3436 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11691...  Training loss: 4.0780...  0.3395 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11692...  Training loss: 4.1074...  0.3428 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11693...  Training loss: 4.1231...  0.3422 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11694...  Training loss: 4.0777...  0.3425 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11695...  Training loss: 4.1630...  0.3399 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11696...  Training loss: 4.0617...  0.3397 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11697...  Training loss: 4.1760...  0.3394 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11698...  Training loss: 4.1829...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11699...  Training loss: 4.1145...  0.3452 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11700...  Training loss: 4.1023...  0.3420 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11701...  Training loss: 4.0512...  0.3420 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11702...  Training loss: 4.0838...  0.3430 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11703...  Training loss: 4.1486...  0.3407 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11704...  Training loss: 4.0770...  0.3415 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11705...  Training loss: 4.0989...  0.3434 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11706...  Training loss: 4.0284...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11707...  Training loss: 4.1355...  0.3436 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11708...  Training loss: 4.0771...  0.3430 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11709...  Training loss: 4.1214...  0.3427 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11710...  Training loss: 4.0392...  0.3426 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11711...  Training loss: 4.1389...  0.3437 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11712...  Training loss: 4.1310...  0.3438 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11713...  Training loss: 4.2809...  0.3411 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11714...  Training loss: 4.2754...  0.3413 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11715...  Training loss: 4.2168...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11716...  Training loss: 4.1240...  0.3402 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11717...  Training loss: 4.1716...  0.3408 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11718...  Training loss: 4.1779...  0.3435 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11719...  Training loss: 4.1499...  0.3413 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11720...  Training loss: 4.1881...  0.3436 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11721...  Training loss: 4.2487...  0.3443 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11722...  Training loss: 4.2370...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11723...  Training loss: 4.2802...  0.3390 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11724...  Training loss: 4.1856...  0.3441 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11725...  Training loss: 4.2119...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11726...  Training loss: 4.1588...  0.3401 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11727...  Training loss: 4.1849...  0.3428 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11728...  Training loss: 4.1438...  0.3431 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11729...  Training loss: 4.1929...  0.3444 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11730...  Training loss: 4.1553...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11731...  Training loss: 4.2113...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11732...  Training loss: 4.1209...  0.3435 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11733...  Training loss: 4.1788...  0.3415 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11734...  Training loss: 4.2074...  0.3432 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11735...  Training loss: 4.1741...  0.3395 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11736...  Training loss: 4.1186...  0.3398 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11737...  Training loss: 4.1649...  0.3392 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11738...  Training loss: 4.1572...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11739...  Training loss: 4.1341...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11740...  Training loss: 4.1407...  0.3398 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11741...  Training loss: 4.1979...  0.3424 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11742...  Training loss: 4.0998...  0.3393 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11743...  Training loss: 4.1263...  0.3418 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11744...  Training loss: 4.1064...  0.3409 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11745...  Training loss: 4.1348...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11746...  Training loss: 4.1547...  0.3421 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11747...  Training loss: 4.1933...  0.3404 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11748...  Training loss: 4.1897...  0.3421 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/100...  Training Step: 11749...  Training loss: 4.1969...  0.3432 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11750...  Training loss: 4.2467...  0.3405 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11751...  Training loss: 4.0966...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11752...  Training loss: 4.1203...  0.3419 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11753...  Training loss: 4.1278...  0.3412 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11754...  Training loss: 4.1782...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11755...  Training loss: 4.1047...  0.3416 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11756...  Training loss: 4.0934...  0.3437 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11757...  Training loss: 4.1627...  0.3428 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11758...  Training loss: 4.1226...  0.3420 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11759...  Training loss: 4.1353...  0.3426 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11760...  Training loss: 4.1559...  0.3397 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11761...  Training loss: 4.2124...  0.3416 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11762...  Training loss: 4.2200...  0.3414 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11763...  Training loss: 4.1954...  0.3405 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11764...  Training loss: 4.1570...  0.3434 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11765...  Training loss: 4.1131...  0.3410 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11766...  Training loss: 4.1516...  0.3447 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11767...  Training loss: 4.0861...  0.3414 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11768...  Training loss: 4.1807...  0.3414 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11769...  Training loss: 4.1891...  0.3393 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11770...  Training loss: 4.1835...  0.3397 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11771...  Training loss: 4.1969...  0.3406 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11772...  Training loss: 4.1466...  0.3431 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11773...  Training loss: 4.1661...  0.3414 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11774...  Training loss: 4.1236...  0.3424 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11775...  Training loss: 4.1417...  0.3413 sec/batch\n",
      "Epoch: 64/100...  Training Step: 11776...  Training loss: 4.1659...  0.3392 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11777...  Training loss: 4.1149...  0.3445 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11778...  Training loss: 3.9130...  0.3434 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11779...  Training loss: 3.9551...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11780...  Training loss: 4.0055...  0.3406 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11781...  Training loss: 3.9418...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11782...  Training loss: 3.9511...  0.3431 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11783...  Training loss: 4.0800...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11784...  Training loss: 4.1360...  0.3419 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11785...  Training loss: 4.1103...  0.3424 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11786...  Training loss: 4.1797...  0.3385 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11787...  Training loss: 4.1510...  0.3407 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11788...  Training loss: 4.0849...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11789...  Training loss: 4.0740...  0.3427 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11790...  Training loss: 4.1296...  0.3413 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11791...  Training loss: 4.1534...  0.3414 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11792...  Training loss: 4.1387...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11793...  Training loss: 4.2248...  0.3389 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11794...  Training loss: 4.1148...  0.3391 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11795...  Training loss: 4.1940...  0.3396 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11796...  Training loss: 4.1643...  0.3396 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11797...  Training loss: 4.1183...  0.3419 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11798...  Training loss: 4.0278...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11799...  Training loss: 4.0613...  0.3422 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11800...  Training loss: 4.1012...  0.3440 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11801...  Training loss: 4.1816...  0.3444 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11802...  Training loss: 4.1878...  0.3408 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11803...  Training loss: 4.1673...  0.3403 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11804...  Training loss: 4.1439...  0.3412 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11805...  Training loss: 4.1643...  0.3438 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11806...  Training loss: 4.2449...  0.3402 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11807...  Training loss: 4.2033...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11808...  Training loss: 4.1451...  0.3393 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11809...  Training loss: 4.0332...  0.3413 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11810...  Training loss: 4.0872...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11811...  Training loss: 4.1666...  0.3446 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11812...  Training loss: 4.2262...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11813...  Training loss: 4.2582...  0.3399 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11814...  Training loss: 4.2300...  0.3387 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11815...  Training loss: 4.2226...  0.3388 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11816...  Training loss: 4.2154...  0.3423 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11817...  Training loss: 4.2284...  0.3403 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11818...  Training loss: 4.2827...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11819...  Training loss: 4.3277...  0.3397 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11820...  Training loss: 4.2453...  0.3408 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11821...  Training loss: 4.2249...  0.3417 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11822...  Training loss: 4.2024...  0.3391 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11823...  Training loss: 4.2267...  0.3427 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11824...  Training loss: 4.1642...  0.3412 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11825...  Training loss: 4.1873...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11826...  Training loss: 4.2022...  0.3419 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11827...  Training loss: 4.2596...  0.3408 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11828...  Training loss: 4.2605...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11829...  Training loss: 4.2489...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11830...  Training loss: 4.2390...  0.3413 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11831...  Training loss: 4.2399...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11832...  Training loss: 4.2228...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11833...  Training loss: 4.1453...  0.3403 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11834...  Training loss: 4.1830...  0.3439 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11835...  Training loss: 4.1027...  0.3438 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11836...  Training loss: 4.1101...  0.3429 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11837...  Training loss: 4.1410...  0.3415 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11838...  Training loss: 4.1789...  0.3404 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11839...  Training loss: 4.1501...  0.3398 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11840...  Training loss: 4.2272...  0.3426 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11841...  Training loss: 4.1274...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11842...  Training loss: 4.1539...  0.3412 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11843...  Training loss: 4.1618...  0.3432 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11844...  Training loss: 4.1312...  0.3392 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/100...  Training Step: 11845...  Training loss: 4.1985...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11846...  Training loss: 4.2074...  0.3430 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11847...  Training loss: 4.1853...  0.3406 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11848...  Training loss: 4.0714...  0.3390 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11849...  Training loss: 4.1125...  0.3416 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11850...  Training loss: 4.1225...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11851...  Training loss: 4.1120...  0.3428 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11852...  Training loss: 4.1184...  0.3428 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11853...  Training loss: 4.1616...  0.3419 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11854...  Training loss: 4.1060...  0.3427 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11855...  Training loss: 4.1532...  0.3418 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11856...  Training loss: 4.0817...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11857...  Training loss: 4.1488...  0.3395 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11858...  Training loss: 4.1153...  0.3431 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11859...  Training loss: 4.1466...  0.3406 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11860...  Training loss: 4.0985...  0.3442 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11861...  Training loss: 4.1235...  0.3406 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11862...  Training loss: 4.1563...  0.3423 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11863...  Training loss: 4.0890...  0.3413 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11864...  Training loss: 4.0681...  0.3422 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11865...  Training loss: 4.1344...  0.3403 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11866...  Training loss: 4.2388...  0.3448 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11867...  Training loss: 4.1154...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11868...  Training loss: 4.0446...  0.3429 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11869...  Training loss: 4.1149...  0.3405 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11870...  Training loss: 4.1122...  0.3420 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11871...  Training loss: 4.1667...  0.3405 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11872...  Training loss: 4.2063...  0.3389 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11873...  Training loss: 4.1340...  0.3446 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11874...  Training loss: 4.1210...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11875...  Training loss: 4.1197...  0.3420 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11876...  Training loss: 4.1291...  0.3397 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11877...  Training loss: 4.1448...  0.3438 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11878...  Training loss: 4.1156...  0.3423 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11879...  Training loss: 4.1414...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11880...  Training loss: 4.0307...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11881...  Training loss: 4.1179...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11882...  Training loss: 4.1485...  0.3415 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11883...  Training loss: 4.1049...  0.3433 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11884...  Training loss: 4.0719...  0.3435 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11885...  Training loss: 4.0549...  0.3405 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11886...  Training loss: 4.0574...  0.3440 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11887...  Training loss: 4.1051...  0.3417 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11888...  Training loss: 4.0646...  0.3440 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11889...  Training loss: 4.1015...  0.3417 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11890...  Training loss: 4.0260...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11891...  Training loss: 4.1205...  0.3434 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11892...  Training loss: 4.0841...  0.3429 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11893...  Training loss: 4.1479...  0.3430 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11894...  Training loss: 4.0528...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11895...  Training loss: 4.1082...  0.3404 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11896...  Training loss: 4.1006...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11897...  Training loss: 4.2426...  0.3402 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11898...  Training loss: 4.2162...  0.3438 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11899...  Training loss: 4.1922...  0.3429 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11900...  Training loss: 4.1075...  0.3409 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11901...  Training loss: 4.1618...  0.3441 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11902...  Training loss: 4.1230...  0.3418 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11903...  Training loss: 4.1152...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11904...  Training loss: 4.1358...  0.3408 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11905...  Training loss: 4.2020...  0.3420 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11906...  Training loss: 4.1706...  0.3399 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11907...  Training loss: 4.1801...  0.3420 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11908...  Training loss: 4.1391...  0.3414 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11909...  Training loss: 4.1593...  0.3400 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11910...  Training loss: 4.1123...  0.3405 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11911...  Training loss: 4.1563...  0.3422 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11912...  Training loss: 4.0989...  0.3401 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11913...  Training loss: 4.1214...  0.3427 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11914...  Training loss: 4.1332...  0.3418 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11915...  Training loss: 4.1616...  0.3396 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11916...  Training loss: 4.0951...  0.3421 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11917...  Training loss: 4.1636...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11918...  Training loss: 4.1535...  0.3439 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11919...  Training loss: 4.1349...  0.3442 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11920...  Training loss: 4.1117...  0.3417 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11921...  Training loss: 4.1740...  0.3415 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11922...  Training loss: 4.1469...  0.3437 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11923...  Training loss: 4.1508...  0.3431 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11924...  Training loss: 4.1674...  0.3421 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11925...  Training loss: 4.1770...  0.3430 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11926...  Training loss: 4.0977...  0.3400 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11927...  Training loss: 4.1030...  0.3427 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11928...  Training loss: 4.0854...  0.3440 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11929...  Training loss: 4.1147...  0.3405 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11930...  Training loss: 4.1107...  0.3424 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11931...  Training loss: 4.1657...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11932...  Training loss: 4.1496...  0.3411 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11933...  Training loss: 4.1493...  0.3421 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11934...  Training loss: 4.1759...  0.3419 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11935...  Training loss: 4.0556...  0.3407 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11936...  Training loss: 4.0868...  0.3424 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11937...  Training loss: 4.1222...  0.3424 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11938...  Training loss: 4.1441...  0.3399 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11939...  Training loss: 4.0954...  0.3414 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11940...  Training loss: 4.0790...  0.3438 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/100...  Training Step: 11941...  Training loss: 4.1610...  0.3395 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11942...  Training loss: 4.1275...  0.3424 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11943...  Training loss: 4.0883...  0.3425 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11944...  Training loss: 4.1190...  0.3400 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11945...  Training loss: 4.1614...  0.3396 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11946...  Training loss: 4.1943...  0.3425 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11947...  Training loss: 4.1345...  0.3395 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11948...  Training loss: 4.0876...  0.3447 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11949...  Training loss: 4.0688...  0.3410 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11950...  Training loss: 4.0621...  0.3432 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11951...  Training loss: 4.0523...  0.3415 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11952...  Training loss: 4.1326...  0.3406 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11953...  Training loss: 4.1840...  0.3417 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11954...  Training loss: 4.1261...  0.3407 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11955...  Training loss: 4.1438...  0.3396 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11956...  Training loss: 4.1021...  0.3420 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11957...  Training loss: 4.1384...  0.3435 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11958...  Training loss: 4.1007...  0.3421 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11959...  Training loss: 4.0948...  0.3423 sec/batch\n",
      "Epoch: 65/100...  Training Step: 11960...  Training loss: 4.1044...  0.3460 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11961...  Training loss: 4.1167...  0.3381 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11962...  Training loss: 3.9086...  0.3383 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11963...  Training loss: 3.8581...  0.3455 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11964...  Training loss: 3.9787...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11965...  Training loss: 3.9482...  0.3381 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11966...  Training loss: 3.8854...  0.3399 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11967...  Training loss: 4.0454...  0.3424 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11968...  Training loss: 4.0984...  0.3431 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11969...  Training loss: 4.1225...  0.3392 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11970...  Training loss: 4.1397...  0.3405 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11971...  Training loss: 4.0636...  0.3427 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11972...  Training loss: 4.0655...  0.3380 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11973...  Training loss: 4.0615...  0.3387 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11974...  Training loss: 4.1024...  0.3417 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11975...  Training loss: 4.1431...  0.3405 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11976...  Training loss: 4.0971...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11977...  Training loss: 4.2095...  0.3436 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11978...  Training loss: 4.0648...  0.3398 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11979...  Training loss: 4.1420...  0.3437 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11980...  Training loss: 4.1500...  0.3446 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11981...  Training loss: 4.0825...  0.3421 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11982...  Training loss: 3.9969...  0.3421 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11983...  Training loss: 4.0277...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11984...  Training loss: 4.0663...  0.3395 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11985...  Training loss: 4.1325...  0.3399 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11986...  Training loss: 4.1184...  0.3413 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11987...  Training loss: 4.1329...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11988...  Training loss: 4.1120...  0.3415 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11989...  Training loss: 4.1372...  0.3413 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11990...  Training loss: 4.1976...  0.3432 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11991...  Training loss: 4.1595...  0.3412 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11992...  Training loss: 4.1211...  0.3391 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11993...  Training loss: 3.9986...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11994...  Training loss: 4.0171...  0.3414 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11995...  Training loss: 4.1167...  0.3434 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11996...  Training loss: 4.2111...  0.3406 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11997...  Training loss: 4.2064...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11998...  Training loss: 4.2017...  0.3403 sec/batch\n",
      "Epoch: 66/100...  Training Step: 11999...  Training loss: 4.2120...  0.3394 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12000...  Training loss: 4.1742...  0.3416 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12001...  Training loss: 4.1791...  0.3921 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12002...  Training loss: 4.2498...  0.3493 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12003...  Training loss: 4.2791...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12004...  Training loss: 4.2319...  0.3402 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12005...  Training loss: 4.2139...  0.3434 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12006...  Training loss: 4.1752...  0.3437 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12007...  Training loss: 4.2099...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12008...  Training loss: 4.1101...  0.3415 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12009...  Training loss: 4.1214...  0.3450 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12010...  Training loss: 4.1573...  0.3390 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12011...  Training loss: 4.1902...  0.3421 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12012...  Training loss: 4.1949...  0.3411 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12013...  Training loss: 4.1856...  0.3417 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12014...  Training loss: 4.1860...  0.3386 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12015...  Training loss: 4.2063...  0.3400 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12016...  Training loss: 4.1894...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12017...  Training loss: 4.1006...  0.3430 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12018...  Training loss: 4.1527...  0.3411 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12019...  Training loss: 4.0649...  0.3430 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12020...  Training loss: 4.0592...  0.3400 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12021...  Training loss: 4.0877...  0.3402 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12022...  Training loss: 4.1460...  0.3429 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12023...  Training loss: 4.1121...  0.3416 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12024...  Training loss: 4.1647...  0.3407 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12025...  Training loss: 4.0821...  0.3433 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12026...  Training loss: 4.0913...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12027...  Training loss: 4.0847...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12028...  Training loss: 4.0412...  0.3425 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12029...  Training loss: 4.1316...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12030...  Training loss: 4.1557...  0.3441 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12031...  Training loss: 4.1329...  0.3416 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12032...  Training loss: 4.0381...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12033...  Training loss: 4.1001...  0.3418 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12034...  Training loss: 4.0982...  0.3404 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12035...  Training loss: 4.0966...  0.3417 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12036...  Training loss: 4.1162...  0.3408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100...  Training Step: 12037...  Training loss: 4.1302...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12038...  Training loss: 4.0418...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12039...  Training loss: 4.1439...  0.3437 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12040...  Training loss: 4.0715...  0.3412 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12041...  Training loss: 4.1493...  0.3391 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12042...  Training loss: 4.1006...  0.3444 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12043...  Training loss: 4.1223...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12044...  Training loss: 4.0852...  0.3387 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12045...  Training loss: 4.0830...  0.3384 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12046...  Training loss: 4.1048...  0.3388 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12047...  Training loss: 4.0637...  0.3401 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12048...  Training loss: 4.0205...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12049...  Training loss: 4.1073...  0.3383 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12050...  Training loss: 4.1750...  0.3404 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12051...  Training loss: 4.0997...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12052...  Training loss: 4.0314...  0.3436 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12053...  Training loss: 4.0563...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12054...  Training loss: 4.0968...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12055...  Training loss: 4.1260...  0.3397 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12056...  Training loss: 4.1492...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12057...  Training loss: 4.0822...  0.3417 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12058...  Training loss: 4.0619...  0.3415 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12059...  Training loss: 4.1091...  0.3411 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12060...  Training loss: 4.1071...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12061...  Training loss: 4.1484...  0.3396 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12062...  Training loss: 4.1052...  0.3395 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12063...  Training loss: 4.1032...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12064...  Training loss: 4.0242...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12065...  Training loss: 4.1356...  0.3387 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12066...  Training loss: 4.1210...  0.3398 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12067...  Training loss: 4.0597...  0.3425 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12068...  Training loss: 4.0512...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12069...  Training loss: 4.0289...  0.3406 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12070...  Training loss: 4.0883...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12071...  Training loss: 4.1157...  0.3416 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12072...  Training loss: 4.0524...  0.3394 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12073...  Training loss: 4.0656...  0.3397 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12074...  Training loss: 4.0052...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12075...  Training loss: 4.0807...  0.3398 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12076...  Training loss: 4.0568...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12077...  Training loss: 4.0757...  0.3413 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12078...  Training loss: 4.0131...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12079...  Training loss: 4.0937...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12080...  Training loss: 4.0827...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12081...  Training loss: 4.2375...  0.3418 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12082...  Training loss: 4.2067...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12083...  Training loss: 4.1850...  0.3387 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12084...  Training loss: 4.1015...  0.3402 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12085...  Training loss: 4.1098...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12086...  Training loss: 4.1379...  0.3398 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12087...  Training loss: 4.1529...  0.3436 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12088...  Training loss: 4.1373...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12089...  Training loss: 4.1911...  0.3385 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12090...  Training loss: 4.1892...  0.3430 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12091...  Training loss: 4.1886...  0.3417 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12092...  Training loss: 4.1135...  0.3406 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12093...  Training loss: 4.1665...  0.3442 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12094...  Training loss: 4.0807...  0.3401 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12095...  Training loss: 4.1581...  0.3400 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12096...  Training loss: 4.0671...  0.3389 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12097...  Training loss: 4.0970...  0.3384 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12098...  Training loss: 4.0908...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12099...  Training loss: 4.1381...  0.3394 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12100...  Training loss: 4.0627...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12101...  Training loss: 4.1396...  0.3437 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12102...  Training loss: 4.1332...  0.3422 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12103...  Training loss: 4.0939...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12104...  Training loss: 4.0334...  0.3402 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12105...  Training loss: 4.0958...  0.3406 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12106...  Training loss: 4.1195...  0.3424 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12107...  Training loss: 4.0833...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12108...  Training loss: 4.1213...  0.3408 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12109...  Training loss: 4.1534...  0.3451 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12110...  Training loss: 4.0735...  0.3398 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12111...  Training loss: 4.0943...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12112...  Training loss: 4.0548...  0.3404 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12113...  Training loss: 4.0787...  0.3405 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12114...  Training loss: 4.1299...  0.3393 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12115...  Training loss: 4.1337...  0.3390 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12116...  Training loss: 4.1443...  0.3405 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12117...  Training loss: 4.1436...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12118...  Training loss: 4.1832...  0.3426 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12119...  Training loss: 4.0518...  0.3411 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12120...  Training loss: 4.0954...  0.3444 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12121...  Training loss: 4.1211...  0.3429 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12122...  Training loss: 4.0808...  0.3430 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12123...  Training loss: 4.0460...  0.3409 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12124...  Training loss: 4.0481...  0.3429 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12125...  Training loss: 4.1450...  0.3419 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12126...  Training loss: 4.0722...  0.3416 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12127...  Training loss: 4.0630...  0.3395 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12128...  Training loss: 4.1091...  0.3414 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12129...  Training loss: 4.1536...  0.3410 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12130...  Training loss: 4.1578...  0.3413 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12131...  Training loss: 4.0996...  0.3411 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12132...  Training loss: 4.0607...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/100...  Training Step: 12133...  Training loss: 4.0310...  0.3390 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12134...  Training loss: 4.0400...  0.3422 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12135...  Training loss: 3.9901...  0.3443 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12136...  Training loss: 4.1028...  0.3401 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12137...  Training loss: 4.1198...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12138...  Training loss: 4.0763...  0.3424 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12139...  Training loss: 4.1259...  0.3393 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12140...  Training loss: 4.0856...  0.3420 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12141...  Training loss: 4.1363...  0.3389 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12142...  Training loss: 4.0905...  0.3423 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12143...  Training loss: 4.0703...  0.3432 sec/batch\n",
      "Epoch: 66/100...  Training Step: 12144...  Training loss: 4.1107...  0.3435 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12145...  Training loss: 4.0587...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12146...  Training loss: 3.9306...  0.3428 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12147...  Training loss: 3.9215...  0.3431 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12148...  Training loss: 4.0000...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12149...  Training loss: 3.9315...  0.3410 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12150...  Training loss: 3.9218...  0.3419 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12151...  Training loss: 4.0680...  0.3406 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12152...  Training loss: 4.0756...  0.3393 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12153...  Training loss: 4.0987...  0.3426 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12154...  Training loss: 4.0949...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12155...  Training loss: 4.0778...  0.3442 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12156...  Training loss: 4.0438...  0.3428 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12157...  Training loss: 4.0633...  0.3435 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12158...  Training loss: 4.0724...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12159...  Training loss: 4.0738...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12160...  Training loss: 4.0696...  0.3395 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12161...  Training loss: 4.1687...  0.3417 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12162...  Training loss: 4.0679...  0.3411 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12163...  Training loss: 4.1320...  0.3396 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12164...  Training loss: 4.0887...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12165...  Training loss: 4.0289...  0.3422 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12166...  Training loss: 3.9509...  0.3406 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12167...  Training loss: 4.0064...  0.3426 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12168...  Training loss: 4.0383...  0.3432 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12169...  Training loss: 4.1085...  0.3416 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12170...  Training loss: 4.1005...  0.3422 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12171...  Training loss: 4.0913...  0.3397 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12172...  Training loss: 4.1083...  0.3426 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12173...  Training loss: 4.1280...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12174...  Training loss: 4.1776...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12175...  Training loss: 4.1517...  0.3432 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12176...  Training loss: 4.1111...  0.3429 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12177...  Training loss: 3.9737...  0.3417 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12178...  Training loss: 4.0408...  0.3408 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12179...  Training loss: 4.1150...  0.3388 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12180...  Training loss: 4.1725...  0.3433 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12181...  Training loss: 4.1804...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12182...  Training loss: 4.1595...  0.3392 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12183...  Training loss: 4.1531...  0.3424 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12184...  Training loss: 4.1330...  0.3397 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12185...  Training loss: 4.1729...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12186...  Training loss: 4.1977...  0.3409 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12187...  Training loss: 4.2323...  0.3429 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12188...  Training loss: 4.1847...  0.3418 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12189...  Training loss: 4.1862...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12190...  Training loss: 4.1531...  0.3409 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12191...  Training loss: 4.1783...  0.3441 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12192...  Training loss: 4.1246...  0.3418 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12193...  Training loss: 4.1325...  0.3443 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12194...  Training loss: 4.1584...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12195...  Training loss: 4.1874...  0.3432 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12196...  Training loss: 4.1929...  0.3392 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12197...  Training loss: 4.1708...  0.3412 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12198...  Training loss: 4.1822...  0.3415 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12199...  Training loss: 4.1722...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12200...  Training loss: 4.1600...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12201...  Training loss: 4.0943...  0.3425 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12202...  Training loss: 4.1452...  0.3433 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12203...  Training loss: 4.0522...  0.3417 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12204...  Training loss: 4.0288...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12205...  Training loss: 4.0732...  0.3414 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12206...  Training loss: 4.1396...  0.3413 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12207...  Training loss: 4.0769...  0.3408 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12208...  Training loss: 4.1710...  0.3416 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12209...  Training loss: 4.0882...  0.3415 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12210...  Training loss: 4.0771...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12211...  Training loss: 4.0655...  0.3413 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12212...  Training loss: 4.0422...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12213...  Training loss: 4.1056...  0.3418 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12214...  Training loss: 4.1270...  0.3419 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12215...  Training loss: 4.0817...  0.3407 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12216...  Training loss: 4.0012...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12217...  Training loss: 4.0590...  0.3398 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12218...  Training loss: 4.0743...  0.3399 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12219...  Training loss: 4.0808...  0.3416 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12220...  Training loss: 4.0590...  0.3425 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12221...  Training loss: 4.1185...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12222...  Training loss: 4.0222...  0.3410 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12223...  Training loss: 4.1024...  0.3409 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12224...  Training loss: 4.0465...  0.3391 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12225...  Training loss: 4.1119...  0.3401 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12226...  Training loss: 4.1030...  0.3409 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12227...  Training loss: 4.0845...  0.3400 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12228...  Training loss: 4.0488...  0.3416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100...  Training Step: 12229...  Training loss: 4.0618...  0.3404 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12230...  Training loss: 4.0838...  0.3404 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12231...  Training loss: 4.0153...  0.3412 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12232...  Training loss: 3.9827...  0.3420 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12233...  Training loss: 4.0928...  0.3401 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12234...  Training loss: 4.1697...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12235...  Training loss: 4.1037...  0.3433 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12236...  Training loss: 4.0286...  0.3381 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12237...  Training loss: 4.0727...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12238...  Training loss: 4.0575...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12239...  Training loss: 4.1144...  0.3386 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12240...  Training loss: 4.1111...  0.3440 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12241...  Training loss: 4.0473...  0.3446 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12242...  Training loss: 4.0136...  0.3417 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12243...  Training loss: 4.0509...  0.3412 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12244...  Training loss: 4.0710...  0.3413 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12245...  Training loss: 4.1412...  0.3399 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12246...  Training loss: 4.0937...  0.3411 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12247...  Training loss: 4.0909...  0.3409 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12248...  Training loss: 3.9930...  0.3381 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12249...  Training loss: 4.1227...  0.3419 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12250...  Training loss: 4.0664...  0.3417 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12251...  Training loss: 4.0610...  0.3404 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12252...  Training loss: 4.0369...  0.3400 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12253...  Training loss: 4.0237...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12254...  Training loss: 4.0354...  0.3414 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12255...  Training loss: 4.0957...  0.3416 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12256...  Training loss: 4.0363...  0.3427 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12257...  Training loss: 4.0607...  0.3466 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12258...  Training loss: 3.9855...  0.3438 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12259...  Training loss: 4.0798...  0.3443 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12260...  Training loss: 4.0126...  0.3467 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12261...  Training loss: 4.0582...  0.3450 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12262...  Training loss: 3.9673...  0.3444 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12263...  Training loss: 4.0647...  0.3444 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12264...  Training loss: 4.0720...  0.3452 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12265...  Training loss: 4.2075...  0.3446 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12266...  Training loss: 4.1783...  0.3413 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12267...  Training loss: 4.1678...  0.3419 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12268...  Training loss: 4.0680...  0.3442 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12269...  Training loss: 4.0968...  0.3441 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12270...  Training loss: 4.1234...  0.3433 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12271...  Training loss: 4.1214...  0.3450 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12272...  Training loss: 4.1530...  0.3437 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12273...  Training loss: 4.1977...  0.3440 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12274...  Training loss: 4.1873...  0.3449 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12275...  Training loss: 4.1949...  0.3443 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12276...  Training loss: 4.1455...  0.3439 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12277...  Training loss: 4.1258...  0.3404 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12278...  Training loss: 4.0583...  0.3407 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12279...  Training loss: 4.1182...  0.3416 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12280...  Training loss: 4.0568...  0.3438 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12281...  Training loss: 4.1029...  0.3408 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12282...  Training loss: 4.0909...  0.3411 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12283...  Training loss: 4.1326...  0.3438 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12284...  Training loss: 4.0368...  0.3435 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12285...  Training loss: 4.1003...  0.3422 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12286...  Training loss: 4.1004...  0.3414 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12287...  Training loss: 4.0427...  0.3410 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12288...  Training loss: 4.0091...  0.3404 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12289...  Training loss: 4.0879...  0.3378 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12290...  Training loss: 4.0804...  0.3413 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12291...  Training loss: 4.0793...  0.3398 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12292...  Training loss: 4.1143...  0.3388 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12293...  Training loss: 4.1046...  0.3400 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12294...  Training loss: 4.0322...  0.3401 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12295...  Training loss: 4.0958...  0.3432 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12296...  Training loss: 4.0701...  0.3434 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12297...  Training loss: 4.0577...  0.3444 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12298...  Training loss: 4.1048...  0.3399 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12299...  Training loss: 4.1392...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12300...  Training loss: 4.1065...  0.3418 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12301...  Training loss: 4.1114...  0.3428 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12302...  Training loss: 4.1519...  0.3403 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12303...  Training loss: 4.0298...  0.3428 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12304...  Training loss: 4.0471...  0.3438 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12305...  Training loss: 4.0820...  0.3408 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12306...  Training loss: 4.0852...  0.3392 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12307...  Training loss: 4.0371...  0.3407 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12308...  Training loss: 3.9987...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12309...  Training loss: 4.0815...  0.3400 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12310...  Training loss: 4.0509...  0.3405 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12311...  Training loss: 4.0450...  0.3396 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12312...  Training loss: 4.0680...  0.3393 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12313...  Training loss: 4.1329...  0.3406 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12314...  Training loss: 4.1677...  0.3437 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12315...  Training loss: 4.0890...  0.3402 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12316...  Training loss: 4.0626...  0.3395 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12317...  Training loss: 4.0291...  0.3396 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12318...  Training loss: 3.9904...  0.3408 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12319...  Training loss: 3.9882...  0.3393 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12320...  Training loss: 4.0637...  0.3421 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12321...  Training loss: 4.1111...  0.3392 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12322...  Training loss: 4.0783...  0.3423 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12323...  Training loss: 4.0840...  0.3397 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12324...  Training loss: 4.0458...  0.3446 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67/100...  Training Step: 12325...  Training loss: 4.1077...  0.3420 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12326...  Training loss: 4.0734...  0.3427 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12327...  Training loss: 4.0331...  0.3437 sec/batch\n",
      "Epoch: 67/100...  Training Step: 12328...  Training loss: 4.0776...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12329...  Training loss: 4.0541...  0.3434 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12330...  Training loss: 3.8467...  0.3417 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12331...  Training loss: 3.8594...  0.3390 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12332...  Training loss: 3.9616...  0.3437 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12333...  Training loss: 3.9225...  0.3442 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12334...  Training loss: 3.8739...  0.3424 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12335...  Training loss: 4.0548...  0.3395 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12336...  Training loss: 4.0826...  0.3428 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12337...  Training loss: 4.0452...  0.3409 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12338...  Training loss: 4.0520...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12339...  Training loss: 4.0412...  0.3394 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12340...  Training loss: 4.0361...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12341...  Training loss: 4.0635...  0.3417 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12342...  Training loss: 4.0429...  0.3434 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12343...  Training loss: 4.0949...  0.3417 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12344...  Training loss: 4.0581...  0.3440 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12345...  Training loss: 4.1269...  0.3397 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12346...  Training loss: 4.0266...  0.3411 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12347...  Training loss: 4.1397...  0.3436 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12348...  Training loss: 4.1080...  0.3397 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12349...  Training loss: 4.0482...  0.3395 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12350...  Training loss: 3.9523...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12351...  Training loss: 3.9951...  0.3411 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12352...  Training loss: 3.9921...  0.3439 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12353...  Training loss: 4.0478...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12354...  Training loss: 4.0482...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12355...  Training loss: 4.0850...  0.3434 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12356...  Training loss: 4.0639...  0.3405 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12357...  Training loss: 4.1115...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12358...  Training loss: 4.1599...  0.3416 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12359...  Training loss: 4.1295...  0.3420 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12360...  Training loss: 4.0914...  0.3429 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12361...  Training loss: 3.9669...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12362...  Training loss: 4.0039...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12363...  Training loss: 4.0982...  0.3401 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12364...  Training loss: 4.1627...  0.3407 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12365...  Training loss: 4.1882...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12366...  Training loss: 4.1430...  0.3426 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12367...  Training loss: 4.1271...  0.3432 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12368...  Training loss: 4.1369...  0.3425 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12369...  Training loss: 4.1670...  0.3388 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12370...  Training loss: 4.1913...  0.3400 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12371...  Training loss: 4.2379...  0.3427 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12372...  Training loss: 4.1730...  0.3411 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12373...  Training loss: 4.1433...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12374...  Training loss: 4.1294...  0.3406 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12375...  Training loss: 4.1987...  0.3404 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12376...  Training loss: 4.1124...  0.3406 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12377...  Training loss: 4.1319...  0.3403 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12378...  Training loss: 4.1414...  0.3392 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12379...  Training loss: 4.1949...  0.3431 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12380...  Training loss: 4.1994...  0.3444 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12381...  Training loss: 4.1658...  0.3405 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12382...  Training loss: 4.1632...  0.3405 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12383...  Training loss: 4.1634...  0.3403 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12384...  Training loss: 4.1362...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12385...  Training loss: 4.0787...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12386...  Training loss: 4.1307...  0.3445 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12387...  Training loss: 4.0426...  0.3429 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12388...  Training loss: 4.0395...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12389...  Training loss: 4.0700...  0.3439 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12390...  Training loss: 4.0863...  0.3425 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12391...  Training loss: 4.0991...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12392...  Training loss: 4.1418...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12393...  Training loss: 4.0773...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12394...  Training loss: 4.0486...  0.3410 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12395...  Training loss: 4.0679...  0.3391 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12396...  Training loss: 4.0050...  0.3391 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12397...  Training loss: 4.0909...  0.3396 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12398...  Training loss: 4.1228...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12399...  Training loss: 4.0617...  0.3424 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12400...  Training loss: 4.0108...  0.3428 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12401...  Training loss: 4.0539...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12402...  Training loss: 4.0357...  0.3414 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12403...  Training loss: 4.0653...  0.3445 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12404...  Training loss: 4.0574...  0.3425 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12405...  Training loss: 4.1021...  0.3441 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12406...  Training loss: 4.0422...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12407...  Training loss: 4.1082...  0.3402 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12408...  Training loss: 4.0358...  0.3409 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12409...  Training loss: 4.1316...  0.3428 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12410...  Training loss: 4.0857...  0.3403 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12411...  Training loss: 4.0748...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12412...  Training loss: 4.0698...  0.3404 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12413...  Training loss: 4.0825...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12414...  Training loss: 4.0949...  0.3421 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12415...  Training loss: 3.9982...  0.3408 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12416...  Training loss: 3.9753...  0.3444 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12417...  Training loss: 4.0638...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12418...  Training loss: 4.1325...  0.3415 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12419...  Training loss: 4.0413...  0.3401 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12420...  Training loss: 4.0002...  0.3415 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/100...  Training Step: 12421...  Training loss: 4.0436...  0.3400 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12422...  Training loss: 4.0584...  0.3434 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12423...  Training loss: 4.0885...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12424...  Training loss: 4.1004...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12425...  Training loss: 4.0035...  0.3414 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12426...  Training loss: 4.0210...  0.3411 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12427...  Training loss: 4.0260...  0.3431 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12428...  Training loss: 4.0272...  0.3402 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12429...  Training loss: 4.0857...  0.3415 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12430...  Training loss: 4.0318...  0.3415 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12431...  Training loss: 4.0594...  0.3407 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12432...  Training loss: 3.9570...  0.3403 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12433...  Training loss: 4.1164...  0.3382 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12434...  Training loss: 4.0758...  0.3407 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12435...  Training loss: 4.0458...  0.3404 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12436...  Training loss: 4.0130...  0.3391 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12437...  Training loss: 4.0079...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12438...  Training loss: 4.0126...  0.3405 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12439...  Training loss: 4.0713...  0.3389 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12440...  Training loss: 4.0429...  0.3442 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12441...  Training loss: 4.0387...  0.3407 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12442...  Training loss: 4.0056...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12443...  Training loss: 4.0955...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12444...  Training loss: 4.0112...  0.3441 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12445...  Training loss: 4.0620...  0.3408 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12446...  Training loss: 3.9802...  0.3396 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12447...  Training loss: 4.0641...  0.3412 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12448...  Training loss: 4.0252...  0.3407 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12449...  Training loss: 4.1679...  0.3402 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12450...  Training loss: 4.1479...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12451...  Training loss: 4.1568...  0.3401 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12452...  Training loss: 4.0719...  0.3412 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12453...  Training loss: 4.0746...  0.3416 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12454...  Training loss: 4.1123...  0.3404 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12455...  Training loss: 4.0802...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12456...  Training loss: 4.0870...  0.3441 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12457...  Training loss: 4.1587...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12458...  Training loss: 4.1370...  0.3443 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12459...  Training loss: 4.1627...  0.3439 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12460...  Training loss: 4.0927...  0.3395 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12461...  Training loss: 4.1136...  0.3442 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12462...  Training loss: 4.0591...  0.3410 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12463...  Training loss: 4.1199...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12464...  Training loss: 4.0619...  0.3432 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12465...  Training loss: 4.0796...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12466...  Training loss: 4.0633...  0.3392 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12467...  Training loss: 4.0946...  0.3418 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12468...  Training loss: 4.0085...  0.3429 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12469...  Training loss: 4.0796...  0.3392 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12470...  Training loss: 4.0803...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12471...  Training loss: 4.0676...  0.3409 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12472...  Training loss: 4.0286...  0.3427 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12473...  Training loss: 4.0655...  0.3433 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12474...  Training loss: 4.0579...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12475...  Training loss: 4.0501...  0.3454 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12476...  Training loss: 4.0608...  0.3401 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12477...  Training loss: 4.1142...  0.3429 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12478...  Training loss: 4.0163...  0.3414 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12479...  Training loss: 4.0565...  0.3409 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12480...  Training loss: 4.0172...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12481...  Training loss: 4.0917...  0.3400 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12482...  Training loss: 4.0639...  0.3412 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12483...  Training loss: 4.1200...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12484...  Training loss: 4.0990...  0.3439 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12485...  Training loss: 4.1049...  0.3399 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12486...  Training loss: 4.1357...  0.3440 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12487...  Training loss: 4.0104...  0.3442 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12488...  Training loss: 4.0528...  0.3400 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12489...  Training loss: 4.0783...  0.3400 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12490...  Training loss: 4.1110...  0.3397 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12491...  Training loss: 4.0213...  0.3398 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12492...  Training loss: 3.9877...  0.3390 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12493...  Training loss: 4.0703...  0.3409 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12494...  Training loss: 4.0370...  0.3446 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12495...  Training loss: 4.0064...  0.3447 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12496...  Training loss: 4.0108...  0.3417 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12497...  Training loss: 4.1275...  0.3420 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12498...  Training loss: 4.1259...  0.3411 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12499...  Training loss: 4.0698...  0.3441 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12500...  Training loss: 4.0315...  0.3422 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12501...  Training loss: 3.9848...  0.3406 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12502...  Training loss: 4.0172...  0.3419 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12503...  Training loss: 3.9651...  0.3420 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12504...  Training loss: 4.0752...  0.3404 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12505...  Training loss: 4.0890...  0.3412 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12506...  Training loss: 4.0711...  0.3423 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12507...  Training loss: 4.0711...  0.3440 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12508...  Training loss: 4.0409...  0.3427 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12509...  Training loss: 4.0530...  0.3420 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12510...  Training loss: 4.0732...  0.3405 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12511...  Training loss: 4.0310...  0.3384 sec/batch\n",
      "Epoch: 68/100...  Training Step: 12512...  Training loss: 4.0627...  0.3412 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12513...  Training loss: 4.0052...  0.3445 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12514...  Training loss: 3.8511...  0.3431 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12515...  Training loss: 3.8034...  0.3432 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12516...  Training loss: 3.9151...  0.3436 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/100...  Training Step: 12517...  Training loss: 3.9083...  0.3401 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12518...  Training loss: 3.8597...  0.3424 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12519...  Training loss: 4.0176...  0.3386 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12520...  Training loss: 4.0413...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12521...  Training loss: 4.0348...  0.3427 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12522...  Training loss: 4.0577...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12523...  Training loss: 4.0142...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12524...  Training loss: 4.0073...  0.3394 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12525...  Training loss: 3.9842...  0.3402 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12526...  Training loss: 3.9996...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12527...  Training loss: 4.0336...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12528...  Training loss: 4.0412...  0.3398 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12529...  Training loss: 4.1052...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12530...  Training loss: 4.0166...  0.3414 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12531...  Training loss: 4.1173...  0.3423 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12532...  Training loss: 4.1242...  0.3426 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12533...  Training loss: 4.0476...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12534...  Training loss: 3.9258...  0.3396 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12535...  Training loss: 3.9732...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12536...  Training loss: 3.9751...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12537...  Training loss: 4.0584...  0.3420 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12538...  Training loss: 4.0218...  0.3410 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12539...  Training loss: 4.0436...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12540...  Training loss: 4.0368...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12541...  Training loss: 4.0681...  0.3424 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12542...  Training loss: 4.1490...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12543...  Training loss: 4.1296...  0.3435 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12544...  Training loss: 4.0755...  0.3398 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12545...  Training loss: 3.9398...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12546...  Training loss: 3.9900...  0.3418 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12547...  Training loss: 4.0543...  0.3422 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12548...  Training loss: 4.1239...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12549...  Training loss: 4.1233...  0.3448 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12550...  Training loss: 4.0732...  0.3452 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12551...  Training loss: 4.1288...  0.3437 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12552...  Training loss: 4.1267...  0.3400 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12553...  Training loss: 4.1103...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12554...  Training loss: 4.1688...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12555...  Training loss: 4.2242...  0.3425 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12556...  Training loss: 4.1620...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12557...  Training loss: 4.1428...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12558...  Training loss: 4.1048...  0.3446 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12559...  Training loss: 4.1484...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12560...  Training loss: 4.0734...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12561...  Training loss: 4.1024...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12562...  Training loss: 4.1375...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12563...  Training loss: 4.1492...  0.3388 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12564...  Training loss: 4.1672...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12565...  Training loss: 4.1741...  0.3389 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12566...  Training loss: 4.1823...  0.3414 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12567...  Training loss: 4.1679...  0.3410 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12568...  Training loss: 4.1688...  0.3410 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12569...  Training loss: 4.0529...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12570...  Training loss: 4.1269...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12571...  Training loss: 4.0332...  0.3412 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12572...  Training loss: 4.0366...  0.3416 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12573...  Training loss: 4.0806...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12574...  Training loss: 4.1064...  0.3446 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12575...  Training loss: 4.0703...  0.3436 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12576...  Training loss: 4.1298...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12577...  Training loss: 4.0363...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12578...  Training loss: 4.0736...  0.3414 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12579...  Training loss: 4.0637...  0.3453 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12580...  Training loss: 4.0021...  0.3389 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12581...  Training loss: 4.0874...  0.3399 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12582...  Training loss: 4.1101...  0.3416 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12583...  Training loss: 4.0354...  0.3423 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12584...  Training loss: 3.9829...  0.3378 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12585...  Training loss: 4.0209...  0.3408 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12586...  Training loss: 4.0304...  0.3402 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12587...  Training loss: 4.0229...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12588...  Training loss: 4.0604...  0.3452 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12589...  Training loss: 4.0797...  0.3427 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12590...  Training loss: 4.0112...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12591...  Training loss: 4.0611...  0.3421 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12592...  Training loss: 4.0111...  0.3409 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12593...  Training loss: 4.0817...  0.3426 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12594...  Training loss: 4.0441...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12595...  Training loss: 4.0667...  0.3427 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12596...  Training loss: 4.0337...  0.3423 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12597...  Training loss: 4.0546...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12598...  Training loss: 4.0914...  0.3418 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12599...  Training loss: 4.0113...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12600...  Training loss: 3.9826...  0.3434 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12601...  Training loss: 4.0572...  0.3440 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12602...  Training loss: 4.1232...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12603...  Training loss: 4.0368...  0.3421 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12604...  Training loss: 3.9919...  0.3393 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12605...  Training loss: 4.0300...  0.3396 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12606...  Training loss: 3.9992...  0.3443 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12607...  Training loss: 4.0731...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12608...  Training loss: 4.0940...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12609...  Training loss: 4.0155...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12610...  Training loss: 4.0159...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12611...  Training loss: 4.0094...  0.3418 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12612...  Training loss: 4.0278...  0.3398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/100...  Training Step: 12613...  Training loss: 4.0702...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12614...  Training loss: 4.0318...  0.3396 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12615...  Training loss: 4.0412...  0.3420 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12616...  Training loss: 3.9420...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12617...  Training loss: 4.0538...  0.3416 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12618...  Training loss: 4.0632...  0.3406 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12619...  Training loss: 4.0262...  0.3402 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12620...  Training loss: 3.9966...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12621...  Training loss: 4.0096...  0.3413 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12622...  Training loss: 4.0040...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12623...  Training loss: 4.0589...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12624...  Training loss: 4.0114...  0.3437 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12625...  Training loss: 4.0337...  0.3433 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12626...  Training loss: 3.9678...  0.3397 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12627...  Training loss: 4.0495...  0.3443 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12628...  Training loss: 4.0110...  0.3423 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12629...  Training loss: 4.0216...  0.3435 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12630...  Training loss: 3.9594...  0.3412 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12631...  Training loss: 4.0536...  0.3396 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12632...  Training loss: 4.0095...  0.3388 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12633...  Training loss: 4.1322...  0.3439 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12634...  Training loss: 4.1005...  0.3390 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12635...  Training loss: 4.1289...  0.3394 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12636...  Training loss: 4.0366...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12637...  Training loss: 4.0739...  0.3390 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12638...  Training loss: 4.1143...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12639...  Training loss: 4.0802...  0.3430 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12640...  Training loss: 4.0690...  0.3399 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12641...  Training loss: 4.1410...  0.3395 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12642...  Training loss: 4.1314...  0.3414 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12643...  Training loss: 4.1572...  0.3431 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12644...  Training loss: 4.0762...  0.3420 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12645...  Training loss: 4.1211...  0.3408 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12646...  Training loss: 4.0380...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12647...  Training loss: 4.0815...  0.3427 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12648...  Training loss: 4.0259...  0.3438 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12649...  Training loss: 4.0492...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12650...  Training loss: 4.0559...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12651...  Training loss: 4.0917...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12652...  Training loss: 3.9839...  0.3425 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12653...  Training loss: 4.0754...  0.3386 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12654...  Training loss: 4.0963...  0.3394 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12655...  Training loss: 4.0378...  0.3422 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12656...  Training loss: 4.0207...  0.3401 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12657...  Training loss: 4.0455...  0.3414 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12658...  Training loss: 4.0661...  0.3395 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12659...  Training loss: 4.0288...  0.3419 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12660...  Training loss: 4.0760...  0.3440 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12661...  Training loss: 4.0893...  0.3415 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12662...  Training loss: 4.0036...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12663...  Training loss: 4.0473...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12664...  Training loss: 3.9995...  0.3406 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12665...  Training loss: 4.0468...  0.3428 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12666...  Training loss: 4.0699...  0.3441 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12667...  Training loss: 4.0968...  0.3419 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12668...  Training loss: 4.0843...  0.3449 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12669...  Training loss: 4.0933...  0.3416 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12670...  Training loss: 4.1321...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12671...  Training loss: 3.9890...  0.3410 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12672...  Training loss: 4.0462...  0.3398 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12673...  Training loss: 4.0386...  0.3456 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12674...  Training loss: 4.0848...  0.3412 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12675...  Training loss: 4.0145...  0.3411 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12676...  Training loss: 3.9995...  0.3404 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12677...  Training loss: 4.0483...  0.3409 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12678...  Training loss: 4.0279...  0.3440 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12679...  Training loss: 3.9906...  0.3442 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12680...  Training loss: 3.9919...  0.3407 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12681...  Training loss: 4.1021...  0.3449 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12682...  Training loss: 4.0999...  0.3417 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12683...  Training loss: 4.0739...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12684...  Training loss: 3.9803...  0.3418 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12685...  Training loss: 3.9772...  0.3429 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12686...  Training loss: 3.9863...  0.3421 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12687...  Training loss: 3.9330...  0.3387 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12688...  Training loss: 4.0157...  0.3401 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12689...  Training loss: 4.0642...  0.3396 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12690...  Training loss: 4.0377...  0.3399 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12691...  Training loss: 4.0308...  0.3443 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12692...  Training loss: 4.0170...  0.3412 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12693...  Training loss: 4.0077...  0.3403 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12694...  Training loss: 4.0195...  0.3413 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12695...  Training loss: 4.0360...  0.3405 sec/batch\n",
      "Epoch: 69/100...  Training Step: 12696...  Training loss: 4.0744...  0.3396 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12697...  Training loss: 3.9818...  0.3438 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12698...  Training loss: 3.8210...  0.3426 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12699...  Training loss: 3.7985...  0.3416 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12700...  Training loss: 3.8898...  0.3385 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12701...  Training loss: 3.8773...  0.3426 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12702...  Training loss: 3.8485...  0.3405 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12703...  Training loss: 4.0169...  0.3398 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12704...  Training loss: 4.0426...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12705...  Training loss: 4.0422...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12706...  Training loss: 4.0446...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12707...  Training loss: 3.9926...  0.3398 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12708...  Training loss: 3.9750...  0.3398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/100...  Training Step: 12709...  Training loss: 3.9931...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12710...  Training loss: 3.9722...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12711...  Training loss: 4.0062...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12712...  Training loss: 4.0166...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12713...  Training loss: 4.0980...  0.3403 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12714...  Training loss: 3.9941...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12715...  Training loss: 4.0898...  0.3430 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12716...  Training loss: 4.1089...  0.3436 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12717...  Training loss: 4.0254...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12718...  Training loss: 3.9371...  0.3400 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12719...  Training loss: 3.9803...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12720...  Training loss: 3.9744...  0.3415 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12721...  Training loss: 4.0265...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12722...  Training loss: 4.0072...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12723...  Training loss: 4.0236...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12724...  Training loss: 4.0310...  0.3416 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12725...  Training loss: 4.0430...  0.3416 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12726...  Training loss: 4.1088...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12727...  Training loss: 4.0823...  0.3401 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12728...  Training loss: 4.0900...  0.3420 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12729...  Training loss: 3.9615...  0.3423 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12730...  Training loss: 3.9920...  0.3411 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12731...  Training loss: 4.0710...  0.3413 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12732...  Training loss: 4.1016...  0.3404 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12733...  Training loss: 4.1351...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12734...  Training loss: 4.0921...  0.3432 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12735...  Training loss: 4.1039...  0.3423 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12736...  Training loss: 4.1242...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12737...  Training loss: 4.1280...  0.3400 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12738...  Training loss: 4.1361...  0.3416 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12739...  Training loss: 4.2081...  0.3437 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12740...  Training loss: 4.1289...  0.3396 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12741...  Training loss: 4.1123...  0.3435 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12742...  Training loss: 4.0819...  0.3431 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12743...  Training loss: 4.1234...  0.3399 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12744...  Training loss: 4.0626...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12745...  Training loss: 4.0883...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12746...  Training loss: 4.0767...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12747...  Training loss: 4.1412...  0.3413 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12748...  Training loss: 4.1342...  0.3431 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12749...  Training loss: 4.1514...  0.3394 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12750...  Training loss: 4.1413...  0.3400 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12751...  Training loss: 4.1331...  0.3438 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12752...  Training loss: 4.1375...  0.3404 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12753...  Training loss: 4.0363...  0.3439 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12754...  Training loss: 4.1282...  0.3413 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12755...  Training loss: 4.0225...  0.3405 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12756...  Training loss: 4.0113...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12757...  Training loss: 4.0507...  0.3391 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12758...  Training loss: 4.0767...  0.3453 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12759...  Training loss: 4.0689...  0.3446 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12760...  Training loss: 4.1048...  0.3389 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12761...  Training loss: 4.0350...  0.3396 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12762...  Training loss: 4.0281...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12763...  Training loss: 4.0167...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12764...  Training loss: 3.9859...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12765...  Training loss: 4.0569...  0.3394 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12766...  Training loss: 4.0690...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12767...  Training loss: 4.0514...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12768...  Training loss: 3.9524...  0.3385 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12769...  Training loss: 4.0113...  0.3406 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12770...  Training loss: 4.0184...  0.3412 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12771...  Training loss: 4.0286...  0.3385 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12772...  Training loss: 4.0634...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12773...  Training loss: 4.0785...  0.3412 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12774...  Training loss: 3.9990...  0.3420 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12775...  Training loss: 4.0743...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12776...  Training loss: 4.0270...  0.3425 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12777...  Training loss: 4.0897...  0.3427 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12778...  Training loss: 4.0731...  0.3401 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12779...  Training loss: 4.0760...  0.3395 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12780...  Training loss: 4.0253...  0.3419 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12781...  Training loss: 4.0263...  0.3403 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12782...  Training loss: 4.0775...  0.3399 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12783...  Training loss: 4.0096...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12784...  Training loss: 3.9731...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12785...  Training loss: 4.0609...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12786...  Training loss: 4.1243...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12787...  Training loss: 4.0366...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12788...  Training loss: 3.9715...  0.3416 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12789...  Training loss: 4.0323...  0.3395 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12790...  Training loss: 4.0287...  0.3403 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12791...  Training loss: 4.0952...  0.3403 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12792...  Training loss: 4.1068...  0.3418 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12793...  Training loss: 4.0252...  0.3404 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12794...  Training loss: 4.0009...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12795...  Training loss: 4.0522...  0.3415 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12796...  Training loss: 4.0643...  0.3397 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12797...  Training loss: 4.0823...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12798...  Training loss: 4.0086...  0.3406 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12799...  Training loss: 4.0492...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12800...  Training loss: 3.9353...  0.3427 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12801...  Training loss: 4.0522...  0.3435 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12802...  Training loss: 4.0464...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12803...  Training loss: 4.0149...  0.3421 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12804...  Training loss: 4.0148...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70/100...  Training Step: 12805...  Training loss: 4.0040...  0.3424 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12806...  Training loss: 3.9972...  0.3418 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12807...  Training loss: 4.0430...  0.3451 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12808...  Training loss: 4.0059...  0.3441 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12809...  Training loss: 4.0332...  0.3396 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12810...  Training loss: 4.0024...  0.3403 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12811...  Training loss: 4.0722...  0.3415 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12812...  Training loss: 4.0462...  0.3436 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12813...  Training loss: 4.0813...  0.3426 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12814...  Training loss: 3.9890...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12815...  Training loss: 4.0640...  0.3390 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12816...  Training loss: 4.0342...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12817...  Training loss: 4.1545...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12818...  Training loss: 4.1272...  0.3404 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12819...  Training loss: 4.1025...  0.3406 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12820...  Training loss: 4.0042...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12821...  Training loss: 4.0778...  0.3449 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12822...  Training loss: 4.0866...  0.3428 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12823...  Training loss: 4.0991...  0.3431 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12824...  Training loss: 4.0831...  0.3444 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12825...  Training loss: 4.1276...  0.3406 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12826...  Training loss: 4.1379...  0.3418 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12827...  Training loss: 4.1286...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12828...  Training loss: 4.0555...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12829...  Training loss: 4.0698...  0.3411 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12830...  Training loss: 4.0086...  0.3432 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12831...  Training loss: 4.0599...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12832...  Training loss: 4.0129...  0.3401 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12833...  Training loss: 4.0690...  0.3397 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12834...  Training loss: 4.0450...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12835...  Training loss: 4.1158...  0.3394 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12836...  Training loss: 4.0399...  0.3384 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12837...  Training loss: 4.0732...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12838...  Training loss: 4.0966...  0.3419 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12839...  Training loss: 4.0219...  0.3423 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12840...  Training loss: 3.9760...  0.3434 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12841...  Training loss: 4.0451...  0.3412 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12842...  Training loss: 4.0248...  0.3445 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12843...  Training loss: 4.0216...  0.3436 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12844...  Training loss: 4.0605...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12845...  Training loss: 4.0813...  0.3406 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12846...  Training loss: 3.9786...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12847...  Training loss: 4.0246...  0.3393 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12848...  Training loss: 4.0075...  0.3436 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12849...  Training loss: 4.0375...  0.3402 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12850...  Training loss: 4.0435...  0.3405 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12851...  Training loss: 4.1074...  0.3401 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12852...  Training loss: 4.0661...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12853...  Training loss: 4.0855...  0.3442 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12854...  Training loss: 4.1351...  0.3435 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12855...  Training loss: 3.9488...  0.3435 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12856...  Training loss: 3.9972...  0.3414 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12857...  Training loss: 4.0432...  0.3398 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12858...  Training loss: 4.0555...  0.3441 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12859...  Training loss: 4.0314...  0.3443 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12860...  Training loss: 3.9757...  0.3434 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12861...  Training loss: 4.0371...  0.3394 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12862...  Training loss: 3.9973...  0.3417 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12863...  Training loss: 3.9812...  0.3424 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12864...  Training loss: 4.0046...  0.3411 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12865...  Training loss: 4.1006...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12866...  Training loss: 4.0969...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12867...  Training loss: 4.0362...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12868...  Training loss: 3.9986...  0.3412 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12869...  Training loss: 3.9606...  0.3409 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12870...  Training loss: 3.9727...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12871...  Training loss: 3.9457...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12872...  Training loss: 4.0051...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12873...  Training loss: 4.0549...  0.3408 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12874...  Training loss: 4.0039...  0.3410 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12875...  Training loss: 3.9977...  0.3407 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12876...  Training loss: 3.9702...  0.3424 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12877...  Training loss: 3.9939...  0.3422 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12878...  Training loss: 4.0217...  0.3429 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12879...  Training loss: 3.9744...  0.3393 sec/batch\n",
      "Epoch: 70/100...  Training Step: 12880...  Training loss: 4.0244...  0.3395 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12881...  Training loss: 3.9904...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12882...  Training loss: 3.8171...  0.3436 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12883...  Training loss: 3.8050...  0.3415 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12884...  Training loss: 3.8920...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12885...  Training loss: 3.8313...  0.3406 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12886...  Training loss: 3.8289...  0.3439 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12887...  Training loss: 3.9835...  0.3425 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12888...  Training loss: 4.0002...  0.3456 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12889...  Training loss: 4.0027...  0.3462 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12890...  Training loss: 4.0083...  0.3418 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12891...  Training loss: 4.0165...  0.3447 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12892...  Training loss: 3.9503...  0.3456 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12893...  Training loss: 4.0000...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12894...  Training loss: 3.9658...  0.3448 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12895...  Training loss: 3.9773...  0.3454 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12896...  Training loss: 3.9684...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12897...  Training loss: 4.0371...  0.3434 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12898...  Training loss: 3.9678...  0.3448 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12899...  Training loss: 4.0590...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12900...  Training loss: 4.0777...  0.3439 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100...  Training Step: 12901...  Training loss: 4.0072...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12902...  Training loss: 3.9015...  0.3436 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12903...  Training loss: 3.9564...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12904...  Training loss: 3.9689...  0.3439 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12905...  Training loss: 4.0012...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12906...  Training loss: 3.9964...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12907...  Training loss: 4.0066...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12908...  Training loss: 4.0132...  0.3447 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12909...  Training loss: 4.0421...  0.3440 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12910...  Training loss: 4.1126...  0.3429 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12911...  Training loss: 4.0851...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12912...  Training loss: 4.0543...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12913...  Training loss: 3.9273...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12914...  Training loss: 3.9659...  0.3444 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12915...  Training loss: 4.0304...  0.3415 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12916...  Training loss: 4.0999...  0.3419 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12917...  Training loss: 4.0926...  0.3443 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12918...  Training loss: 4.0748...  0.3426 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12919...  Training loss: 4.0651...  0.3443 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12920...  Training loss: 4.1000...  0.3417 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12921...  Training loss: 4.0759...  0.3445 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12922...  Training loss: 4.0969...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12923...  Training loss: 4.1568...  0.3428 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12924...  Training loss: 4.1182...  0.3404 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12925...  Training loss: 4.0512...  0.3404 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12926...  Training loss: 4.0712...  0.3417 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12927...  Training loss: 4.1049...  0.3424 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12928...  Training loss: 4.0732...  0.3421 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12929...  Training loss: 4.0672...  0.3419 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12930...  Training loss: 4.1003...  0.3438 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12931...  Training loss: 4.1261...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12932...  Training loss: 4.1392...  0.3416 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12933...  Training loss: 4.1181...  0.3438 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12934...  Training loss: 4.1449...  0.3436 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12935...  Training loss: 4.1558...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12936...  Training loss: 4.1223...  0.3413 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12937...  Training loss: 4.0197...  0.3410 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12938...  Training loss: 4.0974...  0.3417 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12939...  Training loss: 3.9806...  0.3421 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12940...  Training loss: 3.9952...  0.3420 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12941...  Training loss: 4.0208...  0.3426 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12942...  Training loss: 4.0633...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12943...  Training loss: 4.0656...  0.3434 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12944...  Training loss: 4.1170...  0.3448 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12945...  Training loss: 4.0398...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12946...  Training loss: 3.9921...  0.3459 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12947...  Training loss: 4.0108...  0.3419 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12948...  Training loss: 3.9873...  0.3449 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12949...  Training loss: 4.0519...  0.3412 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12950...  Training loss: 4.0526...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12951...  Training loss: 4.0188...  0.3418 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12952...  Training loss: 3.9624...  0.3417 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12953...  Training loss: 4.0155...  0.3455 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12954...  Training loss: 3.9964...  0.3438 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12955...  Training loss: 3.9778...  0.3444 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12956...  Training loss: 3.9990...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12957...  Training loss: 4.0354...  0.3434 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12958...  Training loss: 3.9535...  0.3445 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12959...  Training loss: 4.0139...  0.3419 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12960...  Training loss: 3.9836...  0.3436 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12961...  Training loss: 4.0511...  0.3428 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12962...  Training loss: 4.0625...  0.3434 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12963...  Training loss: 4.0722...  0.3446 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12964...  Training loss: 4.0232...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12965...  Training loss: 4.0534...  0.3448 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12966...  Training loss: 4.0719...  0.3420 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12967...  Training loss: 4.0017...  0.3425 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12968...  Training loss: 3.9751...  0.3451 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12969...  Training loss: 4.0248...  0.3449 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12970...  Training loss: 4.1344...  0.3446 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12971...  Training loss: 4.0474...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12972...  Training loss: 3.9637...  0.3436 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12973...  Training loss: 4.0108...  0.3420 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12974...  Training loss: 4.0240...  0.3450 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12975...  Training loss: 4.0514...  0.3425 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12976...  Training loss: 4.1114...  0.3447 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12977...  Training loss: 4.0536...  0.3440 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12978...  Training loss: 4.0192...  0.3421 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12979...  Training loss: 4.0488...  0.3427 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12980...  Training loss: 4.0293...  0.3448 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12981...  Training loss: 4.0767...  0.3457 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12982...  Training loss: 3.9923...  0.3442 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12983...  Training loss: 4.0424...  0.3423 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12984...  Training loss: 3.8883...  0.3449 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12985...  Training loss: 4.0380...  0.3445 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12986...  Training loss: 4.0131...  0.3429 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12987...  Training loss: 3.9885...  0.3416 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12988...  Training loss: 3.9713...  0.3423 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12989...  Training loss: 3.9807...  0.3438 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12990...  Training loss: 3.9747...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12991...  Training loss: 4.0232...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12992...  Training loss: 3.9891...  0.3453 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12993...  Training loss: 3.9885...  0.3449 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12994...  Training loss: 3.9368...  0.3456 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12995...  Training loss: 4.0547...  0.3429 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12996...  Training loss: 3.9998...  0.3444 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/100...  Training Step: 12997...  Training loss: 4.0564...  0.3449 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12998...  Training loss: 3.9790...  0.3408 sec/batch\n",
      "Epoch: 71/100...  Training Step: 12999...  Training loss: 4.0819...  0.3408 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13000...  Training loss: 4.0229...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13001...  Training loss: 4.1861...  0.3842 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13002...  Training loss: 4.1433...  0.3476 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13003...  Training loss: 4.1036...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13004...  Training loss: 3.9973...  0.3435 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13005...  Training loss: 4.0447...  0.3427 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13006...  Training loss: 4.0646...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13007...  Training loss: 4.0733...  0.3391 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13008...  Training loss: 4.0744...  0.3392 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13009...  Training loss: 4.1182...  0.3418 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13010...  Training loss: 4.1307...  0.3423 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13011...  Training loss: 4.1279...  0.3442 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13012...  Training loss: 4.0504...  0.3415 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13013...  Training loss: 4.0620...  0.3428 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13014...  Training loss: 4.0227...  0.3399 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13015...  Training loss: 4.0589...  0.3404 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13016...  Training loss: 3.9760...  0.3397 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13017...  Training loss: 4.0297...  0.3446 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13018...  Training loss: 4.0464...  0.3411 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13019...  Training loss: 4.1051...  0.3419 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13020...  Training loss: 3.9997...  0.3389 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13021...  Training loss: 4.0854...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13022...  Training loss: 4.0841...  0.3423 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13023...  Training loss: 4.0307...  0.3563 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13024...  Training loss: 4.0127...  0.3437 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13025...  Training loss: 4.0291...  0.3445 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13026...  Training loss: 4.0108...  0.3431 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13027...  Training loss: 4.0136...  0.3432 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13028...  Training loss: 4.0312...  0.3427 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13029...  Training loss: 4.0514...  0.3418 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13030...  Training loss: 3.9849...  0.3392 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13031...  Training loss: 4.0297...  0.3397 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13032...  Training loss: 4.0251...  0.3413 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13033...  Training loss: 4.0246...  0.3401 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13034...  Training loss: 4.0357...  0.3439 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13035...  Training loss: 4.0663...  0.3401 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13036...  Training loss: 4.0411...  0.3410 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13037...  Training loss: 4.0669...  0.3417 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13038...  Training loss: 4.1071...  0.3438 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13039...  Training loss: 3.9783...  0.3413 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13040...  Training loss: 4.0225...  0.3389 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13041...  Training loss: 4.0447...  0.3404 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13042...  Training loss: 4.0716...  0.3431 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13043...  Training loss: 3.9581...  0.3398 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13044...  Training loss: 3.9522...  0.3405 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13045...  Training loss: 4.0234...  0.3410 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13046...  Training loss: 3.9924...  0.3433 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13047...  Training loss: 3.9633...  0.3403 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13048...  Training loss: 4.0041...  0.3403 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13049...  Training loss: 4.0797...  0.3402 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13050...  Training loss: 4.0864...  0.3386 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13051...  Training loss: 4.0386...  0.3432 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13052...  Training loss: 3.9796...  0.3392 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13053...  Training loss: 3.9770...  0.3410 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13054...  Training loss: 3.9482...  0.3388 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13055...  Training loss: 3.8986...  0.3430 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13056...  Training loss: 3.9912...  0.3410 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13057...  Training loss: 4.0286...  0.3407 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13058...  Training loss: 3.9757...  0.3427 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13059...  Training loss: 3.9717...  0.3422 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13060...  Training loss: 3.9884...  0.3414 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13061...  Training loss: 3.9850...  0.3411 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13062...  Training loss: 3.9928...  0.3408 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13063...  Training loss: 3.9678...  0.3415 sec/batch\n",
      "Epoch: 71/100...  Training Step: 13064...  Training loss: 4.0087...  0.3404 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13065...  Training loss: 3.9787...  0.3423 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13066...  Training loss: 3.7629...  0.3416 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13067...  Training loss: 3.7797...  0.3385 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13068...  Training loss: 3.8561...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13069...  Training loss: 3.8176...  0.3388 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13070...  Training loss: 3.7867...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13071...  Training loss: 3.9825...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13072...  Training loss: 3.9774...  0.3370 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13073...  Training loss: 3.9524...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13074...  Training loss: 3.9906...  0.3399 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13075...  Training loss: 3.9805...  0.3423 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13076...  Training loss: 3.9502...  0.3387 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13077...  Training loss: 3.9790...  0.3415 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13078...  Training loss: 3.9652...  0.3388 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13079...  Training loss: 3.9724...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13080...  Training loss: 3.9638...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13081...  Training loss: 4.0169...  0.3431 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13082...  Training loss: 3.9462...  0.3393 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13083...  Training loss: 4.0284...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13084...  Training loss: 4.0531...  0.3401 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13085...  Training loss: 3.9783...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13086...  Training loss: 3.9057...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13087...  Training loss: 3.9613...  0.3383 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13088...  Training loss: 3.9800...  0.3393 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13089...  Training loss: 4.0167...  0.3400 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13090...  Training loss: 3.9656...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13091...  Training loss: 4.0020...  0.3387 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13092...  Training loss: 3.9662...  0.3405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100...  Training Step: 13093...  Training loss: 3.9974...  0.3470 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13094...  Training loss: 4.1037...  0.3384 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13095...  Training loss: 4.0515...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13096...  Training loss: 4.0323...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13097...  Training loss: 3.9109...  0.3373 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13098...  Training loss: 3.9338...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13099...  Training loss: 4.0263...  0.3401 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13100...  Training loss: 4.0812...  0.3407 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13101...  Training loss: 4.1331...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13102...  Training loss: 4.0706...  0.3378 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13103...  Training loss: 4.0808...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13104...  Training loss: 4.0786...  0.3412 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13105...  Training loss: 4.0698...  0.3382 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13106...  Training loss: 4.0873...  0.3396 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13107...  Training loss: 4.1577...  0.3379 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13108...  Training loss: 4.0990...  0.3380 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13109...  Training loss: 4.0862...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13110...  Training loss: 4.0723...  0.3393 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13111...  Training loss: 4.0926...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13112...  Training loss: 4.0248...  0.3386 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13113...  Training loss: 4.0328...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13114...  Training loss: 4.0444...  0.3388 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13115...  Training loss: 4.0683...  0.3399 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13116...  Training loss: 4.0937...  0.3388 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13117...  Training loss: 4.1031...  0.3380 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13118...  Training loss: 4.1037...  0.3403 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13119...  Training loss: 4.0908...  0.3423 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13120...  Training loss: 4.1031...  0.3417 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13121...  Training loss: 4.0177...  0.3419 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13122...  Training loss: 4.0858...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13123...  Training loss: 3.9763...  0.3382 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13124...  Training loss: 3.9523...  0.3412 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13125...  Training loss: 4.0183...  0.3396 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13126...  Training loss: 4.0377...  0.3408 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13127...  Training loss: 4.0355...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13128...  Training loss: 4.1002...  0.3420 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13129...  Training loss: 4.0125...  0.3389 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13130...  Training loss: 4.0025...  0.3395 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13131...  Training loss: 4.0278...  0.3403 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13132...  Training loss: 3.9652...  0.3400 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13133...  Training loss: 4.0438...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13134...  Training loss: 4.0673...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13135...  Training loss: 3.9995...  0.3372 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13136...  Training loss: 3.9500...  0.3393 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13137...  Training loss: 4.0221...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13138...  Training loss: 3.9914...  0.3387 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13139...  Training loss: 3.9823...  0.3396 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13140...  Training loss: 3.9980...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13141...  Training loss: 4.0248...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13142...  Training loss: 3.9428...  0.3387 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13143...  Training loss: 4.0205...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13144...  Training loss: 3.9669...  0.3380 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13145...  Training loss: 4.0362...  0.3384 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13146...  Training loss: 4.0252...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13147...  Training loss: 4.0447...  0.3399 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13148...  Training loss: 4.0256...  0.3414 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13149...  Training loss: 4.0332...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13150...  Training loss: 4.0970...  0.3401 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13151...  Training loss: 4.0092...  0.3436 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13152...  Training loss: 3.9690...  0.3414 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13153...  Training loss: 4.0396...  0.3404 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13154...  Training loss: 4.1013...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13155...  Training loss: 4.0135...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13156...  Training loss: 3.9663...  0.3390 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13157...  Training loss: 4.0100...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13158...  Training loss: 3.9716...  0.3389 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13159...  Training loss: 4.0617...  0.3390 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13160...  Training loss: 4.0830...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13161...  Training loss: 4.0329...  0.3418 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13162...  Training loss: 3.9967...  0.3395 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13163...  Training loss: 4.0344...  0.3411 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13164...  Training loss: 4.0489...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13165...  Training loss: 4.0774...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13166...  Training loss: 4.0054...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13167...  Training loss: 4.0265...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13168...  Training loss: 3.9328...  0.3401 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13169...  Training loss: 3.9991...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13170...  Training loss: 3.9990...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13171...  Training loss: 3.9811...  0.3397 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13172...  Training loss: 3.9754...  0.3399 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13173...  Training loss: 3.9610...  0.3422 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13174...  Training loss: 3.9524...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13175...  Training loss: 3.9716...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13176...  Training loss: 3.9717...  0.3395 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13177...  Training loss: 3.9694...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13178...  Training loss: 3.9145...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13179...  Training loss: 4.0100...  0.3390 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13180...  Training loss: 3.9724...  0.3382 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13181...  Training loss: 4.0130...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13182...  Training loss: 3.9579...  0.3404 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13183...  Training loss: 4.0396...  0.3422 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13184...  Training loss: 4.0048...  0.3409 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13185...  Training loss: 4.1699...  0.3390 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13186...  Training loss: 4.1308...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13187...  Training loss: 4.1265...  0.3416 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13188...  Training loss: 3.9924...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72/100...  Training Step: 13189...  Training loss: 4.0648...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13190...  Training loss: 4.0860...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13191...  Training loss: 4.0717...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13192...  Training loss: 4.1039...  0.3408 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13193...  Training loss: 4.1236...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13194...  Training loss: 4.0997...  0.3382 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13195...  Training loss: 4.1277...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13196...  Training loss: 4.0395...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13197...  Training loss: 4.0927...  0.3384 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13198...  Training loss: 4.0307...  0.3415 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13199...  Training loss: 4.0778...  0.3393 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13200...  Training loss: 3.9949...  0.3414 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13201...  Training loss: 4.0343...  0.3428 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13202...  Training loss: 4.0166...  0.3401 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13203...  Training loss: 4.0852...  0.3386 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13204...  Training loss: 4.0190...  0.3409 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13205...  Training loss: 4.0852...  0.3396 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13206...  Training loss: 4.0579...  0.3420 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13207...  Training loss: 4.0228...  0.3382 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13208...  Training loss: 3.9865...  0.3426 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13209...  Training loss: 4.0279...  0.3432 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13210...  Training loss: 4.0274...  0.3426 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13211...  Training loss: 3.9976...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13212...  Training loss: 4.0322...  0.3417 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13213...  Training loss: 4.0428...  0.3416 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13214...  Training loss: 3.9702...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13215...  Training loss: 4.0269...  0.3383 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13216...  Training loss: 4.0287...  0.3410 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13217...  Training loss: 4.0069...  0.3383 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13218...  Training loss: 4.0436...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13219...  Training loss: 4.0429...  0.3392 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13220...  Training loss: 4.0333...  0.3380 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13221...  Training loss: 4.0277...  0.3388 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13222...  Training loss: 4.0796...  0.3413 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13223...  Training loss: 3.9360...  0.3381 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13224...  Training loss: 4.0021...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13225...  Training loss: 4.0147...  0.3406 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13226...  Training loss: 4.0556...  0.3407 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13227...  Training loss: 4.0027...  0.3422 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13228...  Training loss: 3.9525...  0.3380 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13229...  Training loss: 4.0452...  0.3396 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13230...  Training loss: 3.9960...  0.3407 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13231...  Training loss: 3.9676...  0.3402 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13232...  Training loss: 3.9637...  0.3416 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13233...  Training loss: 4.0627...  0.3418 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13234...  Training loss: 4.0786...  0.3400 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13235...  Training loss: 4.0242...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13236...  Training loss: 3.9798...  0.3391 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13237...  Training loss: 3.9541...  0.3404 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13238...  Training loss: 3.9561...  0.3403 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13239...  Training loss: 3.9099...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13240...  Training loss: 3.9845...  0.3407 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13241...  Training loss: 4.0333...  0.3425 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13242...  Training loss: 3.9738...  0.3398 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13243...  Training loss: 3.9769...  0.3405 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13244...  Training loss: 3.9637...  0.3390 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13245...  Training loss: 3.9628...  0.3394 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13246...  Training loss: 3.9729...  0.3407 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13247...  Training loss: 3.9397...  0.3384 sec/batch\n",
      "Epoch: 72/100...  Training Step: 13248...  Training loss: 3.9864...  0.3412 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13249...  Training loss: 3.9546...  0.3423 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13250...  Training loss: 3.7710...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13251...  Training loss: 3.7513...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13252...  Training loss: 3.8353...  0.3437 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13253...  Training loss: 3.8140...  0.3418 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13254...  Training loss: 3.7742...  0.3385 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13255...  Training loss: 3.9478...  0.3400 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13256...  Training loss: 3.9557...  0.3392 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13257...  Training loss: 3.9508...  0.3388 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13258...  Training loss: 3.9528...  0.3403 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13259...  Training loss: 3.9360...  0.3385 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13260...  Training loss: 3.9376...  0.3382 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13261...  Training loss: 3.9557...  0.3377 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13262...  Training loss: 3.9018...  0.3455 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13263...  Training loss: 3.9770...  0.3456 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13264...  Training loss: 3.9485...  0.3404 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13265...  Training loss: 4.0190...  0.3387 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13266...  Training loss: 3.9446...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13267...  Training loss: 4.0055...  0.3419 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13268...  Training loss: 4.0021...  0.3419 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13269...  Training loss: 3.9563...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13270...  Training loss: 3.8718...  0.3426 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13271...  Training loss: 3.9277...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13272...  Training loss: 3.9234...  0.3412 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13273...  Training loss: 3.9847...  0.3377 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13274...  Training loss: 3.9584...  0.3421 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13275...  Training loss: 3.9816...  0.3435 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13276...  Training loss: 3.9563...  0.3408 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13277...  Training loss: 3.9742...  0.3433 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13278...  Training loss: 4.0755...  0.3436 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13279...  Training loss: 4.0215...  0.3400 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13280...  Training loss: 3.9778...  0.3406 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13281...  Training loss: 3.9093...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13282...  Training loss: 3.9488...  0.3428 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13283...  Training loss: 4.0071...  0.3432 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13284...  Training loss: 4.1021...  0.3396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/100...  Training Step: 13285...  Training loss: 4.0804...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13286...  Training loss: 4.0555...  0.3429 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13287...  Training loss: 4.0817...  0.3389 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13288...  Training loss: 4.0887...  0.3417 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13289...  Training loss: 4.0492...  0.3441 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13290...  Training loss: 4.1005...  0.3393 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13291...  Training loss: 4.1620...  0.3429 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13292...  Training loss: 4.0984...  0.3403 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13293...  Training loss: 4.0566...  0.3411 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13294...  Training loss: 4.0030...  0.3420 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13295...  Training loss: 4.1005...  0.3410 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13296...  Training loss: 4.0284...  0.3434 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13297...  Training loss: 4.0497...  0.3405 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13298...  Training loss: 4.0626...  0.3414 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13299...  Training loss: 4.0753...  0.3411 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13300...  Training loss: 4.1016...  0.3413 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13301...  Training loss: 4.0754...  0.3431 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13302...  Training loss: 4.0565...  0.3389 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13303...  Training loss: 4.1007...  0.3426 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13304...  Training loss: 4.0953...  0.3404 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13305...  Training loss: 4.0024...  0.3416 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13306...  Training loss: 4.0627...  0.3439 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13307...  Training loss: 3.9699...  0.3436 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13308...  Training loss: 3.9412...  0.3389 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13309...  Training loss: 3.9741...  0.3393 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13310...  Training loss: 4.0210...  0.3408 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13311...  Training loss: 4.0230...  0.3412 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13312...  Training loss: 4.0885...  0.3432 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13313...  Training loss: 4.0019...  0.3406 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13314...  Training loss: 3.9928...  0.3403 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13315...  Training loss: 4.0190...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13316...  Training loss: 3.9452...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13317...  Training loss: 4.0133...  0.3413 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13318...  Training loss: 4.0351...  0.3409 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13319...  Training loss: 4.0013...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13320...  Training loss: 3.9185...  0.3409 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13321...  Training loss: 3.9694...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13322...  Training loss: 3.9566...  0.3428 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13323...  Training loss: 3.9695...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13324...  Training loss: 3.9840...  0.3425 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13325...  Training loss: 4.0335...  0.3384 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13326...  Training loss: 3.9600...  0.3395 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13327...  Training loss: 4.0038...  0.3391 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13328...  Training loss: 3.9552...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13329...  Training loss: 3.9793...  0.3387 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13330...  Training loss: 4.0183...  0.3379 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13331...  Training loss: 4.0036...  0.3379 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13332...  Training loss: 3.9913...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13333...  Training loss: 3.9835...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13334...  Training loss: 4.0196...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13335...  Training loss: 3.9688...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13336...  Training loss: 3.8896...  0.3409 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13337...  Training loss: 4.0230...  0.3425 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13338...  Training loss: 4.0643...  0.3417 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13339...  Training loss: 4.0053...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13340...  Training loss: 3.9517...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13341...  Training loss: 4.0030...  0.3417 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13342...  Training loss: 3.9682...  0.3395 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13343...  Training loss: 4.0077...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13344...  Training loss: 4.0254...  0.3381 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13345...  Training loss: 3.9823...  0.3414 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13346...  Training loss: 3.9637...  0.3426 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13347...  Training loss: 4.0179...  0.3417 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13348...  Training loss: 4.0394...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13349...  Training loss: 4.0803...  0.3420 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13350...  Training loss: 4.0154...  0.3419 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13351...  Training loss: 4.0408...  0.3448 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13352...  Training loss: 3.9128...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13353...  Training loss: 4.0140...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13354...  Training loss: 3.9932...  0.3398 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13355...  Training loss: 3.9486...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13356...  Training loss: 3.9615...  0.3400 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13357...  Training loss: 3.9655...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13358...  Training loss: 3.9723...  0.3392 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13359...  Training loss: 4.0116...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13360...  Training loss: 3.9783...  0.3388 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13361...  Training loss: 3.9682...  0.3397 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13362...  Training loss: 3.8939...  0.3442 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13363...  Training loss: 3.9932...  0.3394 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13364...  Training loss: 3.9159...  0.3422 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13365...  Training loss: 3.9871...  0.3426 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13366...  Training loss: 3.9277...  0.3427 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13367...  Training loss: 4.0246...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13368...  Training loss: 3.9662...  0.3398 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13369...  Training loss: 4.1041...  0.3390 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13370...  Training loss: 4.0999...  0.3380 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13371...  Training loss: 4.0645...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13372...  Training loss: 3.9683...  0.3406 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13373...  Training loss: 4.0170...  0.3389 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13374...  Training loss: 4.0354...  0.3429 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13375...  Training loss: 4.0320...  0.3390 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13376...  Training loss: 4.0793...  0.3414 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13377...  Training loss: 4.0875...  0.3392 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13378...  Training loss: 4.1088...  0.3434 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13379...  Training loss: 4.1210...  0.3398 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13380...  Training loss: 4.0492...  0.3390 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73/100...  Training Step: 13381...  Training loss: 4.0796...  0.3404 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13382...  Training loss: 4.0447...  0.3390 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13383...  Training loss: 4.1169...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13384...  Training loss: 3.9864...  0.3404 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13385...  Training loss: 4.0170...  0.3415 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13386...  Training loss: 4.0249...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13387...  Training loss: 4.0615...  0.3436 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13388...  Training loss: 3.9781...  0.3422 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13389...  Training loss: 4.0732...  0.3422 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13390...  Training loss: 4.0640...  0.3407 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13391...  Training loss: 4.0091...  0.3396 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13392...  Training loss: 3.9811...  0.3385 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13393...  Training loss: 4.0503...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13394...  Training loss: 4.0082...  0.3405 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13395...  Training loss: 3.9751...  0.3414 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13396...  Training loss: 4.0124...  0.3390 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13397...  Training loss: 4.0411...  0.3400 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13398...  Training loss: 3.9604...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13399...  Training loss: 4.0152...  0.3392 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13400...  Training loss: 3.9952...  0.3406 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13401...  Training loss: 4.0278...  0.3388 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13402...  Training loss: 4.0352...  0.3391 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13403...  Training loss: 4.0676...  0.3417 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13404...  Training loss: 4.0492...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13405...  Training loss: 4.0202...  0.3416 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13406...  Training loss: 4.0936...  0.3399 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13407...  Training loss: 3.9464...  0.3387 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13408...  Training loss: 3.9811...  0.3402 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13409...  Training loss: 4.0006...  0.3391 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13410...  Training loss: 4.0162...  0.3437 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13411...  Training loss: 3.9806...  0.3435 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13412...  Training loss: 3.9621...  0.3379 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13413...  Training loss: 4.0429...  0.3409 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13414...  Training loss: 3.9951...  0.3395 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13415...  Training loss: 3.9632...  0.3368 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13416...  Training loss: 3.9530...  0.3409 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13417...  Training loss: 4.0684...  0.3386 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13418...  Training loss: 4.0519...  0.3376 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13419...  Training loss: 4.0044...  0.3380 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13420...  Training loss: 3.9407...  0.3378 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13421...  Training loss: 3.9422...  0.3388 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13422...  Training loss: 3.9408...  0.3406 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13423...  Training loss: 3.9137...  0.3401 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13424...  Training loss: 3.9822...  0.3412 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13425...  Training loss: 4.0253...  0.3403 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13426...  Training loss: 3.9963...  0.3400 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13427...  Training loss: 3.9855...  0.3432 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13428...  Training loss: 3.9261...  0.3434 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13429...  Training loss: 3.9457...  0.3437 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13430...  Training loss: 3.9464...  0.3413 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13431...  Training loss: 3.9308...  0.3448 sec/batch\n",
      "Epoch: 73/100...  Training Step: 13432...  Training loss: 3.9832...  0.3397 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13433...  Training loss: 3.9317...  0.3434 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13434...  Training loss: 3.7361...  0.3406 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13435...  Training loss: 3.7627...  0.3422 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13436...  Training loss: 3.8345...  0.3414 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13437...  Training loss: 3.7983...  0.3388 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13438...  Training loss: 3.7867...  0.3403 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13439...  Training loss: 3.9510...  0.3422 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13440...  Training loss: 3.9346...  0.3436 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13441...  Training loss: 3.9154...  0.3427 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13442...  Training loss: 3.9675...  0.3396 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13443...  Training loss: 3.9069...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13444...  Training loss: 3.9138...  0.3426 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13445...  Training loss: 3.9096...  0.3442 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13446...  Training loss: 3.8987...  0.3411 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13447...  Training loss: 3.9462...  0.3449 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13448...  Training loss: 3.9074...  0.3427 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13449...  Training loss: 3.9938...  0.3418 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13450...  Training loss: 3.9413...  0.3424 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13451...  Training loss: 4.0024...  0.3418 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13452...  Training loss: 4.0057...  0.3403 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13453...  Training loss: 3.9226...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13454...  Training loss: 3.8255...  0.3405 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13455...  Training loss: 3.9211...  0.3394 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13456...  Training loss: 3.9088...  0.3417 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13457...  Training loss: 3.9653...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13458...  Training loss: 3.9526...  0.3425 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13459...  Training loss: 3.9674...  0.3396 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13460...  Training loss: 3.9308...  0.3431 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13461...  Training loss: 3.9380...  0.3396 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13462...  Training loss: 4.0218...  0.3433 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13463...  Training loss: 4.0033...  0.3439 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13464...  Training loss: 3.9758...  0.3400 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13465...  Training loss: 3.8643...  0.3405 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13466...  Training loss: 3.9008...  0.3392 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13467...  Training loss: 3.9942...  0.3412 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13468...  Training loss: 4.0560...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13469...  Training loss: 4.0668...  0.3424 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13470...  Training loss: 4.0383...  0.3408 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13471...  Training loss: 4.0151...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13472...  Training loss: 4.0658...  0.3408 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13473...  Training loss: 4.0405...  0.3408 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13474...  Training loss: 4.0802...  0.3397 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13475...  Training loss: 4.1352...  0.3406 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13476...  Training loss: 4.0727...  0.3409 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/100...  Training Step: 13477...  Training loss: 4.0434...  0.3411 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13478...  Training loss: 4.0194...  0.3431 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13479...  Training loss: 4.0737...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13480...  Training loss: 3.9989...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13481...  Training loss: 4.0055...  0.3410 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13482...  Training loss: 4.0728...  0.3422 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13483...  Training loss: 4.0590...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13484...  Training loss: 4.1040...  0.3437 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13485...  Training loss: 4.0818...  0.3425 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13486...  Training loss: 4.0775...  0.3448 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13487...  Training loss: 4.0622...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13488...  Training loss: 4.0654...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13489...  Training loss: 3.9736...  0.3421 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13490...  Training loss: 4.0591...  0.3425 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13491...  Training loss: 3.9415...  0.3425 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13492...  Training loss: 3.9242...  0.3407 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13493...  Training loss: 3.9889...  0.3395 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13494...  Training loss: 4.0356...  0.3393 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13495...  Training loss: 4.0287...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13496...  Training loss: 4.0660...  0.3416 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13497...  Training loss: 4.0013...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13498...  Training loss: 3.9937...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13499...  Training loss: 4.0142...  0.3406 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13500...  Training loss: 3.9297...  0.3435 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13501...  Training loss: 3.9902...  0.3417 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13502...  Training loss: 4.0345...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13503...  Training loss: 3.9662...  0.3424 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13504...  Training loss: 3.8963...  0.3431 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13505...  Training loss: 3.9711...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13506...  Training loss: 3.9665...  0.3398 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13507...  Training loss: 3.9649...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13508...  Training loss: 3.9930...  0.3432 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13509...  Training loss: 4.0001...  0.3436 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13510...  Training loss: 3.9297...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13511...  Training loss: 3.9876...  0.3409 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13512...  Training loss: 3.9300...  0.3440 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13513...  Training loss: 3.9843...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13514...  Training loss: 3.9721...  0.3418 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13515...  Training loss: 3.9653...  0.3436 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13516...  Training loss: 3.9456...  0.3434 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13517...  Training loss: 3.9809...  0.3435 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13518...  Training loss: 4.0097...  0.3419 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13519...  Training loss: 3.9722...  0.3443 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13520...  Training loss: 3.9155...  0.3434 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13521...  Training loss: 4.0182...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13522...  Training loss: 4.0368...  0.3413 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13523...  Training loss: 3.9665...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13524...  Training loss: 3.9418...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13525...  Training loss: 3.9572...  0.3402 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13526...  Training loss: 3.9701...  0.3411 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13527...  Training loss: 4.0233...  0.3397 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13528...  Training loss: 4.0216...  0.3427 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13529...  Training loss: 3.9473...  0.3413 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13530...  Training loss: 3.9553...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13531...  Training loss: 3.9683...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13532...  Training loss: 3.9677...  0.3428 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13533...  Training loss: 4.0277...  0.3428 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13534...  Training loss: 3.9668...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13535...  Training loss: 3.9745...  0.3424 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13536...  Training loss: 3.8758...  0.3409 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13537...  Training loss: 4.0306...  0.3388 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13538...  Training loss: 3.9573...  0.3432 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13539...  Training loss: 3.9274...  0.3441 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13540...  Training loss: 3.9407...  0.3444 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13541...  Training loss: 3.9586...  0.3404 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13542...  Training loss: 3.9414...  0.3389 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13543...  Training loss: 4.0064...  0.3413 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13544...  Training loss: 3.9593...  0.3407 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13545...  Training loss: 3.9720...  0.3421 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13546...  Training loss: 3.8702...  0.3420 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13547...  Training loss: 3.9938...  0.3412 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13548...  Training loss: 3.9376...  0.3449 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13549...  Training loss: 3.9637...  0.3394 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13550...  Training loss: 3.8898...  0.3414 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13551...  Training loss: 3.9789...  0.3445 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13552...  Training loss: 3.9460...  0.3447 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13553...  Training loss: 4.0890...  0.3442 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13554...  Training loss: 4.0941...  0.3430 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13555...  Training loss: 4.0873...  0.3437 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13556...  Training loss: 3.9589...  0.3430 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13557...  Training loss: 3.9852...  0.3447 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13558...  Training loss: 4.0337...  0.3442 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13559...  Training loss: 4.0245...  0.3406 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13560...  Training loss: 4.0345...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13561...  Training loss: 4.1068...  0.3428 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13562...  Training loss: 4.0942...  0.3397 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13563...  Training loss: 4.1183...  0.3396 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13564...  Training loss: 4.0330...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13565...  Training loss: 4.0531...  0.3411 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13566...  Training loss: 4.0415...  0.3396 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13567...  Training loss: 4.0756...  0.3407 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13568...  Training loss: 3.9967...  0.3417 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13569...  Training loss: 4.0185...  0.3438 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13570...  Training loss: 4.0117...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13571...  Training loss: 4.0653...  0.3436 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13572...  Training loss: 3.9815...  0.3419 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74/100...  Training Step: 13573...  Training loss: 4.0457...  0.3431 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13574...  Training loss: 4.0750...  0.3438 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13575...  Training loss: 3.9903...  0.3404 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13576...  Training loss: 4.0017...  0.3426 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13577...  Training loss: 4.0113...  0.3391 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13578...  Training loss: 4.0095...  0.3429 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13579...  Training loss: 3.9957...  0.3438 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13580...  Training loss: 4.0413...  0.3407 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13581...  Training loss: 4.0347...  0.3405 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13582...  Training loss: 3.9402...  0.3415 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13583...  Training loss: 3.9773...  0.3419 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13584...  Training loss: 3.9778...  0.3395 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13585...  Training loss: 4.0023...  0.3435 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13586...  Training loss: 4.0234...  0.3422 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13587...  Training loss: 4.0387...  0.3408 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13588...  Training loss: 4.0306...  0.3392 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13589...  Training loss: 4.0105...  0.3395 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13590...  Training loss: 4.0603...  0.3393 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13591...  Training loss: 3.9275...  0.3448 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13592...  Training loss: 3.9532...  0.3431 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13593...  Training loss: 3.9874...  0.3426 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13594...  Training loss: 4.0085...  0.3435 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13595...  Training loss: 3.9745...  0.3393 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13596...  Training loss: 3.9112...  0.3399 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13597...  Training loss: 4.0332...  0.3435 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13598...  Training loss: 3.9452...  0.3405 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13599...  Training loss: 3.9333...  0.3421 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13600...  Training loss: 3.9669...  0.3427 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13601...  Training loss: 4.0304...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13602...  Training loss: 4.0299...  0.3410 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13603...  Training loss: 3.9707...  0.3395 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13604...  Training loss: 3.9438...  0.3400 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13605...  Training loss: 3.9236...  0.3387 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13606...  Training loss: 3.9189...  0.3423 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13607...  Training loss: 3.8939...  0.3448 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13608...  Training loss: 3.9657...  0.3414 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13609...  Training loss: 4.0213...  0.3419 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13610...  Training loss: 3.9699...  0.3415 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13611...  Training loss: 3.9689...  0.3411 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13612...  Training loss: 3.9421...  0.3401 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13613...  Training loss: 3.9201...  0.3414 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13614...  Training loss: 3.9452...  0.3407 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13615...  Training loss: 3.9244...  0.3398 sec/batch\n",
      "Epoch: 74/100...  Training Step: 13616...  Training loss: 3.9639...  0.3444 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13617...  Training loss: 3.8889...  0.3399 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13618...  Training loss: 3.7265...  0.3443 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13619...  Training loss: 3.7431...  0.3412 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13620...  Training loss: 3.8465...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13621...  Training loss: 3.8031...  0.3439 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13622...  Training loss: 3.7607...  0.3398 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13623...  Training loss: 3.9092...  0.3399 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13624...  Training loss: 3.9415...  0.3423 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13625...  Training loss: 3.9194...  0.3431 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13626...  Training loss: 3.9387...  0.3392 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13627...  Training loss: 3.9181...  0.3420 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13628...  Training loss: 3.8964...  0.3438 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13629...  Training loss: 3.9129...  0.3444 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13630...  Training loss: 3.8753...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13631...  Training loss: 3.9346...  0.3454 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13632...  Training loss: 3.9283...  0.3433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13633...  Training loss: 3.9905...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13634...  Training loss: 3.9198...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13635...  Training loss: 3.9680...  0.3415 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13636...  Training loss: 3.9904...  0.3431 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13637...  Training loss: 3.9417...  0.3415 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13638...  Training loss: 3.8316...  0.3393 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13639...  Training loss: 3.8705...  0.3416 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13640...  Training loss: 3.8823...  0.3434 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13641...  Training loss: 3.9695...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13642...  Training loss: 3.9430...  0.3446 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13643...  Training loss: 3.9449...  0.3396 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13644...  Training loss: 3.9030...  0.3459 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13645...  Training loss: 3.9532...  0.3436 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13646...  Training loss: 4.0435...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13647...  Training loss: 4.0061...  0.3405 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13648...  Training loss: 3.9725...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13649...  Training loss: 3.8747...  0.3431 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13650...  Training loss: 3.9152...  0.3419 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13651...  Training loss: 3.9949...  0.3418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13652...  Training loss: 4.0374...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13653...  Training loss: 4.0592...  0.3392 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13654...  Training loss: 4.0016...  0.3408 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13655...  Training loss: 4.0497...  0.3394 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13656...  Training loss: 4.0472...  0.3441 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13657...  Training loss: 4.0143...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13658...  Training loss: 4.0682...  0.3407 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13659...  Training loss: 4.1077...  0.3393 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13660...  Training loss: 4.0360...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13661...  Training loss: 4.0480...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13662...  Training loss: 3.9898...  0.3396 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13663...  Training loss: 4.0445...  0.3412 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13664...  Training loss: 3.9901...  0.3388 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13665...  Training loss: 4.0240...  0.3406 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13666...  Training loss: 4.0819...  0.3416 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13667...  Training loss: 4.0808...  0.3401 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13668...  Training loss: 4.0746...  0.3396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/100...  Training Step: 13669...  Training loss: 4.0616...  0.3411 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13670...  Training loss: 4.0690...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13671...  Training loss: 4.0759...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13672...  Training loss: 4.0667...  0.3421 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13673...  Training loss: 3.9660...  0.3440 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13674...  Training loss: 4.0225...  0.3429 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13675...  Training loss: 3.9167...  0.3409 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13676...  Training loss: 3.9265...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13677...  Training loss: 3.9770...  0.3434 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13678...  Training loss: 3.9817...  0.3406 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13679...  Training loss: 3.9791...  0.3438 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13680...  Training loss: 4.0630...  0.3397 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13681...  Training loss: 3.9721...  0.3412 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13682...  Training loss: 3.9923...  0.3443 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13683...  Training loss: 3.9663...  0.3429 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13684...  Training loss: 3.9227...  0.3420 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13685...  Training loss: 4.0138...  0.3403 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13686...  Training loss: 4.0320...  0.3391 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13687...  Training loss: 3.9509...  0.3409 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13688...  Training loss: 3.8984...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13689...  Training loss: 3.9424...  0.3423 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13690...  Training loss: 3.9216...  0.3441 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13691...  Training loss: 3.9527...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13692...  Training loss: 3.9541...  0.3440 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13693...  Training loss: 3.9785...  0.3430 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13694...  Training loss: 3.9289...  0.3432 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13695...  Training loss: 4.0004...  0.3419 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13696...  Training loss: 3.9307...  0.3390 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13697...  Training loss: 3.9695...  0.3426 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13698...  Training loss: 3.9329...  0.3436 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13699...  Training loss: 3.9784...  0.3396 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13700...  Training loss: 3.9251...  0.3409 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13701...  Training loss: 3.9530...  0.3418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13702...  Training loss: 4.0033...  0.3440 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13703...  Training loss: 3.9493...  0.3402 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13704...  Training loss: 3.8931...  0.3443 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13705...  Training loss: 4.0105...  0.3403 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13706...  Training loss: 4.0331...  0.3449 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13707...  Training loss: 3.9479...  0.3423 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13708...  Training loss: 3.9207...  0.3400 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13709...  Training loss: 3.9332...  0.3403 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13710...  Training loss: 3.9501...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13711...  Training loss: 4.0222...  0.3430 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13712...  Training loss: 4.0082...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13713...  Training loss: 3.9541...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13714...  Training loss: 3.9031...  0.3412 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13715...  Training loss: 3.9148...  0.3407 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13716...  Training loss: 3.9642...  0.3411 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13717...  Training loss: 3.9528...  0.3420 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13718...  Training loss: 3.9057...  0.3444 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13719...  Training loss: 3.9762...  0.3433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13720...  Training loss: 3.8895...  0.3436 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13721...  Training loss: 3.9945...  0.3434 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13722...  Training loss: 3.9565...  0.3400 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13723...  Training loss: 3.9535...  0.3399 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13724...  Training loss: 3.9528...  0.3386 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13725...  Training loss: 3.9408...  0.3430 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13726...  Training loss: 3.9540...  0.3446 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13727...  Training loss: 3.9991...  0.3416 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13728...  Training loss: 3.9654...  0.3436 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13729...  Training loss: 3.9612...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13730...  Training loss: 3.9173...  0.3381 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13731...  Training loss: 4.0104...  0.3425 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13732...  Training loss: 3.9419...  0.3419 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13733...  Training loss: 3.9710...  0.3437 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13734...  Training loss: 3.9004...  0.3425 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13735...  Training loss: 3.9643...  0.3410 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13736...  Training loss: 3.9177...  0.3507 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13737...  Training loss: 4.0685...  0.3392 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13738...  Training loss: 4.0520...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13739...  Training loss: 4.0562...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13740...  Training loss: 3.9473...  0.3417 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13741...  Training loss: 3.9649...  0.3403 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13742...  Training loss: 4.0129...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13743...  Training loss: 4.0056...  0.3439 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13744...  Training loss: 3.9857...  0.3437 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13745...  Training loss: 4.0242...  0.3440 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13746...  Training loss: 4.0578...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13747...  Training loss: 4.0661...  0.3426 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13748...  Training loss: 4.0179...  0.3433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13749...  Training loss: 4.0692...  0.3443 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13750...  Training loss: 4.0302...  0.3399 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13751...  Training loss: 4.0726...  0.3443 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13752...  Training loss: 4.0473...  0.3435 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13753...  Training loss: 3.9986...  0.3402 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13754...  Training loss: 4.0490...  0.3395 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13755...  Training loss: 4.0503...  0.3391 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13756...  Training loss: 3.9549...  0.3412 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13757...  Training loss: 4.0054...  0.3433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13758...  Training loss: 4.0693...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13759...  Training loss: 4.0142...  0.3428 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13760...  Training loss: 3.9679...  0.3424 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13761...  Training loss: 4.0011...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13762...  Training loss: 4.0166...  0.3418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13763...  Training loss: 4.0054...  0.3407 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13764...  Training loss: 4.0435...  0.3416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/100...  Training Step: 13765...  Training loss: 4.0519...  0.3385 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13766...  Training loss: 3.9304...  0.3429 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13767...  Training loss: 4.0030...  0.3444 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13768...  Training loss: 3.9620...  0.3421 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13769...  Training loss: 3.9892...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13770...  Training loss: 4.0214...  0.3396 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13771...  Training loss: 4.0207...  0.3456 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13772...  Training loss: 4.0483...  0.3429 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13773...  Training loss: 4.0322...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13774...  Training loss: 4.0745...  0.3393 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13775...  Training loss: 3.9471...  0.3424 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13776...  Training loss: 3.9339...  0.3417 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13777...  Training loss: 3.9618...  0.3440 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13778...  Training loss: 4.0155...  0.3484 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13779...  Training loss: 3.9235...  0.3442 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13780...  Training loss: 3.8972...  0.3437 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13781...  Training loss: 4.0055...  0.3444 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13782...  Training loss: 3.9678...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13783...  Training loss: 3.9534...  0.3417 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13784...  Training loss: 3.9747...  0.3427 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13785...  Training loss: 4.0456...  0.3398 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13786...  Training loss: 4.0622...  0.3446 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13787...  Training loss: 4.0165...  0.3420 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13788...  Training loss: 3.9453...  0.3433 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13789...  Training loss: 3.9100...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13790...  Training loss: 3.9161...  0.3414 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13791...  Training loss: 3.8787...  0.3446 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13792...  Training loss: 3.9597...  0.3425 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13793...  Training loss: 4.0040...  0.3418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13794...  Training loss: 3.9790...  0.3432 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13795...  Training loss: 3.9693...  0.3429 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13796...  Training loss: 3.9567...  0.3423 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13797...  Training loss: 3.9417...  0.3418 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13798...  Training loss: 3.9373...  0.3404 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13799...  Training loss: 3.9091...  0.3386 sec/batch\n",
      "Epoch: 75/100...  Training Step: 13800...  Training loss: 3.9669...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13801...  Training loss: 3.8921...  0.3430 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13802...  Training loss: 3.7509...  0.3426 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13803...  Training loss: 3.7009...  0.3434 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13804...  Training loss: 3.8035...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13805...  Training loss: 3.8062...  0.3398 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13806...  Training loss: 3.7617...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13807...  Training loss: 3.9581...  0.3429 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13808...  Training loss: 3.9104...  0.3421 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13809...  Training loss: 3.8929...  0.3420 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13810...  Training loss: 3.9508...  0.3417 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13811...  Training loss: 3.8996...  0.3402 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13812...  Training loss: 3.8838...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13813...  Training loss: 3.8979...  0.3422 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13814...  Training loss: 3.8603...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13815...  Training loss: 3.9195...  0.3443 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13816...  Training loss: 3.9196...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13817...  Training loss: 3.9572...  0.3421 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13818...  Training loss: 3.8897...  0.3430 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13819...  Training loss: 3.9559...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13820...  Training loss: 3.9727...  0.3434 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13821...  Training loss: 3.8964...  0.3406 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13822...  Training loss: 3.8012...  0.3408 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13823...  Training loss: 3.8576...  0.3405 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13824...  Training loss: 3.8823...  0.3431 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13825...  Training loss: 3.9728...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13826...  Training loss: 3.9658...  0.3423 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13827...  Training loss: 3.9509...  0.3403 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13828...  Training loss: 3.9315...  0.3400 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13829...  Training loss: 3.9271...  0.3408 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13830...  Training loss: 4.0535...  0.3428 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13831...  Training loss: 4.0150...  0.3419 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13832...  Training loss: 3.9556...  0.3431 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13833...  Training loss: 3.8437...  0.3391 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13834...  Training loss: 3.8744...  0.3434 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13835...  Training loss: 3.9606...  0.3390 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13836...  Training loss: 4.0174...  0.3441 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13837...  Training loss: 4.0475...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13838...  Training loss: 3.9994...  0.3414 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13839...  Training loss: 3.9991...  0.3416 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13840...  Training loss: 4.0306...  0.3447 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13841...  Training loss: 4.0253...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13842...  Training loss: 4.0492...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13843...  Training loss: 4.1134...  0.3402 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13844...  Training loss: 4.0376...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13845...  Training loss: 4.0184...  0.3429 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13846...  Training loss: 3.9740...  0.3425 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13847...  Training loss: 4.0423...  0.3403 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13848...  Training loss: 4.0123...  0.3428 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13849...  Training loss: 3.9729...  0.3393 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13850...  Training loss: 4.0234...  0.3392 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13851...  Training loss: 4.0568...  0.3400 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13852...  Training loss: 4.0451...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13853...  Training loss: 4.0258...  0.3406 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13854...  Training loss: 4.0214...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13855...  Training loss: 4.0530...  0.3399 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13856...  Training loss: 4.0534...  0.3415 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13857...  Training loss: 3.9708...  0.3444 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13858...  Training loss: 4.0148...  0.3417 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13859...  Training loss: 3.9317...  0.3447 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13860...  Training loss: 3.9034...  0.3400 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76/100...  Training Step: 13861...  Training loss: 3.9480...  0.3429 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13862...  Training loss: 3.9899...  0.3414 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13863...  Training loss: 3.9773...  0.3429 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13864...  Training loss: 4.0450...  0.3421 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13865...  Training loss: 3.9655...  0.3425 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13866...  Training loss: 3.9481...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13867...  Training loss: 3.9455...  0.3442 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13868...  Training loss: 3.9056...  0.3429 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13869...  Training loss: 4.0025...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13870...  Training loss: 4.0241...  0.3397 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13871...  Training loss: 3.9309...  0.3453 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13872...  Training loss: 3.8885...  0.3428 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13873...  Training loss: 3.9364...  0.3419 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13874...  Training loss: 3.9392...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13875...  Training loss: 3.9508...  0.3426 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13876...  Training loss: 3.9664...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13877...  Training loss: 3.9551...  0.3420 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13878...  Training loss: 3.8707...  0.3448 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13879...  Training loss: 3.9604...  0.3442 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13880...  Training loss: 3.8859...  0.3434 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13881...  Training loss: 3.9459...  0.3456 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13882...  Training loss: 3.9205...  0.3419 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13883...  Training loss: 3.9659...  0.3396 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13884...  Training loss: 3.9093...  0.3408 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13885...  Training loss: 3.9496...  0.3427 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13886...  Training loss: 3.9980...  0.3431 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13887...  Training loss: 3.9124...  0.3415 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13888...  Training loss: 3.8731...  0.3445 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13889...  Training loss: 4.0020...  0.3426 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13890...  Training loss: 4.0653...  0.3434 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13891...  Training loss: 3.9273...  0.3422 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13892...  Training loss: 3.9066...  0.3415 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13893...  Training loss: 3.9426...  0.3384 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13894...  Training loss: 3.8937...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13895...  Training loss: 3.9654...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13896...  Training loss: 3.9926...  0.3417 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13897...  Training loss: 3.9355...  0.3445 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13898...  Training loss: 3.9268...  0.3388 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13899...  Training loss: 3.9343...  0.3420 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13900...  Training loss: 3.9703...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13901...  Training loss: 3.9756...  0.3420 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13902...  Training loss: 3.9065...  0.3390 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13903...  Training loss: 3.9383...  0.3416 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13904...  Training loss: 3.8275...  0.3416 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13905...  Training loss: 3.9635...  0.3417 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13906...  Training loss: 3.9612...  0.3383 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13907...  Training loss: 3.9219...  0.3409 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13908...  Training loss: 3.9363...  0.3428 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13909...  Training loss: 3.9248...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13910...  Training loss: 3.9325...  0.3447 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13911...  Training loss: 3.9749...  0.3442 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13912...  Training loss: 3.9369...  0.3403 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13913...  Training loss: 3.9074...  0.3423 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13914...  Training loss: 3.8733...  0.3397 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13915...  Training loss: 3.9879...  0.3408 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13916...  Training loss: 3.9260...  0.3390 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13917...  Training loss: 3.9339...  0.3456 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13918...  Training loss: 3.8643...  0.3477 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13919...  Training loss: 3.9649...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13920...  Training loss: 3.9254...  0.3430 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13921...  Training loss: 4.0350...  0.3430 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13922...  Training loss: 4.0197...  0.3477 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13923...  Training loss: 4.0244...  0.3420 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13924...  Training loss: 3.9128...  0.3388 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13925...  Training loss: 3.9783...  0.3393 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13926...  Training loss: 4.0089...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13927...  Training loss: 3.9583...  0.3399 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13928...  Training loss: 4.0063...  0.3418 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13929...  Training loss: 4.0477...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13930...  Training loss: 4.0471...  0.3441 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13931...  Training loss: 4.0437...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13932...  Training loss: 3.9682...  0.3449 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13933...  Training loss: 4.0030...  0.3388 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13934...  Training loss: 3.9854...  0.3397 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13935...  Training loss: 4.0554...  0.3411 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13936...  Training loss: 3.9871...  0.3422 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13937...  Training loss: 4.0032...  0.3389 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13938...  Training loss: 4.0088...  0.3403 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13939...  Training loss: 4.0759...  0.3395 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13940...  Training loss: 3.9177...  0.3402 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13941...  Training loss: 4.0418...  0.3426 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13942...  Training loss: 4.0180...  0.3432 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13943...  Training loss: 3.9587...  0.3431 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13944...  Training loss: 3.9397...  0.3407 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13945...  Training loss: 3.9868...  0.3423 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13946...  Training loss: 3.9680...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13947...  Training loss: 3.9794...  0.3440 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13948...  Training loss: 4.0076...  0.3428 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13949...  Training loss: 4.0188...  0.3427 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13950...  Training loss: 3.9399...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13951...  Training loss: 3.9859...  0.3417 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13952...  Training loss: 3.9478...  0.3391 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13953...  Training loss: 3.9688...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13954...  Training loss: 3.9850...  0.3443 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13955...  Training loss: 4.0041...  0.3426 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13956...  Training loss: 3.9824...  0.3407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76/100...  Training Step: 13957...  Training loss: 4.0333...  0.3412 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13958...  Training loss: 4.0390...  0.3419 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13959...  Training loss: 3.9118...  0.3442 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13960...  Training loss: 3.9521...  0.3394 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13961...  Training loss: 3.9710...  0.3439 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13962...  Training loss: 4.0379...  0.3440 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13963...  Training loss: 3.9268...  0.3406 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13964...  Training loss: 3.8914...  0.3421 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13965...  Training loss: 3.9773...  0.3390 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13966...  Training loss: 3.9319...  0.3436 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13967...  Training loss: 3.9221...  0.3421 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13968...  Training loss: 3.9219...  0.3407 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13969...  Training loss: 4.0501...  0.3431 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13970...  Training loss: 4.0477...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13971...  Training loss: 3.9778...  0.3439 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13972...  Training loss: 3.9329...  0.3427 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13973...  Training loss: 3.9013...  0.3442 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13974...  Training loss: 3.8685...  0.3399 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13975...  Training loss: 3.8740...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13976...  Training loss: 3.9892...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13977...  Training loss: 4.0016...  0.3399 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13978...  Training loss: 3.9575...  0.3392 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13979...  Training loss: 3.9795...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13980...  Training loss: 3.9586...  0.3400 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13981...  Training loss: 3.9254...  0.3424 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13982...  Training loss: 3.9535...  0.3435 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13983...  Training loss: 3.9182...  0.3437 sec/batch\n",
      "Epoch: 76/100...  Training Step: 13984...  Training loss: 3.9548...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13985...  Training loss: 3.8757...  0.3427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13986...  Training loss: 3.7028...  0.3423 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13987...  Training loss: 3.6748...  0.3429 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13988...  Training loss: 3.8071...  0.3398 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13989...  Training loss: 3.7683...  0.3427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13990...  Training loss: 3.7639...  0.3402 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13991...  Training loss: 3.8991...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13992...  Training loss: 3.9461...  0.3403 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13993...  Training loss: 3.9110...  0.3430 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13994...  Training loss: 3.9396...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13995...  Training loss: 3.9191...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13996...  Training loss: 3.8706...  0.3448 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13997...  Training loss: 3.8990...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13998...  Training loss: 3.8796...  0.3420 sec/batch\n",
      "Epoch: 77/100...  Training Step: 13999...  Training loss: 3.9046...  0.3428 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14000...  Training loss: 3.9387...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14001...  Training loss: 3.9765...  0.3836 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14002...  Training loss: 3.8942...  0.3458 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14003...  Training loss: 3.9742...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14004...  Training loss: 3.9568...  0.3438 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14005...  Training loss: 3.8977...  0.3448 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14006...  Training loss: 3.7973...  0.3450 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14007...  Training loss: 3.8563...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14008...  Training loss: 3.8741...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14009...  Training loss: 3.9541...  0.3457 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14010...  Training loss: 3.9124...  0.3421 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14011...  Training loss: 3.9336...  0.3420 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14012...  Training loss: 3.9026...  0.3420 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14013...  Training loss: 3.9334...  0.3434 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14014...  Training loss: 4.0154...  0.3404 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14015...  Training loss: 3.9792...  0.3434 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14016...  Training loss: 3.9472...  0.3439 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14017...  Training loss: 3.8023...  0.3456 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14018...  Training loss: 3.8603...  0.3446 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14019...  Training loss: 3.9499...  0.3452 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14020...  Training loss: 4.0146...  0.3435 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14021...  Training loss: 4.0270...  0.3460 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14022...  Training loss: 3.9995...  0.3436 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14023...  Training loss: 3.9994...  0.3417 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14024...  Training loss: 4.0042...  0.3452 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14025...  Training loss: 4.0179...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14026...  Training loss: 4.0514...  0.3415 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14027...  Training loss: 4.1248...  0.3421 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14028...  Training loss: 4.0192...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14029...  Training loss: 4.0110...  0.3444 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14030...  Training loss: 4.0009...  0.3417 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14031...  Training loss: 4.0317...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14032...  Training loss: 3.9646...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14033...  Training loss: 4.0237...  0.3412 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14034...  Training loss: 4.0038...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14035...  Training loss: 4.0293...  0.3449 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14036...  Training loss: 4.0606...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14037...  Training loss: 4.0428...  0.3396 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14038...  Training loss: 4.0089...  0.3426 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14039...  Training loss: 4.0339...  0.3422 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14040...  Training loss: 4.0130...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14041...  Training loss: 3.9245...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14042...  Training loss: 3.9893...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14043...  Training loss: 3.9053...  0.3405 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14044...  Training loss: 3.8838...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14045...  Training loss: 3.9391...  0.3427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14046...  Training loss: 3.9828...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14047...  Training loss: 3.9781...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14048...  Training loss: 4.0380...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14049...  Training loss: 3.9822...  0.3414 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14050...  Training loss: 3.9348...  0.3428 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14051...  Training loss: 3.9737...  0.3455 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14052...  Training loss: 3.9025...  0.3411 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100...  Training Step: 14053...  Training loss: 3.9477...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14054...  Training loss: 4.0228...  0.3448 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14055...  Training loss: 3.9082...  0.3407 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14056...  Training loss: 3.8697...  0.3449 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14057...  Training loss: 3.9130...  0.3444 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14058...  Training loss: 3.9130...  0.3448 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14059...  Training loss: 3.9241...  0.3467 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14060...  Training loss: 3.9665...  0.3405 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14061...  Training loss: 3.9852...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14062...  Training loss: 3.9016...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14063...  Training loss: 3.9913...  0.3420 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14064...  Training loss: 3.9111...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14065...  Training loss: 3.9653...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14066...  Training loss: 3.9428...  0.3446 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14067...  Training loss: 3.9188...  0.3412 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14068...  Training loss: 3.8958...  0.3409 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14069...  Training loss: 3.9276...  0.3427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14070...  Training loss: 3.9445...  0.3407 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14071...  Training loss: 3.8954...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14072...  Training loss: 3.8577...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14073...  Training loss: 3.9592...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14074...  Training loss: 4.0319...  0.3415 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14075...  Training loss: 3.9543...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14076...  Training loss: 3.8776...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14077...  Training loss: 3.9313...  0.3399 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14078...  Training loss: 3.8801...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14079...  Training loss: 3.9790...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14080...  Training loss: 4.0006...  0.3419 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14081...  Training loss: 3.9254...  0.3421 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14082...  Training loss: 3.8854...  0.3439 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14083...  Training loss: 3.9236...  0.3426 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14084...  Training loss: 3.9295...  0.3411 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14085...  Training loss: 3.9654...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14086...  Training loss: 3.9053...  0.3456 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14087...  Training loss: 3.9519...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14088...  Training loss: 3.8626...  0.3439 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14089...  Training loss: 3.9512...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14090...  Training loss: 3.9170...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14091...  Training loss: 3.8901...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14092...  Training loss: 3.9322...  0.3427 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14093...  Training loss: 3.9237...  0.3446 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14094...  Training loss: 3.9511...  0.3454 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14095...  Training loss: 3.9635...  0.3445 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14096...  Training loss: 3.9226...  0.3436 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14097...  Training loss: 3.8956...  0.3412 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14098...  Training loss: 3.8560...  0.3422 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14099...  Training loss: 3.9561...  0.3443 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14100...  Training loss: 3.9192...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14101...  Training loss: 3.9479...  0.3411 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14102...  Training loss: 3.8531...  0.3428 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14103...  Training loss: 3.9510...  0.3449 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14104...  Training loss: 3.9423...  0.3429 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14105...  Training loss: 4.0745...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14106...  Training loss: 4.0006...  0.3412 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14107...  Training loss: 4.0109...  0.3404 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14108...  Training loss: 3.9017...  0.3412 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14109...  Training loss: 3.9468...  0.3453 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14110...  Training loss: 3.9718...  0.3439 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14111...  Training loss: 3.9466...  0.3419 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14112...  Training loss: 3.9495...  0.3448 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14113...  Training loss: 4.0414...  0.3430 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14114...  Training loss: 4.0174...  0.3450 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14115...  Training loss: 4.0111...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14116...  Training loss: 3.9446...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14117...  Training loss: 3.9788...  0.3444 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14118...  Training loss: 3.9462...  0.3423 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14119...  Training loss: 4.0073...  0.3410 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14120...  Training loss: 3.9663...  0.3452 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14121...  Training loss: 3.9803...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14122...  Training loss: 3.9890...  0.3421 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14123...  Training loss: 4.0334...  0.3418 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14124...  Training loss: 3.9196...  0.3411 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14125...  Training loss: 4.0037...  0.3408 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14126...  Training loss: 4.0178...  0.3429 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14127...  Training loss: 3.9311...  0.3423 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14128...  Training loss: 3.9225...  0.3398 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14129...  Training loss: 3.9544...  0.3435 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14130...  Training loss: 3.9578...  0.3429 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14131...  Training loss: 3.9385...  0.3415 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14132...  Training loss: 4.0148...  0.3442 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14133...  Training loss: 4.0141...  0.3437 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14134...  Training loss: 3.9195...  0.3446 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14135...  Training loss: 3.9667...  0.3449 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14136...  Training loss: 3.9196...  0.3428 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14137...  Training loss: 3.9600...  0.3436 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14138...  Training loss: 3.9672...  0.3403 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14139...  Training loss: 3.9914...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14140...  Training loss: 3.9827...  0.3425 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14141...  Training loss: 3.9666...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14142...  Training loss: 4.0162...  0.3410 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14143...  Training loss: 3.9321...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14144...  Training loss: 3.9388...  0.3403 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14145...  Training loss: 3.9595...  0.3420 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14146...  Training loss: 3.9978...  0.3449 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14147...  Training loss: 3.9342...  0.3441 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14148...  Training loss: 3.8818...  0.3453 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77/100...  Training Step: 14149...  Training loss: 3.9881...  0.3457 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14150...  Training loss: 3.9062...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14151...  Training loss: 3.9035...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14152...  Training loss: 3.8801...  0.3447 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14153...  Training loss: 3.9881...  0.3409 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14154...  Training loss: 3.9984...  0.3410 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14155...  Training loss: 3.9323...  0.3424 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14156...  Training loss: 3.9183...  0.3458 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14157...  Training loss: 3.8742...  0.3422 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14158...  Training loss: 3.8785...  0.3403 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14159...  Training loss: 3.8361...  0.3430 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14160...  Training loss: 3.9541...  0.3421 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14161...  Training loss: 3.9858...  0.3436 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14162...  Training loss: 3.9725...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14163...  Training loss: 3.9550...  0.3419 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14164...  Training loss: 3.9520...  0.3437 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14165...  Training loss: 3.9234...  0.3440 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14166...  Training loss: 3.9207...  0.3444 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14167...  Training loss: 3.8964...  0.3432 sec/batch\n",
      "Epoch: 77/100...  Training Step: 14168...  Training loss: 3.9417...  0.3411 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14169...  Training loss: 3.8581...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14170...  Training loss: 3.7113...  0.3416 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14171...  Training loss: 3.6743...  0.3449 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14172...  Training loss: 3.7967...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14173...  Training loss: 3.7391...  0.3416 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14174...  Training loss: 3.7135...  0.3414 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14175...  Training loss: 3.8850...  0.3410 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14176...  Training loss: 3.8986...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14177...  Training loss: 3.9227...  0.3428 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14178...  Training loss: 3.9068...  0.3429 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14179...  Training loss: 3.8976...  0.3434 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14180...  Training loss: 3.8814...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14181...  Training loss: 3.8986...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14182...  Training loss: 3.8535...  0.3447 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14183...  Training loss: 3.9173...  0.3432 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14184...  Training loss: 3.9058...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14185...  Training loss: 3.9635...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14186...  Training loss: 3.8915...  0.3404 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14187...  Training loss: 3.9553...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14188...  Training loss: 3.9246...  0.3434 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14189...  Training loss: 3.8630...  0.3426 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14190...  Training loss: 3.7750...  0.3417 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14191...  Training loss: 3.8321...  0.3421 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14192...  Training loss: 3.8450...  0.3417 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14193...  Training loss: 3.9196...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14194...  Training loss: 3.8926...  0.3440 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14195...  Training loss: 3.9138...  0.3456 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14196...  Training loss: 3.8712...  0.3410 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14197...  Training loss: 3.9096...  0.3422 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14198...  Training loss: 4.0055...  0.3448 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14199...  Training loss: 3.9952...  0.3410 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14200...  Training loss: 3.9233...  0.3451 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14201...  Training loss: 3.8180...  0.3457 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14202...  Training loss: 3.8279...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14203...  Training loss: 3.9461...  0.3424 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14204...  Training loss: 3.9926...  0.3416 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14205...  Training loss: 4.0095...  0.3457 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14206...  Training loss: 3.9804...  0.3445 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14207...  Training loss: 4.0109...  0.3419 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14208...  Training loss: 4.0202...  0.3444 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14209...  Training loss: 4.0029...  0.3417 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14210...  Training loss: 4.0181...  0.3436 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14211...  Training loss: 4.1114...  0.3418 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14212...  Training loss: 4.0144...  0.3444 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14213...  Training loss: 3.9918...  0.3463 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14214...  Training loss: 3.9698...  0.3450 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14215...  Training loss: 4.0123...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14216...  Training loss: 3.9665...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14217...  Training loss: 3.9968...  0.3432 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14218...  Training loss: 4.0417...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14219...  Training loss: 4.0650...  0.3421 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14220...  Training loss: 4.0619...  0.3409 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14221...  Training loss: 4.0314...  0.3436 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14222...  Training loss: 4.0218...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14223...  Training loss: 4.0186...  0.3420 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14224...  Training loss: 3.9827...  0.3426 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14225...  Training loss: 3.9284...  0.3418 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14226...  Training loss: 3.9886...  0.3410 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14227...  Training loss: 3.8734...  0.3430 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14228...  Training loss: 3.8498...  0.3434 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14229...  Training loss: 3.9218...  0.3425 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14230...  Training loss: 3.9711...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14231...  Training loss: 3.9200...  0.3417 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14232...  Training loss: 4.0175...  0.3412 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14233...  Training loss: 3.9496...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14234...  Training loss: 3.9126...  0.3420 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14235...  Training loss: 3.9458...  0.3456 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14236...  Training loss: 3.9059...  0.3433 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14237...  Training loss: 3.9602...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14238...  Training loss: 3.9796...  0.3406 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14239...  Training loss: 3.9006...  0.3439 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14240...  Training loss: 3.8512...  0.3425 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14241...  Training loss: 3.8727...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14242...  Training loss: 3.8840...  0.3427 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14243...  Training loss: 3.9261...  0.3452 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14244...  Training loss: 3.9286...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/100...  Training Step: 14245...  Training loss: 3.9282...  0.3434 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14246...  Training loss: 3.8562...  0.3431 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14247...  Training loss: 3.9328...  0.3450 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14248...  Training loss: 3.8813...  0.3424 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14249...  Training loss: 3.9678...  0.3404 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14250...  Training loss: 3.9376...  0.3451 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14251...  Training loss: 3.9354...  0.3454 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14252...  Training loss: 3.8792...  0.3448 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14253...  Training loss: 3.9007...  0.3455 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14254...  Training loss: 3.9571...  0.3452 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14255...  Training loss: 3.9021...  0.3433 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14256...  Training loss: 3.8732...  0.3406 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14257...  Training loss: 3.9591...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14258...  Training loss: 4.0217...  0.3416 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14259...  Training loss: 3.9425...  0.3450 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14260...  Training loss: 3.8605...  0.3469 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14261...  Training loss: 3.8876...  0.3431 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14262...  Training loss: 3.8943...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14263...  Training loss: 3.9671...  0.3449 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14264...  Training loss: 3.9630...  0.3413 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14265...  Training loss: 3.9035...  0.3460 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14266...  Training loss: 3.8697...  0.3403 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14267...  Training loss: 3.8914...  0.3392 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14268...  Training loss: 3.9073...  0.3406 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14269...  Training loss: 3.9543...  0.3462 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14270...  Training loss: 3.8758...  0.3444 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14271...  Training loss: 3.9181...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14272...  Training loss: 3.8140...  0.3411 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14273...  Training loss: 3.9457...  0.3448 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14274...  Training loss: 3.9339...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14275...  Training loss: 3.8991...  0.3450 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14276...  Training loss: 3.9003...  0.3449 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14277...  Training loss: 3.8879...  0.3442 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14278...  Training loss: 3.9179...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14279...  Training loss: 3.9590...  0.3455 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14280...  Training loss: 3.8933...  0.3431 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14281...  Training loss: 3.8996...  0.3449 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14282...  Training loss: 3.8327...  0.3440 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14283...  Training loss: 3.9703...  0.3444 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14284...  Training loss: 3.9060...  0.3418 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14285...  Training loss: 3.9317...  0.3419 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14286...  Training loss: 3.8530...  0.3452 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14287...  Training loss: 3.9350...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14288...  Training loss: 3.8859...  0.3407 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14289...  Training loss: 4.0563...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14290...  Training loss: 3.9940...  0.3418 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14291...  Training loss: 4.0167...  0.3434 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14292...  Training loss: 3.8625...  0.3404 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14293...  Training loss: 3.9359...  0.3456 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14294...  Training loss: 3.9692...  0.3445 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14295...  Training loss: 3.9263...  0.3438 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14296...  Training loss: 3.9390...  0.3405 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14297...  Training loss: 4.0121...  0.3462 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14298...  Training loss: 3.9827...  0.3429 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14299...  Training loss: 4.0065...  0.3427 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14300...  Training loss: 3.9314...  0.3448 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14301...  Training loss: 3.9757...  0.3446 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14302...  Training loss: 3.9508...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14303...  Training loss: 3.9960...  0.3449 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14304...  Training loss: 3.9337...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14305...  Training loss: 3.9507...  0.3445 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14306...  Training loss: 3.9600...  0.3436 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14307...  Training loss: 4.0291...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14308...  Training loss: 3.8827...  0.3439 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14309...  Training loss: 4.0034...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14310...  Training loss: 4.0248...  0.3415 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14311...  Training loss: 3.9373...  0.3452 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14312...  Training loss: 3.9228...  0.3443 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14313...  Training loss: 3.9624...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14314...  Training loss: 3.9238...  0.3407 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14315...  Training loss: 3.9030...  0.3428 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14316...  Training loss: 3.9560...  0.3421 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14317...  Training loss: 3.9792...  0.3418 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14318...  Training loss: 3.9107...  0.3443 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14319...  Training loss: 3.9380...  0.3448 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14320...  Training loss: 3.9615...  0.3443 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14321...  Training loss: 3.9864...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14322...  Training loss: 3.9705...  0.3419 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14323...  Training loss: 3.9880...  0.3436 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14324...  Training loss: 3.9736...  0.3431 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14325...  Training loss: 3.9738...  0.3428 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14326...  Training loss: 4.0190...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14327...  Training loss: 3.8995...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14328...  Training loss: 3.8914...  0.3453 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14329...  Training loss: 3.9457...  0.3435 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14330...  Training loss: 3.9666...  0.3427 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14331...  Training loss: 3.8958...  0.3407 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14332...  Training loss: 3.9041...  0.3408 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14333...  Training loss: 3.9544...  0.3439 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14334...  Training loss: 3.9162...  0.3404 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14335...  Training loss: 3.9109...  0.3406 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14336...  Training loss: 3.9256...  0.3441 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14337...  Training loss: 3.9811...  0.3454 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14338...  Training loss: 4.0158...  0.3432 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14339...  Training loss: 3.9224...  0.3457 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14340...  Training loss: 3.8682...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78/100...  Training Step: 14341...  Training loss: 3.8605...  0.3433 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14342...  Training loss: 3.8611...  0.3429 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14343...  Training loss: 3.8634...  0.3433 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14344...  Training loss: 3.9213...  0.3437 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14345...  Training loss: 3.9733...  0.3450 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14346...  Training loss: 3.9449...  0.3465 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14347...  Training loss: 3.9509...  0.3445 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14348...  Training loss: 3.9193...  0.3445 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14349...  Training loss: 3.8799...  0.3452 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14350...  Training loss: 3.9167...  0.3385 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14351...  Training loss: 3.8847...  0.3447 sec/batch\n",
      "Epoch: 78/100...  Training Step: 14352...  Training loss: 3.9568...  0.3457 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14353...  Training loss: 3.8339...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14354...  Training loss: 3.6994...  0.3429 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14355...  Training loss: 3.6673...  0.3417 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14356...  Training loss: 3.7577...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14357...  Training loss: 3.7260...  0.3437 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14358...  Training loss: 3.7103...  0.3420 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14359...  Training loss: 3.8589...  0.3453 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14360...  Training loss: 3.9181...  0.3454 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14361...  Training loss: 3.8727...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14362...  Training loss: 3.9174...  0.3402 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14363...  Training loss: 3.8705...  0.3453 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14364...  Training loss: 3.8821...  0.3443 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14365...  Training loss: 3.8880...  0.3478 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14366...  Training loss: 3.8392...  0.3465 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14367...  Training loss: 3.9113...  0.3426 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14368...  Training loss: 3.9207...  0.3411 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14369...  Training loss: 3.9445...  0.3400 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14370...  Training loss: 3.9006...  0.3419 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14371...  Training loss: 3.9729...  0.3440 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14372...  Training loss: 3.9514...  0.3445 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14373...  Training loss: 3.8888...  0.3418 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14374...  Training loss: 3.8036...  0.3428 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14375...  Training loss: 3.8339...  0.3444 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14376...  Training loss: 3.8673...  0.3412 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14377...  Training loss: 3.9127...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14378...  Training loss: 3.9432...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14379...  Training loss: 3.9143...  0.3459 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14380...  Training loss: 3.8697...  0.3455 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14381...  Training loss: 3.9007...  0.3440 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14382...  Training loss: 3.9919...  0.3445 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14383...  Training loss: 3.9651...  0.3448 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14384...  Training loss: 3.9069...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14385...  Training loss: 3.8016...  0.3457 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14386...  Training loss: 3.8426...  0.3409 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14387...  Training loss: 3.9446...  0.3441 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14388...  Training loss: 3.9969...  0.3452 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14389...  Training loss: 3.9908...  0.3415 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14390...  Training loss: 3.9731...  0.3427 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14391...  Training loss: 3.9754...  0.3434 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14392...  Training loss: 4.0176...  0.3438 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14393...  Training loss: 4.0007...  0.3448 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14394...  Training loss: 4.0065...  0.3436 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14395...  Training loss: 4.1004...  0.3406 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14396...  Training loss: 4.0278...  0.3458 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14397...  Training loss: 4.0020...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14398...  Training loss: 3.9674...  0.3428 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14399...  Training loss: 4.0074...  0.3397 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14400...  Training loss: 3.9528...  0.3418 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14401...  Training loss: 3.9749...  0.3444 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14402...  Training loss: 4.0091...  0.3414 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14403...  Training loss: 4.0343...  0.3413 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14404...  Training loss: 4.0813...  0.3424 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14405...  Training loss: 4.0810...  0.3460 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14406...  Training loss: 4.0211...  0.3404 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14407...  Training loss: 4.0570...  0.3432 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14408...  Training loss: 4.0170...  0.3446 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14409...  Training loss: 3.9308...  0.3436 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14410...  Training loss: 3.9630...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14411...  Training loss: 3.8857...  0.3458 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14412...  Training loss: 3.8718...  0.3426 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14413...  Training loss: 3.9215...  0.3448 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14414...  Training loss: 3.9401...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14415...  Training loss: 3.9275...  0.3414 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14416...  Training loss: 4.0298...  0.3438 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14417...  Training loss: 3.9336...  0.3449 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14418...  Training loss: 3.9455...  0.3441 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14419...  Training loss: 3.9629...  0.3415 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14420...  Training loss: 3.8824...  0.3425 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14421...  Training loss: 3.9626...  0.3446 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14422...  Training loss: 3.9966...  0.3408 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14423...  Training loss: 3.9108...  0.3450 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14424...  Training loss: 3.8555...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14425...  Training loss: 3.8957...  0.3444 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14426...  Training loss: 3.8690...  0.3416 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14427...  Training loss: 3.9130...  0.3404 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14428...  Training loss: 3.9005...  0.3408 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14429...  Training loss: 3.9368...  0.3456 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14430...  Training loss: 3.8621...  0.3445 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14431...  Training loss: 3.9280...  0.3413 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14432...  Training loss: 3.8732...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14433...  Training loss: 3.9415...  0.3411 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14434...  Training loss: 3.9307...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14435...  Training loss: 3.9122...  0.3408 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14436...  Training loss: 3.8666...  0.3458 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/100...  Training Step: 14437...  Training loss: 3.9361...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14438...  Training loss: 3.9357...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14439...  Training loss: 3.9014...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14440...  Training loss: 3.8259...  0.3463 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14441...  Training loss: 3.9146...  0.3441 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14442...  Training loss: 3.9948...  0.3452 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14443...  Training loss: 3.9159...  0.3430 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14444...  Training loss: 3.8331...  0.3460 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14445...  Training loss: 3.8851...  0.3437 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14446...  Training loss: 3.8910...  0.3418 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14447...  Training loss: 3.9572...  0.3425 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14448...  Training loss: 3.9705...  0.3442 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14449...  Training loss: 3.9172...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14450...  Training loss: 3.8492...  0.3451 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14451...  Training loss: 3.8709...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14452...  Training loss: 3.8876...  0.3438 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14453...  Training loss: 3.9105...  0.3406 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14454...  Training loss: 3.8620...  0.3442 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14455...  Training loss: 3.9078...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14456...  Training loss: 3.8261...  0.3436 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14457...  Training loss: 3.9287...  0.3407 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14458...  Training loss: 3.9087...  0.3440 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14459...  Training loss: 3.8514...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14460...  Training loss: 3.8610...  0.3452 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14461...  Training loss: 3.8610...  0.3401 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14462...  Training loss: 3.8980...  0.3408 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14463...  Training loss: 3.9690...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14464...  Training loss: 3.8927...  0.3461 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14465...  Training loss: 3.8969...  0.3423 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14466...  Training loss: 3.8258...  0.3435 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14467...  Training loss: 3.9398...  0.3440 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14468...  Training loss: 3.9043...  0.3424 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14469...  Training loss: 3.9057...  0.3426 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14470...  Training loss: 3.8265...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14471...  Training loss: 3.9280...  0.3413 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14472...  Training loss: 3.9134...  0.3440 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14473...  Training loss: 4.0353...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14474...  Training loss: 3.9726...  0.3455 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14475...  Training loss: 3.9703...  0.3424 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14476...  Training loss: 3.8827...  0.3413 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14477...  Training loss: 3.9294...  0.3450 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14478...  Training loss: 3.9143...  0.3435 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14479...  Training loss: 3.9236...  0.3417 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14480...  Training loss: 3.9281...  0.3413 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14481...  Training loss: 3.9875...  0.3435 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14482...  Training loss: 3.9817...  0.3443 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14483...  Training loss: 3.9797...  0.3442 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14484...  Training loss: 3.9045...  0.3446 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14485...  Training loss: 3.9655...  0.3454 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14486...  Training loss: 3.9369...  0.3442 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14487...  Training loss: 3.9410...  0.3444 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14488...  Training loss: 3.9263...  0.3436 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14489...  Training loss: 3.8755...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14490...  Training loss: 3.9521...  0.3418 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14491...  Training loss: 4.0201...  0.3453 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14492...  Training loss: 3.8664...  0.3450 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14493...  Training loss: 3.9992...  0.3447 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14494...  Training loss: 3.9764...  0.3449 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14495...  Training loss: 3.9262...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14496...  Training loss: 3.9084...  0.3432 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14497...  Training loss: 3.9350...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14498...  Training loss: 3.9353...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14499...  Training loss: 3.9145...  0.3412 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14500...  Training loss: 3.9354...  0.3406 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14501...  Training loss: 3.9271...  0.3407 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14502...  Training loss: 3.8719...  0.3415 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14503...  Training loss: 3.9302...  0.3412 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14504...  Training loss: 3.8974...  0.3450 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14505...  Training loss: 3.9550...  0.3404 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14506...  Training loss: 3.9644...  0.3453 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14507...  Training loss: 4.0042...  0.3410 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14508...  Training loss: 3.9744...  0.3419 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14509...  Training loss: 3.9683...  0.3436 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14510...  Training loss: 3.9987...  0.3426 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14511...  Training loss: 3.8760...  0.3448 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14512...  Training loss: 3.8931...  0.3429 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14513...  Training loss: 3.9218...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14514...  Training loss: 3.9469...  0.3451 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14515...  Training loss: 3.8880...  0.3443 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14516...  Training loss: 3.8776...  0.3421 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14517...  Training loss: 3.9399...  0.3464 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14518...  Training loss: 3.9374...  0.3409 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14519...  Training loss: 3.8960...  0.3437 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14520...  Training loss: 3.9084...  0.3399 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14521...  Training loss: 3.9740...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14522...  Training loss: 3.9863...  0.3405 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14523...  Training loss: 3.9088...  0.3435 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14524...  Training loss: 3.8793...  0.3433 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14525...  Training loss: 3.8677...  0.3439 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14526...  Training loss: 3.8581...  0.3455 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14527...  Training loss: 3.8234...  0.3448 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14528...  Training loss: 3.9376...  0.3423 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14529...  Training loss: 3.9661...  0.3430 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14530...  Training loss: 3.9544...  0.3446 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14531...  Training loss: 3.9437...  0.3419 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14532...  Training loss: 3.8955...  0.3398 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79/100...  Training Step: 14533...  Training loss: 3.8734...  0.3454 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14534...  Training loss: 3.8755...  0.3445 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14535...  Training loss: 3.8430...  0.3443 sec/batch\n",
      "Epoch: 79/100...  Training Step: 14536...  Training loss: 3.9356...  0.3426 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14537...  Training loss: 3.8247...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14538...  Training loss: 3.6502...  0.3395 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14539...  Training loss: 3.6735...  0.3439 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14540...  Training loss: 3.7282...  0.3414 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14541...  Training loss: 3.7142...  0.3411 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14542...  Training loss: 3.6767...  0.3457 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14543...  Training loss: 3.8357...  0.3445 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14544...  Training loss: 3.8812...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14545...  Training loss: 3.8313...  0.3408 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14546...  Training loss: 3.8819...  0.3429 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14547...  Training loss: 3.8464...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14548...  Training loss: 3.8349...  0.3451 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14549...  Training loss: 3.8653...  0.3452 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14550...  Training loss: 3.8293...  0.3404 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14551...  Training loss: 3.8519...  0.3418 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14552...  Training loss: 3.8779...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14553...  Training loss: 3.9324...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14554...  Training loss: 3.9123...  0.3449 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14555...  Training loss: 3.9576...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14556...  Training loss: 3.9648...  0.3404 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14557...  Training loss: 3.8910...  0.3459 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14558...  Training loss: 3.7744...  0.3423 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14559...  Training loss: 3.8093...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14560...  Training loss: 3.8181...  0.3459 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14561...  Training loss: 3.8721...  0.3400 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14562...  Training loss: 3.8857...  0.3446 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14563...  Training loss: 3.9150...  0.3446 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14564...  Training loss: 3.8681...  0.3443 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14565...  Training loss: 3.9082...  0.3450 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14566...  Training loss: 4.0171...  0.3438 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14567...  Training loss: 4.0094...  0.3401 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14568...  Training loss: 3.9368...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14569...  Training loss: 3.8231...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14570...  Training loss: 3.8388...  0.3445 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14571...  Training loss: 3.8957...  0.3466 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14572...  Training loss: 3.9536...  0.3405 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14573...  Training loss: 3.9644...  0.3412 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14574...  Training loss: 3.9526...  0.3408 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14575...  Training loss: 3.9561...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14576...  Training loss: 4.0323...  0.3433 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14577...  Training loss: 3.9992...  0.3434 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14578...  Training loss: 4.0603...  0.3465 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14579...  Training loss: 4.0948...  0.3445 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14580...  Training loss: 4.0013...  0.3433 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14581...  Training loss: 3.9800...  0.3434 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14582...  Training loss: 3.9680...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14583...  Training loss: 3.9774...  0.3462 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14584...  Training loss: 3.9347...  0.3427 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14585...  Training loss: 3.9393...  0.3430 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14586...  Training loss: 3.9580...  0.3444 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14587...  Training loss: 4.0061...  0.3416 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14588...  Training loss: 4.0375...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14589...  Training loss: 4.0449...  0.3446 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14590...  Training loss: 4.0615...  0.3411 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14591...  Training loss: 4.0844...  0.3419 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14592...  Training loss: 4.0620...  0.3430 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14593...  Training loss: 3.9897...  0.3426 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14594...  Training loss: 3.9969...  0.3426 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14595...  Training loss: 3.8750...  0.3404 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14596...  Training loss: 3.8672...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14597...  Training loss: 3.9297...  0.3422 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14598...  Training loss: 3.9494...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14599...  Training loss: 3.9506...  0.3451 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14600...  Training loss: 4.0062...  0.3456 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14601...  Training loss: 3.9343...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14602...  Training loss: 3.9090...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14603...  Training loss: 3.9099...  0.3420 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14604...  Training loss: 3.8647...  0.3411 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14605...  Training loss: 3.9474...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14606...  Training loss: 3.9338...  0.3399 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14607...  Training loss: 3.9111...  0.3414 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14608...  Training loss: 3.8429...  0.3412 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14609...  Training loss: 3.9003...  0.3449 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14610...  Training loss: 3.8866...  0.3429 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14611...  Training loss: 3.8897...  0.3433 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14612...  Training loss: 3.9689...  0.3445 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14613...  Training loss: 3.9236...  0.3441 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14614...  Training loss: 3.8893...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14615...  Training loss: 3.9257...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14616...  Training loss: 3.8381...  0.3456 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14617...  Training loss: 3.9330...  0.3439 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14618...  Training loss: 3.9355...  0.3441 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14619...  Training loss: 3.9461...  0.3429 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14620...  Training loss: 3.9071...  0.3455 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14621...  Training loss: 3.9068...  0.3416 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14622...  Training loss: 3.9683...  0.3434 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14623...  Training loss: 3.9028...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14624...  Training loss: 3.8441...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14625...  Training loss: 3.9778...  0.3414 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14626...  Training loss: 4.0322...  0.3422 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14627...  Training loss: 3.9335...  0.3398 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14628...  Training loss: 3.8520...  0.3422 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80/100...  Training Step: 14629...  Training loss: 3.9032...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14630...  Training loss: 3.8935...  0.3442 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14631...  Training loss: 3.9543...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14632...  Training loss: 3.9754...  0.3452 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14633...  Training loss: 3.9333...  0.3447 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14634...  Training loss: 3.8730...  0.3405 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14635...  Training loss: 3.9001...  0.3407 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14636...  Training loss: 3.8977...  0.3431 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14637...  Training loss: 3.9243...  0.3430 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14638...  Training loss: 3.8541...  0.3427 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14639...  Training loss: 3.8877...  0.3434 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14640...  Training loss: 3.8010...  0.3416 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14641...  Training loss: 3.9436...  0.3409 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14642...  Training loss: 3.9237...  0.3412 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14643...  Training loss: 3.8809...  0.3427 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14644...  Training loss: 3.8931...  0.3444 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14645...  Training loss: 3.8907...  0.3439 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14646...  Training loss: 3.8996...  0.3407 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14647...  Training loss: 3.9763...  0.3411 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14648...  Training loss: 3.8839...  0.3453 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14649...  Training loss: 3.9065...  0.3404 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14650...  Training loss: 3.8593...  0.3435 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14651...  Training loss: 3.9236...  0.3451 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14652...  Training loss: 3.9014...  0.3423 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14653...  Training loss: 3.9164...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14654...  Training loss: 3.8557...  0.3428 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14655...  Training loss: 3.9360...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14656...  Training loss: 3.8973...  0.3453 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14657...  Training loss: 4.0449...  0.3452 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14658...  Training loss: 3.9871...  0.3444 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14659...  Training loss: 3.9647...  0.3450 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14660...  Training loss: 3.8494...  0.3430 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14661...  Training loss: 3.8938...  0.3454 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14662...  Training loss: 3.9309...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14663...  Training loss: 3.9279...  0.3436 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14664...  Training loss: 3.9192...  0.3451 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14665...  Training loss: 3.9766...  0.3448 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14666...  Training loss: 3.9556...  0.3403 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14667...  Training loss: 3.9694...  0.3415 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14668...  Training loss: 3.9178...  0.3434 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14669...  Training loss: 3.9401...  0.3444 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14670...  Training loss: 3.9123...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14671...  Training loss: 3.9280...  0.3420 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14672...  Training loss: 3.8854...  0.3430 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14673...  Training loss: 3.8823...  0.3421 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14674...  Training loss: 3.9089...  0.3433 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14675...  Training loss: 3.9742...  0.3417 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14676...  Training loss: 3.8697...  0.3439 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14677...  Training loss: 3.9481...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14678...  Training loss: 3.9523...  0.3418 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14679...  Training loss: 3.8786...  0.3421 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14680...  Training loss: 3.8874...  0.3446 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14681...  Training loss: 3.9178...  0.3455 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14682...  Training loss: 3.9471...  0.3447 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14683...  Training loss: 3.9043...  0.3421 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14684...  Training loss: 3.9754...  0.3443 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14685...  Training loss: 3.9389...  0.3401 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14686...  Training loss: 3.8745...  0.3405 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14687...  Training loss: 3.9020...  0.3436 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14688...  Training loss: 3.8980...  0.3431 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14689...  Training loss: 3.8988...  0.3418 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14690...  Training loss: 3.9311...  0.3431 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14691...  Training loss: 3.9569...  0.3451 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14692...  Training loss: 3.9654...  0.3440 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14693...  Training loss: 3.9540...  0.3407 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14694...  Training loss: 4.0087...  0.3403 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14695...  Training loss: 3.8559...  0.3409 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14696...  Training loss: 3.9112...  0.3424 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14697...  Training loss: 3.8846...  0.3418 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14698...  Training loss: 3.9221...  0.3440 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14699...  Training loss: 3.8469...  0.3439 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14700...  Training loss: 3.8288...  0.3437 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14701...  Training loss: 3.9249...  0.3464 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14702...  Training loss: 3.8711...  0.3446 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14703...  Training loss: 3.8829...  0.3450 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14704...  Training loss: 3.8861...  0.3442 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14705...  Training loss: 3.9692...  0.3449 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14706...  Training loss: 3.9736...  0.3412 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14707...  Training loss: 3.8986...  0.3419 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14708...  Training loss: 3.8619...  0.3404 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14709...  Training loss: 3.8548...  0.3413 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14710...  Training loss: 3.8146...  0.3402 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14711...  Training loss: 3.8015...  0.3399 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14712...  Training loss: 3.9177...  0.3429 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14713...  Training loss: 3.9551...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14714...  Training loss: 3.9153...  0.3419 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14715...  Training loss: 3.9153...  0.3450 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14716...  Training loss: 3.8741...  0.3410 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14717...  Training loss: 3.8637...  0.3436 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14718...  Training loss: 3.8716...  0.3447 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14719...  Training loss: 3.8485...  0.3417 sec/batch\n",
      "Epoch: 80/100...  Training Step: 14720...  Training loss: 3.8986...  0.3452 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14721...  Training loss: 3.8407...  0.3442 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14722...  Training loss: 3.6452...  0.3449 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14723...  Training loss: 3.6091...  0.3443 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14724...  Training loss: 3.7138...  0.3427 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100...  Training Step: 14725...  Training loss: 3.6727...  0.3426 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14726...  Training loss: 3.6367...  0.3447 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14727...  Training loss: 3.8256...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14728...  Training loss: 3.8641...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14729...  Training loss: 3.8526...  0.3452 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14730...  Training loss: 3.8990...  0.3405 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14731...  Training loss: 3.8311...  0.3437 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14732...  Training loss: 3.8545...  0.3440 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14733...  Training loss: 3.8613...  0.3445 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14734...  Training loss: 3.8153...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14735...  Training loss: 3.8873...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14736...  Training loss: 3.8629...  0.3436 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14737...  Training loss: 3.9213...  0.3408 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14738...  Training loss: 3.8791...  0.3420 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14739...  Training loss: 3.9306...  0.3401 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14740...  Training loss: 3.9618...  0.3464 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14741...  Training loss: 3.8565...  0.3459 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14742...  Training loss: 3.7727...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14743...  Training loss: 3.7835...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14744...  Training loss: 3.8078...  0.3411 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14745...  Training loss: 3.8591...  0.3429 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14746...  Training loss: 3.8569...  0.3413 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14747...  Training loss: 3.8749...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14748...  Training loss: 3.8236...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14749...  Training loss: 3.8994...  0.3417 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14750...  Training loss: 3.9968...  0.3396 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14751...  Training loss: 3.9951...  0.3464 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14752...  Training loss: 3.9108...  0.3453 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14753...  Training loss: 3.8151...  0.3453 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14754...  Training loss: 3.8106...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14755...  Training loss: 3.9178...  0.3435 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14756...  Training loss: 3.9737...  0.3450 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14757...  Training loss: 3.9819...  0.3445 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14758...  Training loss: 3.9038...  0.3408 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14759...  Training loss: 3.9310...  0.3431 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14760...  Training loss: 3.9925...  0.3417 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14761...  Training loss: 3.9452...  0.3450 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14762...  Training loss: 4.0272...  0.3402 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14763...  Training loss: 4.0521...  0.3412 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14764...  Training loss: 4.0037...  0.3422 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14765...  Training loss: 3.9709...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14766...  Training loss: 3.9504...  0.3427 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14767...  Training loss: 3.9944...  0.3430 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14768...  Training loss: 3.9039...  0.3447 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14769...  Training loss: 3.9427...  0.3447 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14770...  Training loss: 3.9500...  0.3439 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14771...  Training loss: 4.0050...  0.3411 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14772...  Training loss: 4.0020...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14773...  Training loss: 3.9933...  0.3405 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14774...  Training loss: 4.0018...  0.3413 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14775...  Training loss: 4.0546...  0.3449 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14776...  Training loss: 4.0777...  0.3446 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14777...  Training loss: 3.9528...  0.3412 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14778...  Training loss: 3.9916...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14779...  Training loss: 3.8922...  0.3440 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14780...  Training loss: 3.8446...  0.3443 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14781...  Training loss: 3.9149...  0.3458 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14782...  Training loss: 3.9374...  0.3450 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14783...  Training loss: 3.9185...  0.3439 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14784...  Training loss: 4.0012...  0.3410 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14785...  Training loss: 3.9353...  0.3430 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14786...  Training loss: 3.9216...  0.3414 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14787...  Training loss: 3.9291...  0.3405 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14788...  Training loss: 3.8845...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14789...  Training loss: 3.9462...  0.3418 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14790...  Training loss: 3.9467...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14791...  Training loss: 3.8805...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14792...  Training loss: 3.8412...  0.3444 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14793...  Training loss: 3.8838...  0.3411 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14794...  Training loss: 3.8221...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14795...  Training loss: 3.8851...  0.3424 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14796...  Training loss: 3.9137...  0.3438 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14797...  Training loss: 3.9149...  0.3431 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14798...  Training loss: 3.8656...  0.3410 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14799...  Training loss: 3.9137...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14800...  Training loss: 3.8592...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14801...  Training loss: 3.9207...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14802...  Training loss: 3.9180...  0.3435 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14803...  Training loss: 3.9223...  0.3442 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14804...  Training loss: 3.8993...  0.3404 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14805...  Training loss: 3.9229...  0.3431 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14806...  Training loss: 3.9664...  0.3429 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14807...  Training loss: 3.9111...  0.3418 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14808...  Training loss: 3.8268...  0.3455 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14809...  Training loss: 3.9447...  0.3436 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14810...  Training loss: 4.0215...  0.3425 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14811...  Training loss: 3.8947...  0.3446 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14812...  Training loss: 3.8207...  0.3445 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14813...  Training loss: 3.8725...  0.3425 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14814...  Training loss: 3.8981...  0.3426 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14815...  Training loss: 3.9966...  0.3449 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14816...  Training loss: 3.9574...  0.3428 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14817...  Training loss: 3.9329...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14818...  Training loss: 3.8616...  0.3443 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14819...  Training loss: 3.8783...  0.3448 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14820...  Training loss: 3.8787...  0.3435 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/100...  Training Step: 14821...  Training loss: 3.8878...  0.3458 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14822...  Training loss: 3.8465...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14823...  Training loss: 3.8992...  0.3410 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14824...  Training loss: 3.7508...  0.3435 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14825...  Training loss: 3.9004...  0.3433 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14826...  Training loss: 3.9056...  0.3444 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14827...  Training loss: 3.8252...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14828...  Training loss: 3.8238...  0.3424 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14829...  Training loss: 3.8397...  0.3413 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14830...  Training loss: 3.8881...  0.3405 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14831...  Training loss: 3.9310...  0.3446 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14832...  Training loss: 3.8831...  0.3442 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14833...  Training loss: 3.8738...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14834...  Training loss: 3.8512...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14835...  Training loss: 3.9461...  0.3437 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14836...  Training loss: 3.8928...  0.3414 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14837...  Training loss: 3.9294...  0.3450 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14838...  Training loss: 3.8232...  0.3429 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14839...  Training loss: 3.9258...  0.3423 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14840...  Training loss: 3.9105...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14841...  Training loss: 4.0101...  0.3412 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14842...  Training loss: 3.9868...  0.3435 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14843...  Training loss: 3.9903...  0.3460 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14844...  Training loss: 3.8293...  0.3439 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14845...  Training loss: 3.8942...  0.3425 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14846...  Training loss: 3.9250...  0.3409 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14847...  Training loss: 3.9117...  0.3442 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14848...  Training loss: 3.9053...  0.3400 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14849...  Training loss: 3.9583...  0.3433 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14850...  Training loss: 3.9560...  0.3456 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14851...  Training loss: 3.9707...  0.3419 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14852...  Training loss: 3.9162...  0.3465 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14853...  Training loss: 3.9319...  0.3408 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14854...  Training loss: 3.8860...  0.3426 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14855...  Training loss: 3.9531...  0.3428 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14856...  Training loss: 3.8719...  0.3425 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14857...  Training loss: 3.8609...  0.3436 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14858...  Training loss: 3.8868...  0.3429 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14859...  Training loss: 3.9718...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14860...  Training loss: 3.8002...  0.3424 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14861...  Training loss: 3.9213...  0.3404 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14862...  Training loss: 3.9515...  0.3434 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14863...  Training loss: 3.8770...  0.3431 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14864...  Training loss: 3.8767...  0.3467 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14865...  Training loss: 3.9304...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14866...  Training loss: 3.9020...  0.3433 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14867...  Training loss: 3.8729...  0.3447 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14868...  Training loss: 3.9137...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14869...  Training loss: 3.9336...  0.3440 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14870...  Training loss: 3.8424...  0.3422 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14871...  Training loss: 3.8727...  0.3430 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14872...  Training loss: 3.8653...  0.3417 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14873...  Training loss: 3.8912...  0.3421 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14874...  Training loss: 3.9113...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14875...  Training loss: 3.9243...  0.3422 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14876...  Training loss: 3.9425...  0.3422 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14877...  Training loss: 3.9237...  0.3413 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14878...  Training loss: 3.9735...  0.3420 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14879...  Training loss: 3.8336...  0.3439 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14880...  Training loss: 3.8573...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14881...  Training loss: 3.8747...  0.3433 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14882...  Training loss: 3.8962...  0.3434 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14883...  Training loss: 3.8320...  0.3437 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14884...  Training loss: 3.8196...  0.3416 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14885...  Training loss: 3.9179...  0.3430 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14886...  Training loss: 3.8646...  0.3401 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14887...  Training loss: 3.8967...  0.3427 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14888...  Training loss: 3.8668...  0.3422 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14889...  Training loss: 3.9687...  0.3444 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14890...  Training loss: 3.9599...  0.3431 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14891...  Training loss: 3.9052...  0.3408 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14892...  Training loss: 3.8431...  0.3425 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14893...  Training loss: 3.8495...  0.3447 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14894...  Training loss: 3.7901...  0.3415 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14895...  Training loss: 3.7830...  0.3411 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14896...  Training loss: 3.8764...  0.3450 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14897...  Training loss: 3.9205...  0.3451 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14898...  Training loss: 3.8584...  0.3441 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14899...  Training loss: 3.8998...  0.3428 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14900...  Training loss: 3.8758...  0.3436 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14901...  Training loss: 3.8660...  0.3408 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14902...  Training loss: 3.8765...  0.3417 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14903...  Training loss: 3.8657...  0.3411 sec/batch\n",
      "Epoch: 81/100...  Training Step: 14904...  Training loss: 3.8986...  0.3425 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14905...  Training loss: 3.8208...  0.3434 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14906...  Training loss: 3.6190...  0.3454 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14907...  Training loss: 3.6348...  0.3429 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14908...  Training loss: 3.6895...  0.3429 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14909...  Training loss: 3.6567...  0.3405 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14910...  Training loss: 3.6220...  0.3416 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14911...  Training loss: 3.8324...  0.3450 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14912...  Training loss: 3.8257...  0.3446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14913...  Training loss: 3.7926...  0.3454 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14914...  Training loss: 3.8362...  0.3448 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14915...  Training loss: 3.8311...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14916...  Training loss: 3.7888...  0.3410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100...  Training Step: 14917...  Training loss: 3.8149...  0.3421 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14918...  Training loss: 3.8057...  0.3419 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14919...  Training loss: 3.8280...  0.3429 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14920...  Training loss: 3.8223...  0.3404 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14921...  Training loss: 3.9054...  0.3418 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14922...  Training loss: 3.8422...  0.3407 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14923...  Training loss: 3.9091...  0.3432 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14924...  Training loss: 3.8902...  0.3446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14925...  Training loss: 3.8269...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14926...  Training loss: 3.7939...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14927...  Training loss: 3.7640...  0.3449 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14928...  Training loss: 3.8184...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14929...  Training loss: 3.8693...  0.3453 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14930...  Training loss: 3.8696...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14931...  Training loss: 3.8794...  0.3413 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14932...  Training loss: 3.8251...  0.3452 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14933...  Training loss: 3.8412...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14934...  Training loss: 3.9542...  0.3407 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14935...  Training loss: 3.9229...  0.3405 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14936...  Training loss: 3.8566...  0.3417 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14937...  Training loss: 3.7761...  0.3441 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14938...  Training loss: 3.8328...  0.3405 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14939...  Training loss: 3.9230...  0.3451 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14940...  Training loss: 3.9619...  0.3440 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14941...  Training loss: 3.9738...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14942...  Training loss: 3.9225...  0.3439 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14943...  Training loss: 3.9201...  0.3442 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14944...  Training loss: 3.9529...  0.3403 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14945...  Training loss: 3.9542...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14946...  Training loss: 4.0078...  0.3438 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14947...  Training loss: 4.0301...  0.3435 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14948...  Training loss: 3.9761...  0.3443 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14949...  Training loss: 3.9477...  0.3448 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14950...  Training loss: 3.9349...  0.3433 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14951...  Training loss: 4.0022...  0.3432 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14952...  Training loss: 3.8744...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14953...  Training loss: 3.9292...  0.3433 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14954...  Training loss: 3.9458...  0.3431 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14955...  Training loss: 3.9511...  0.3450 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14956...  Training loss: 3.9888...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14957...  Training loss: 3.9913...  0.3458 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14958...  Training loss: 3.9672...  0.3433 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14959...  Training loss: 3.9972...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14960...  Training loss: 4.0131...  0.3441 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14961...  Training loss: 3.9228...  0.3428 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14962...  Training loss: 3.9782...  0.3420 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14963...  Training loss: 3.8927...  0.3409 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14964...  Training loss: 3.8494...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14965...  Training loss: 3.8990...  0.3427 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14966...  Training loss: 3.9188...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14967...  Training loss: 3.9151...  0.3446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14968...  Training loss: 4.0123...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14969...  Training loss: 3.9045...  0.3427 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14970...  Training loss: 3.9045...  0.3428 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14971...  Training loss: 3.9112...  0.3431 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14972...  Training loss: 3.8399...  0.3453 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14973...  Training loss: 3.9076...  0.3441 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14974...  Training loss: 3.9519...  0.3426 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14975...  Training loss: 3.9129...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14976...  Training loss: 3.8383...  0.3430 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14977...  Training loss: 3.8851...  0.3465 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14978...  Training loss: 3.8410...  0.3453 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14979...  Training loss: 3.8506...  0.3404 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14980...  Training loss: 3.9044...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14981...  Training loss: 3.9179...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14982...  Training loss: 3.8677...  0.3420 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14983...  Training loss: 3.9119...  0.3407 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14984...  Training loss: 3.8414...  0.3458 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14985...  Training loss: 3.9079...  0.3426 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14986...  Training loss: 3.8900...  0.3429 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14987...  Training loss: 3.9203...  0.3419 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14988...  Training loss: 3.8871...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14989...  Training loss: 3.8988...  0.3459 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14990...  Training loss: 3.9725...  0.3422 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14991...  Training loss: 3.8849...  0.3439 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14992...  Training loss: 3.8392...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14993...  Training loss: 3.8920...  0.3465 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14994...  Training loss: 4.0160...  0.3478 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14995...  Training loss: 3.9100...  0.3453 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14996...  Training loss: 3.8442...  0.3451 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14997...  Training loss: 3.8479...  0.3440 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14998...  Training loss: 3.8661...  0.3400 sec/batch\n",
      "Epoch: 82/100...  Training Step: 14999...  Training loss: 3.9182...  0.3413 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15000...  Training loss: 3.9273...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15001...  Training loss: 3.8801...  0.3942 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15002...  Training loss: 3.8583...  0.3472 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15003...  Training loss: 3.8982...  0.3446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15004...  Training loss: 3.8741...  0.3437 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15005...  Training loss: 3.9122...  0.3423 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15006...  Training loss: 3.8374...  0.3434 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15007...  Training loss: 3.8651...  0.3463 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15008...  Training loss: 3.7596...  0.3449 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15009...  Training loss: 3.9016...  0.3440 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15010...  Training loss: 3.8814...  0.3444 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15011...  Training loss: 3.8274...  0.3445 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15012...  Training loss: 3.8457...  0.3460 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/100...  Training Step: 15013...  Training loss: 3.8475...  0.3413 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15014...  Training loss: 3.8175...  0.3426 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15015...  Training loss: 3.9059...  0.3405 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15016...  Training loss: 3.8398...  0.3446 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15017...  Training loss: 3.8448...  0.3406 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15018...  Training loss: 3.7978...  0.3456 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15019...  Training loss: 3.9221...  0.3424 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15020...  Training loss: 3.8559...  0.3450 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15021...  Training loss: 3.8849...  0.3418 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15022...  Training loss: 3.7936...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15023...  Training loss: 3.8813...  0.3412 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15024...  Training loss: 3.8743...  0.3411 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15025...  Training loss: 3.9846...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15026...  Training loss: 3.9858...  0.3449 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15027...  Training loss: 3.9555...  0.3449 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15028...  Training loss: 3.8667...  0.3448 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15029...  Training loss: 3.9055...  0.3420 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15030...  Training loss: 3.9486...  0.3416 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15031...  Training loss: 3.9472...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15032...  Training loss: 3.9097...  0.3430 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15033...  Training loss: 3.9508...  0.3424 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15034...  Training loss: 3.9613...  0.3418 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15035...  Training loss: 3.9731...  0.3396 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15036...  Training loss: 3.9070...  0.3421 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15037...  Training loss: 3.9537...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15038...  Training loss: 3.8895...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15039...  Training loss: 3.9256...  0.3448 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15040...  Training loss: 3.8931...  0.3409 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15041...  Training loss: 3.8852...  0.3414 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15042...  Training loss: 3.8960...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15043...  Training loss: 3.9511...  0.3448 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15044...  Training loss: 3.7887...  0.3422 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15045...  Training loss: 3.9240...  0.3424 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15046...  Training loss: 3.8966...  0.3401 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15047...  Training loss: 3.8702...  0.3386 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15048...  Training loss: 3.8579...  0.3404 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15049...  Training loss: 3.9100...  0.3408 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15050...  Training loss: 3.8862...  0.3393 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15051...  Training loss: 3.8898...  0.3436 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15052...  Training loss: 3.9023...  0.3455 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15053...  Training loss: 3.9175...  0.3404 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15054...  Training loss: 3.8422...  0.3450 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15055...  Training loss: 3.8752...  0.3439 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15056...  Training loss: 3.8631...  0.3401 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15057...  Training loss: 3.8872...  0.3405 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15058...  Training loss: 3.9407...  0.3394 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15059...  Training loss: 3.9596...  0.3409 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15060...  Training loss: 3.9461...  0.3400 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15061...  Training loss: 3.9282...  0.3426 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15062...  Training loss: 3.9818...  0.3420 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15063...  Training loss: 3.8139...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15064...  Training loss: 3.8587...  0.3413 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15065...  Training loss: 3.8766...  0.3412 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15066...  Training loss: 3.8980...  0.3410 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15067...  Training loss: 3.8341...  0.3401 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15068...  Training loss: 3.7981...  0.3421 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15069...  Training loss: 3.8927...  0.3438 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15070...  Training loss: 3.8324...  0.3467 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15071...  Training loss: 3.8504...  0.3469 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15072...  Training loss: 3.8443...  0.3480 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15073...  Training loss: 3.9546...  0.3450 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15074...  Training loss: 3.9617...  0.3443 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15075...  Training loss: 3.8831...  0.3442 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15076...  Training loss: 3.8366...  0.3394 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15077...  Training loss: 3.8243...  0.3399 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15078...  Training loss: 3.7991...  0.3399 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15079...  Training loss: 3.7556...  0.3399 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15080...  Training loss: 3.8802...  0.3401 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15081...  Training loss: 3.9035...  0.3416 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15082...  Training loss: 3.8432...  0.3409 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15083...  Training loss: 3.8984...  0.3407 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15084...  Training loss: 3.8766...  0.3452 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15085...  Training loss: 3.8131...  0.3440 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15086...  Training loss: 3.8673...  0.3438 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15087...  Training loss: 3.8163...  0.3457 sec/batch\n",
      "Epoch: 82/100...  Training Step: 15088...  Training loss: 3.8858...  0.3419 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15089...  Training loss: 3.7926...  0.3426 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15090...  Training loss: 3.6135...  0.3403 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15091...  Training loss: 3.6071...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15092...  Training loss: 3.6961...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15093...  Training loss: 3.6702...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15094...  Training loss: 3.6323...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15095...  Training loss: 3.8215...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15096...  Training loss: 3.8271...  0.3419 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15097...  Training loss: 3.7983...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15098...  Training loss: 3.8249...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15099...  Training loss: 3.7784...  0.3403 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15100...  Training loss: 3.7904...  0.3451 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15101...  Training loss: 3.7966...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15102...  Training loss: 3.7734...  0.3429 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15103...  Training loss: 3.8384...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15104...  Training loss: 3.8386...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15105...  Training loss: 3.8677...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15106...  Training loss: 3.8059...  0.3389 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15107...  Training loss: 3.8854...  0.3450 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15108...  Training loss: 3.9097...  0.3403 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/100...  Training Step: 15109...  Training loss: 3.7989...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15110...  Training loss: 3.7194...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15111...  Training loss: 3.7560...  0.3424 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15112...  Training loss: 3.7790...  0.3402 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15113...  Training loss: 3.8611...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15114...  Training loss: 3.8485...  0.3442 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15115...  Training loss: 3.9059...  0.3436 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15116...  Training loss: 3.8348...  0.3437 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15117...  Training loss: 3.8601...  0.3424 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15118...  Training loss: 3.9149...  0.3397 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15119...  Training loss: 3.9077...  0.3424 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15120...  Training loss: 3.8424...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15121...  Training loss: 3.7303...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15122...  Training loss: 3.7955...  0.3394 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15123...  Training loss: 3.8907...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15124...  Training loss: 3.9423...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15125...  Training loss: 3.9660...  0.3410 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15126...  Training loss: 3.9035...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15127...  Training loss: 3.9412...  0.3401 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15128...  Training loss: 3.9524...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15129...  Training loss: 3.9330...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15130...  Training loss: 3.9804...  0.3416 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15131...  Training loss: 4.0350...  0.3431 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15132...  Training loss: 3.9424...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15133...  Training loss: 3.9494...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15134...  Training loss: 3.9361...  0.3393 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15135...  Training loss: 3.9616...  0.3425 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15136...  Training loss: 3.8899...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15137...  Training loss: 3.8946...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15138...  Training loss: 3.9232...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15139...  Training loss: 3.9486...  0.3402 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15140...  Training loss: 3.9989...  0.3396 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15141...  Training loss: 3.9414...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15142...  Training loss: 3.9595...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15143...  Training loss: 3.9840...  0.3392 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15144...  Training loss: 3.9732...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15145...  Training loss: 3.9015...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15146...  Training loss: 3.9518...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15147...  Training loss: 3.8573...  0.3403 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15148...  Training loss: 3.8476...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15149...  Training loss: 3.9011...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15150...  Training loss: 3.9440...  0.3393 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15151...  Training loss: 3.9026...  0.3413 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15152...  Training loss: 3.9828...  0.3388 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15153...  Training loss: 3.8867...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15154...  Training loss: 3.8751...  0.3421 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15155...  Training loss: 3.9081...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15156...  Training loss: 3.8758...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15157...  Training loss: 3.9123...  0.3424 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15158...  Training loss: 3.9440...  0.3436 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15159...  Training loss: 3.8901...  0.3437 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15160...  Training loss: 3.8186...  0.3426 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15161...  Training loss: 3.8774...  0.3423 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15162...  Training loss: 3.8607...  0.3433 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15163...  Training loss: 3.8739...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15164...  Training loss: 3.9157...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15165...  Training loss: 3.9277...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15166...  Training loss: 3.8263...  0.3417 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15167...  Training loss: 3.9333...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15168...  Training loss: 3.8488...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15169...  Training loss: 3.9053...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15170...  Training loss: 3.9086...  0.3420 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15171...  Training loss: 3.8724...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15172...  Training loss: 3.8447...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15173...  Training loss: 3.8739...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15174...  Training loss: 3.9706...  0.3410 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15175...  Training loss: 3.8619...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15176...  Training loss: 3.8130...  0.3409 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15177...  Training loss: 3.9170...  0.3427 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15178...  Training loss: 3.9724...  0.3428 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15179...  Training loss: 3.9176...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15180...  Training loss: 3.8549...  0.3436 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15181...  Training loss: 3.8454...  0.3442 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15182...  Training loss: 3.8343...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15183...  Training loss: 3.9244...  0.3429 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15184...  Training loss: 3.8992...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15185...  Training loss: 3.8604...  0.3421 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15186...  Training loss: 3.8341...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15187...  Training loss: 3.8521...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15188...  Training loss: 3.8691...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15189...  Training loss: 3.8834...  0.3425 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15190...  Training loss: 3.8473...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15191...  Training loss: 3.8507...  0.3402 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15192...  Training loss: 3.7678...  0.3397 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15193...  Training loss: 3.8836...  0.3398 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15194...  Training loss: 3.8534...  0.3436 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15195...  Training loss: 3.8000...  0.3429 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15196...  Training loss: 3.8331...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15197...  Training loss: 3.8331...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15198...  Training loss: 3.8342...  0.3418 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15199...  Training loss: 3.8876...  0.3438 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15200...  Training loss: 3.8309...  0.3427 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15201...  Training loss: 3.8369...  0.3413 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15202...  Training loss: 3.7530...  0.3402 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15203...  Training loss: 3.9118...  0.3401 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15204...  Training loss: 3.8489...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83/100...  Training Step: 15205...  Training loss: 3.8906...  0.3424 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15206...  Training loss: 3.7874...  0.3413 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15207...  Training loss: 3.8784...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15208...  Training loss: 3.8434...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15209...  Training loss: 3.9817...  0.3393 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15210...  Training loss: 3.9572...  0.3396 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15211...  Training loss: 3.9562...  0.3410 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15212...  Training loss: 3.8404...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15213...  Training loss: 3.8986...  0.3394 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15214...  Training loss: 3.9256...  0.3416 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15215...  Training loss: 3.9245...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15216...  Training loss: 3.8877...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15217...  Training loss: 3.9188...  0.3506 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15218...  Training loss: 3.9649...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15219...  Training loss: 3.9524...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15220...  Training loss: 3.8747...  0.3428 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15221...  Training loss: 3.9324...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15222...  Training loss: 3.8948...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15223...  Training loss: 3.9313...  0.3426 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15224...  Training loss: 3.8471...  0.3398 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15225...  Training loss: 3.8898...  0.3429 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15226...  Training loss: 3.8929...  0.3401 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15227...  Training loss: 3.9542...  0.3438 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15228...  Training loss: 3.7891...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15229...  Training loss: 3.8953...  0.3405 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15230...  Training loss: 3.8715...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15231...  Training loss: 3.8359...  0.3421 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15232...  Training loss: 3.8345...  0.3425 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15233...  Training loss: 3.8801...  0.3392 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15234...  Training loss: 3.8558...  0.3406 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15235...  Training loss: 3.8661...  0.3388 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15236...  Training loss: 3.8871...  0.3398 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15237...  Training loss: 3.8890...  0.3417 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15238...  Training loss: 3.8039...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15239...  Training loss: 3.8554...  0.3428 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15240...  Training loss: 3.8466...  0.3399 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15241...  Training loss: 3.8680...  0.3431 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15242...  Training loss: 3.9150...  0.3430 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15243...  Training loss: 3.9140...  0.3413 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15244...  Training loss: 3.9294...  0.3404 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15245...  Training loss: 3.8808...  0.3425 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15246...  Training loss: 3.9614...  0.3390 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15247...  Training loss: 3.8317...  0.3400 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15248...  Training loss: 3.8524...  0.3411 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15249...  Training loss: 3.8589...  0.3395 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15250...  Training loss: 3.8950...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15251...  Training loss: 3.8204...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15252...  Training loss: 3.7938...  0.3418 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15253...  Training loss: 3.8765...  0.3410 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15254...  Training loss: 3.8221...  0.3421 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15255...  Training loss: 3.8018...  0.3441 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15256...  Training loss: 3.8304...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15257...  Training loss: 3.8967...  0.3396 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15258...  Training loss: 3.9328...  0.3430 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15259...  Training loss: 3.8968...  0.3427 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15260...  Training loss: 3.8123...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15261...  Training loss: 3.7869...  0.3387 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15262...  Training loss: 3.8051...  0.3415 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15263...  Training loss: 3.7523...  0.3439 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15264...  Training loss: 3.8403...  0.3414 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15265...  Training loss: 3.8729...  0.3422 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15266...  Training loss: 3.8617...  0.3410 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15267...  Training loss: 3.8640...  0.3407 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15268...  Training loss: 3.8594...  0.3426 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15269...  Training loss: 3.7854...  0.3408 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15270...  Training loss: 3.8539...  0.3412 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15271...  Training loss: 3.8037...  0.3419 sec/batch\n",
      "Epoch: 83/100...  Training Step: 15272...  Training loss: 3.8511...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15273...  Training loss: 3.7639...  0.3415 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15274...  Training loss: 3.6275...  0.3425 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15275...  Training loss: 3.6141...  0.3404 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15276...  Training loss: 3.6815...  0.3416 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15277...  Training loss: 3.6415...  0.3399 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15278...  Training loss: 3.6423...  0.3420 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15279...  Training loss: 3.7833...  0.3434 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15280...  Training loss: 3.8120...  0.3425 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15281...  Training loss: 3.7478...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15282...  Training loss: 3.8255...  0.3408 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15283...  Training loss: 3.7565...  0.3426 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15284...  Training loss: 3.7757...  0.3403 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15285...  Training loss: 3.8003...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15286...  Training loss: 3.7647...  0.3389 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15287...  Training loss: 3.7714...  0.3415 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15288...  Training loss: 3.8226...  0.3404 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15289...  Training loss: 3.8658...  0.3398 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15290...  Training loss: 3.8049...  0.3428 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15291...  Training loss: 3.9017...  0.3420 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15292...  Training loss: 3.8793...  0.3457 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15293...  Training loss: 3.8030...  0.3431 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15294...  Training loss: 3.7105...  0.3390 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15295...  Training loss: 3.7542...  0.3414 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15296...  Training loss: 3.7847...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15297...  Training loss: 3.8654...  0.3423 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15298...  Training loss: 3.8278...  0.3400 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15299...  Training loss: 3.8589...  0.3425 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15300...  Training loss: 3.8266...  0.3418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84/100...  Training Step: 15301...  Training loss: 3.8113...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15302...  Training loss: 3.9070...  0.3429 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15303...  Training loss: 3.8842...  0.3397 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15304...  Training loss: 3.8426...  0.3409 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15305...  Training loss: 3.7078...  0.3430 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15306...  Training loss: 3.7683...  0.3406 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15307...  Training loss: 3.8659...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15308...  Training loss: 3.9480...  0.3418 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15309...  Training loss: 3.9476...  0.3424 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15310...  Training loss: 3.8872...  0.3386 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15311...  Training loss: 3.8792...  0.3387 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15312...  Training loss: 3.9426...  0.3403 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15313...  Training loss: 3.9336...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15314...  Training loss: 3.9657...  0.3418 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15315...  Training loss: 4.0211...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15316...  Training loss: 3.9578...  0.3390 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15317...  Training loss: 3.9109...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15318...  Training loss: 3.8843...  0.3402 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15319...  Training loss: 3.9363...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15320...  Training loss: 3.8841...  0.3443 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15321...  Training loss: 3.9058...  0.3414 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15322...  Training loss: 3.9398...  0.3401 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15323...  Training loss: 3.9436...  0.3425 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15324...  Training loss: 3.9794...  0.3429 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15325...  Training loss: 3.9731...  0.3422 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15326...  Training loss: 3.8961...  0.3415 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15327...  Training loss: 3.9260...  0.3400 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15328...  Training loss: 3.9468...  0.3421 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15329...  Training loss: 3.8659...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15330...  Training loss: 3.9116...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15331...  Training loss: 3.8361...  0.3405 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15332...  Training loss: 3.8381...  0.3423 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15333...  Training loss: 3.8741...  0.3397 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15334...  Training loss: 3.8911...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15335...  Training loss: 3.8695...  0.3397 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15336...  Training loss: 3.9772...  0.3402 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15337...  Training loss: 3.8803...  0.3427 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15338...  Training loss: 3.8578...  0.3426 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15339...  Training loss: 3.8797...  0.3402 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15340...  Training loss: 3.8336...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15341...  Training loss: 3.8703...  0.3389 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15342...  Training loss: 3.9233...  0.3414 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15343...  Training loss: 3.8531...  0.3404 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15344...  Training loss: 3.8072...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15345...  Training loss: 3.8372...  0.3404 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15346...  Training loss: 3.8368...  0.3391 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15347...  Training loss: 3.8675...  0.3403 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15348...  Training loss: 3.8649...  0.3406 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15349...  Training loss: 3.8873...  0.3416 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15350...  Training loss: 3.8415...  0.3394 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15351...  Training loss: 3.8829...  0.3441 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15352...  Training loss: 3.8559...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15353...  Training loss: 3.8962...  0.3429 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15354...  Training loss: 3.8385...  0.3385 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15355...  Training loss: 3.8750...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15356...  Training loss: 3.8353...  0.3399 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15357...  Training loss: 3.8412...  0.3427 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15358...  Training loss: 3.9326...  0.3433 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15359...  Training loss: 3.8611...  0.3461 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15360...  Training loss: 3.7842...  0.3428 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15361...  Training loss: 3.9245...  0.3439 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15362...  Training loss: 3.9375...  0.3396 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15363...  Training loss: 3.8720...  0.3405 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15364...  Training loss: 3.8364...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15365...  Training loss: 3.8507...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15366...  Training loss: 3.8444...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15367...  Training loss: 3.9061...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15368...  Training loss: 3.9096...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15369...  Training loss: 3.8435...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15370...  Training loss: 3.7979...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15371...  Training loss: 3.8267...  0.3422 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15372...  Training loss: 3.8239...  0.3402 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15373...  Training loss: 3.8686...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15374...  Training loss: 3.8016...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15375...  Training loss: 3.8424...  0.3401 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15376...  Training loss: 3.7506...  0.3436 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15377...  Training loss: 3.8819...  0.3435 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15378...  Training loss: 3.8619...  0.3408 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15379...  Training loss: 3.8124...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15380...  Training loss: 3.8316...  0.3426 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15381...  Training loss: 3.8085...  0.3423 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15382...  Training loss: 3.7921...  0.3427 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15383...  Training loss: 3.9041...  0.3409 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15384...  Training loss: 3.8356...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15385...  Training loss: 3.8305...  0.3416 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15386...  Training loss: 3.7572...  0.3417 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15387...  Training loss: 3.8795...  0.3387 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15388...  Training loss: 3.8045...  0.3431 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15389...  Training loss: 3.8495...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15390...  Training loss: 3.7505...  0.3404 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15391...  Training loss: 3.8565...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15392...  Training loss: 3.8398...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15393...  Training loss: 3.9772...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15394...  Training loss: 3.9271...  0.3394 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15395...  Training loss: 3.9456...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15396...  Training loss: 3.8360...  0.3407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84/100...  Training Step: 15397...  Training loss: 3.8986...  0.3405 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15398...  Training loss: 3.9140...  0.3439 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15399...  Training loss: 3.8606...  0.3421 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15400...  Training loss: 3.8873...  0.3430 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15401...  Training loss: 3.9290...  0.3431 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15402...  Training loss: 3.9446...  0.3399 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15403...  Training loss: 3.9660...  0.3409 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15404...  Training loss: 3.8842...  0.3409 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15405...  Training loss: 3.9054...  0.3398 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15406...  Training loss: 3.8347...  0.3403 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15407...  Training loss: 3.9235...  0.3430 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15408...  Training loss: 3.8506...  0.3403 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15409...  Training loss: 3.8598...  0.3398 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15410...  Training loss: 3.8888...  0.3399 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15411...  Training loss: 3.9358...  0.3399 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15412...  Training loss: 3.7913...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15413...  Training loss: 3.8969...  0.3417 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15414...  Training loss: 3.8977...  0.3396 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15415...  Training loss: 3.8511...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15416...  Training loss: 3.8010...  0.3427 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15417...  Training loss: 3.8703...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15418...  Training loss: 3.8536...  0.3427 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15419...  Training loss: 3.8338...  0.3432 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15420...  Training loss: 3.8624...  0.3409 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15421...  Training loss: 3.8888...  0.3431 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15422...  Training loss: 3.8324...  0.3421 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15423...  Training loss: 3.8517...  0.3431 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15424...  Training loss: 3.8430...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15425...  Training loss: 3.8687...  0.3407 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15426...  Training loss: 3.9268...  0.3390 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15427...  Training loss: 3.9199...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15428...  Training loss: 3.9283...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15429...  Training loss: 3.8772...  0.3393 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15430...  Training loss: 3.9471...  0.3397 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15431...  Training loss: 3.7950...  0.3416 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15432...  Training loss: 3.8449...  0.3381 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15433...  Training loss: 3.8384...  0.3408 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15434...  Training loss: 3.8720...  0.3384 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15435...  Training loss: 3.8241...  0.3392 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15436...  Training loss: 3.8012...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15437...  Training loss: 3.8517...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15438...  Training loss: 3.8198...  0.3406 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15439...  Training loss: 3.8053...  0.3424 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15440...  Training loss: 3.8485...  0.3385 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15441...  Training loss: 3.8886...  0.3400 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15442...  Training loss: 3.9099...  0.3406 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15443...  Training loss: 3.8721...  0.3417 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15444...  Training loss: 3.8359...  0.3436 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15445...  Training loss: 3.7719...  0.3384 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15446...  Training loss: 3.7832...  0.3383 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15447...  Training loss: 3.7484...  0.3413 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15448...  Training loss: 3.8543...  0.3408 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15449...  Training loss: 3.8631...  0.3411 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15450...  Training loss: 3.8422...  0.3410 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15451...  Training loss: 3.8569...  0.3421 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15452...  Training loss: 3.8107...  0.3419 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15453...  Training loss: 3.7620...  0.3412 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15454...  Training loss: 3.8384...  0.3420 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15455...  Training loss: 3.8033...  0.3417 sec/batch\n",
      "Epoch: 84/100...  Training Step: 15456...  Training loss: 3.8462...  0.3401 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15457...  Training loss: 3.7399...  0.3427 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15458...  Training loss: 3.6176...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15459...  Training loss: 3.6332...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15460...  Training loss: 3.7101...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15461...  Training loss: 3.6451...  0.3398 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15462...  Training loss: 3.6142...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15463...  Training loss: 3.8092...  0.3450 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15464...  Training loss: 3.8448...  0.3417 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15465...  Training loss: 3.7780...  0.3422 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15466...  Training loss: 3.8110...  0.3408 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15467...  Training loss: 3.7829...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15468...  Training loss: 3.7444...  0.3429 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15469...  Training loss: 3.8275...  0.3418 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15470...  Training loss: 3.8326...  0.3399 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15471...  Training loss: 3.8173...  0.3396 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15472...  Training loss: 3.8326...  0.3423 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15473...  Training loss: 3.8876...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15474...  Training loss: 3.8104...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15475...  Training loss: 3.8620...  0.3392 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15476...  Training loss: 3.8787...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15477...  Training loss: 3.8100...  0.3399 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15478...  Training loss: 3.6854...  0.3401 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15479...  Training loss: 3.7537...  0.3423 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15480...  Training loss: 3.7922...  0.3395 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15481...  Training loss: 3.8752...  0.3422 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15482...  Training loss: 3.8354...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15483...  Training loss: 3.8772...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15484...  Training loss: 3.8099...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15485...  Training loss: 3.8855...  0.3410 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15486...  Training loss: 3.9463...  0.3421 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15487...  Training loss: 3.9123...  0.3403 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15488...  Training loss: 3.8479...  0.3416 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15489...  Training loss: 3.7393...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15490...  Training loss: 3.7689...  0.3449 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15491...  Training loss: 3.8659...  0.3452 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15492...  Training loss: 3.9316...  0.3404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100...  Training Step: 15493...  Training loss: 3.9420...  0.3410 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15494...  Training loss: 3.9094...  0.3408 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15495...  Training loss: 3.9035...  0.3420 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15496...  Training loss: 3.9758...  0.3429 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15497...  Training loss: 3.9537...  0.3449 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15498...  Training loss: 4.0042...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15499...  Training loss: 4.0540...  0.3414 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15500...  Training loss: 3.9347...  0.3432 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15501...  Training loss: 3.9255...  0.3436 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15502...  Training loss: 3.9178...  0.3403 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15503...  Training loss: 3.9609...  0.3430 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15504...  Training loss: 3.8929...  0.3410 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15505...  Training loss: 3.9250...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15506...  Training loss: 3.9539...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15507...  Training loss: 3.9831...  0.3419 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15508...  Training loss: 3.9956...  0.3382 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15509...  Training loss: 3.9443...  0.3416 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15510...  Training loss: 3.9602...  0.3402 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15511...  Training loss: 3.9407...  0.3408 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15512...  Training loss: 3.9509...  0.3401 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15513...  Training loss: 3.8884...  0.3423 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15514...  Training loss: 3.9236...  0.3413 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15515...  Training loss: 3.8307...  0.3422 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15516...  Training loss: 3.8473...  0.3440 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15517...  Training loss: 3.8974...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15518...  Training loss: 3.9330...  0.3442 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15519...  Training loss: 3.8996...  0.3420 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15520...  Training loss: 3.9364...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15521...  Training loss: 3.8810...  0.3424 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15522...  Training loss: 3.8767...  0.3400 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15523...  Training loss: 3.8654...  0.3411 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15524...  Training loss: 3.8274...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15525...  Training loss: 3.9006...  0.3432 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15526...  Training loss: 3.9136...  0.3401 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15527...  Training loss: 3.8617...  0.3427 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15528...  Training loss: 3.8151...  0.3412 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15529...  Training loss: 3.8309...  0.3415 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15530...  Training loss: 3.8203...  0.3435 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15531...  Training loss: 3.8379...  0.3389 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15532...  Training loss: 3.8621...  0.3391 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15533...  Training loss: 3.8800...  0.3402 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15534...  Training loss: 3.8510...  0.3419 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15535...  Training loss: 3.8966...  0.3391 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15536...  Training loss: 3.8381...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15537...  Training loss: 3.8906...  0.3398 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15538...  Training loss: 3.8648...  0.3410 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15539...  Training loss: 3.8626...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15540...  Training loss: 3.8488...  0.3441 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15541...  Training loss: 3.8379...  0.3417 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15542...  Training loss: 3.8955...  0.3432 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15543...  Training loss: 3.8200...  0.3418 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15544...  Training loss: 3.7789...  0.3416 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15545...  Training loss: 3.8810...  0.3390 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15546...  Training loss: 3.9517...  0.3428 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15547...  Training loss: 3.8836...  0.3392 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15548...  Training loss: 3.8059...  0.3397 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15549...  Training loss: 3.8291...  0.3408 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15550...  Training loss: 3.8357...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15551...  Training loss: 3.8947...  0.3431 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15552...  Training loss: 3.8950...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15553...  Training loss: 3.8544...  0.3414 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15554...  Training loss: 3.8023...  0.3438 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15555...  Training loss: 3.8160...  0.3393 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15556...  Training loss: 3.8113...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15557...  Training loss: 3.8548...  0.3412 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15558...  Training loss: 3.8226...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15559...  Training loss: 3.8250...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15560...  Training loss: 3.7349...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15561...  Training loss: 3.8834...  0.3400 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15562...  Training loss: 3.8618...  0.3400 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15563...  Training loss: 3.8019...  0.3425 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15564...  Training loss: 3.8241...  0.3444 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15565...  Training loss: 3.8132...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15566...  Training loss: 3.7950...  0.3398 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15567...  Training loss: 3.8982...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15568...  Training loss: 3.8092...  0.3394 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15569...  Training loss: 3.8215...  0.3444 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15570...  Training loss: 3.7908...  0.3391 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15571...  Training loss: 3.8803...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15572...  Training loss: 3.8201...  0.3419 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15573...  Training loss: 3.8562...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15574...  Training loss: 3.7901...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15575...  Training loss: 3.8596...  0.3411 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15576...  Training loss: 3.8205...  0.3413 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15577...  Training loss: 3.9537...  0.3386 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15578...  Training loss: 3.9298...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15579...  Training loss: 3.9467...  0.3389 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15580...  Training loss: 3.8055...  0.3424 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15581...  Training loss: 3.8407...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15582...  Training loss: 3.8731...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15583...  Training loss: 3.8533...  0.3406 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15584...  Training loss: 3.8632...  0.3421 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15585...  Training loss: 3.8860...  0.3423 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15586...  Training loss: 3.9233...  0.3416 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15587...  Training loss: 3.9580...  0.3403 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15588...  Training loss: 3.8645...  0.3404 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85/100...  Training Step: 15589...  Training loss: 3.8841...  0.3386 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15590...  Training loss: 3.8661...  0.3411 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15591...  Training loss: 3.8701...  0.3439 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15592...  Training loss: 3.8461...  0.3393 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15593...  Training loss: 3.8400...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15594...  Training loss: 3.8577...  0.3422 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15595...  Training loss: 3.9005...  0.3429 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15596...  Training loss: 3.8093...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15597...  Training loss: 3.8963...  0.3412 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15598...  Training loss: 3.8966...  0.3393 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15599...  Training loss: 3.8493...  0.3415 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15600...  Training loss: 3.8392...  0.3420 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15601...  Training loss: 3.8830...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15602...  Training loss: 3.8626...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15603...  Training loss: 3.8356...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15604...  Training loss: 3.8663...  0.3410 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15605...  Training loss: 3.9030...  0.3440 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15606...  Training loss: 3.7968...  0.3421 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15607...  Training loss: 3.8652...  0.3412 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15608...  Training loss: 3.8078...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15609...  Training loss: 3.8252...  0.3404 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15610...  Training loss: 3.8724...  0.3391 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15611...  Training loss: 3.9200...  0.3400 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15612...  Training loss: 3.8873...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15613...  Training loss: 3.8823...  0.3400 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15614...  Training loss: 3.9482...  0.3428 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15615...  Training loss: 3.7753...  0.3415 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15616...  Training loss: 3.8470...  0.3419 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15617...  Training loss: 3.8488...  0.3409 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15618...  Training loss: 3.8536...  0.3429 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15619...  Training loss: 3.8013...  0.3425 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15620...  Training loss: 3.7411...  0.3437 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15621...  Training loss: 3.8354...  0.3427 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15622...  Training loss: 3.7902...  0.3393 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15623...  Training loss: 3.8155...  0.3398 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15624...  Training loss: 3.8221...  0.3414 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15625...  Training loss: 3.8785...  0.3427 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15626...  Training loss: 3.9221...  0.3403 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15627...  Training loss: 3.8405...  0.3397 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15628...  Training loss: 3.8215...  0.3411 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15629...  Training loss: 3.8010...  0.3407 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15630...  Training loss: 3.7536...  0.3440 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15631...  Training loss: 3.7074...  0.3435 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15632...  Training loss: 3.8339...  0.3394 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15633...  Training loss: 3.8579...  0.3426 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15634...  Training loss: 3.8501...  0.3430 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15635...  Training loss: 3.8223...  0.3447 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15636...  Training loss: 3.8080...  0.3416 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15637...  Training loss: 3.7894...  0.3405 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15638...  Training loss: 3.7926...  0.3437 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15639...  Training loss: 3.7990...  0.3428 sec/batch\n",
      "Epoch: 85/100...  Training Step: 15640...  Training loss: 3.8416...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15641...  Training loss: 3.7348...  0.3400 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15642...  Training loss: 3.5846...  0.3392 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15643...  Training loss: 3.5985...  0.3419 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15644...  Training loss: 3.6845...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15645...  Training loss: 3.6643...  0.3401 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15646...  Training loss: 3.6228...  0.3435 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15647...  Training loss: 3.7834...  0.3382 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15648...  Training loss: 3.8195...  0.3421 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15649...  Training loss: 3.7676...  0.3446 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15650...  Training loss: 3.8143...  0.3438 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15651...  Training loss: 3.7822...  0.3393 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15652...  Training loss: 3.7334...  0.3415 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15653...  Training loss: 3.7604...  0.3395 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15654...  Training loss: 3.7883...  0.3428 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15655...  Training loss: 3.8007...  0.3421 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15656...  Training loss: 3.7734...  0.3420 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15657...  Training loss: 3.8749...  0.3416 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15658...  Training loss: 3.7821...  0.3415 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15659...  Training loss: 3.8697...  0.3417 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15660...  Training loss: 3.8468...  0.3404 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15661...  Training loss: 3.7580...  0.3396 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15662...  Training loss: 3.6712...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15663...  Training loss: 3.6835...  0.3435 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15664...  Training loss: 3.7228...  0.3399 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15665...  Training loss: 3.8078...  0.3397 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15666...  Training loss: 3.7740...  0.3418 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15667...  Training loss: 3.8207...  0.3420 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15668...  Training loss: 3.7871...  0.3389 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15669...  Training loss: 3.8101...  0.3410 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15670...  Training loss: 3.9188...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15671...  Training loss: 3.8911...  0.3384 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15672...  Training loss: 3.8397...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15673...  Training loss: 3.7599...  0.3415 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15674...  Training loss: 3.7635...  0.3423 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15675...  Training loss: 3.8450...  0.3403 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15676...  Training loss: 3.9266...  0.3438 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15677...  Training loss: 3.9202...  0.3423 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15678...  Training loss: 3.8770...  0.3429 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15679...  Training loss: 3.8712...  0.3443 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15680...  Training loss: 3.9032...  0.3436 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15681...  Training loss: 3.9172...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15682...  Training loss: 3.9658...  0.3401 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15683...  Training loss: 4.0332...  0.3412 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15684...  Training loss: 3.9426...  0.3424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100...  Training Step: 15685...  Training loss: 3.9260...  0.3435 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15686...  Training loss: 3.8946...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15687...  Training loss: 3.9586...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15688...  Training loss: 3.8652...  0.3401 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15689...  Training loss: 3.9284...  0.3412 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15690...  Training loss: 3.9317...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15691...  Training loss: 3.9810...  0.3398 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15692...  Training loss: 3.9771...  0.3416 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15693...  Training loss: 3.9847...  0.3391 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15694...  Training loss: 3.9472...  0.3420 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15695...  Training loss: 3.9244...  0.3392 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15696...  Training loss: 3.9391...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15697...  Training loss: 3.8640...  0.3413 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15698...  Training loss: 3.8851...  0.3410 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15699...  Training loss: 3.7958...  0.3416 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15700...  Training loss: 3.8019...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15701...  Training loss: 3.8828...  0.3413 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15702...  Training loss: 3.9048...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15703...  Training loss: 3.8762...  0.3396 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15704...  Training loss: 3.9597...  0.3398 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15705...  Training loss: 3.8844...  0.3419 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15706...  Training loss: 3.8696...  0.3414 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15707...  Training loss: 3.8344...  0.3413 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15708...  Training loss: 3.8096...  0.3414 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15709...  Training loss: 3.8606...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15710...  Training loss: 3.9250...  0.3410 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15711...  Training loss: 3.8469...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15712...  Training loss: 3.7980...  0.3454 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15713...  Training loss: 3.8287...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15714...  Training loss: 3.8360...  0.3429 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15715...  Training loss: 3.8303...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15716...  Training loss: 3.8781...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15717...  Training loss: 3.8565...  0.3406 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15718...  Training loss: 3.7538...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15719...  Training loss: 3.8407...  0.3396 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15720...  Training loss: 3.8291...  0.3419 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15721...  Training loss: 3.8743...  0.3421 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15722...  Training loss: 3.8501...  0.3423 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15723...  Training loss: 3.8491...  0.3437 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15724...  Training loss: 3.8511...  0.3441 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15725...  Training loss: 3.8506...  0.3432 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15726...  Training loss: 3.9248...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15727...  Training loss: 3.8072...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15728...  Training loss: 3.7774...  0.3391 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15729...  Training loss: 3.8569...  0.3415 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15730...  Training loss: 3.9407...  0.3425 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15731...  Training loss: 3.8547...  0.3411 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15732...  Training loss: 3.7889...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15733...  Training loss: 3.8097...  0.3414 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15734...  Training loss: 3.8161...  0.3424 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15735...  Training loss: 3.8670...  0.3417 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15736...  Training loss: 3.9194...  0.3419 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15737...  Training loss: 3.8312...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15738...  Training loss: 3.7890...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15739...  Training loss: 3.8314...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15740...  Training loss: 3.8086...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15741...  Training loss: 3.8191...  0.3404 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15742...  Training loss: 3.8082...  0.3424 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15743...  Training loss: 3.8209...  0.3399 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15744...  Training loss: 3.7334...  0.3412 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15745...  Training loss: 3.8674...  0.3428 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15746...  Training loss: 3.8584...  0.3417 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15747...  Training loss: 3.7789...  0.3390 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15748...  Training loss: 3.8308...  0.3404 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15749...  Training loss: 3.8139...  0.3384 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15750...  Training loss: 3.7998...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15751...  Training loss: 3.8811...  0.3411 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15752...  Training loss: 3.8155...  0.3398 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15753...  Training loss: 3.8119...  0.3401 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15754...  Training loss: 3.7757...  0.3407 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15755...  Training loss: 3.8453...  0.3385 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15756...  Training loss: 3.8225...  0.3442 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15757...  Training loss: 3.8554...  0.3444 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15758...  Training loss: 3.7691...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15759...  Training loss: 3.8660...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15760...  Training loss: 3.8350...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15761...  Training loss: 3.9376...  0.3420 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15762...  Training loss: 3.9277...  0.3421 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15763...  Training loss: 3.9006...  0.3398 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15764...  Training loss: 3.8039...  0.3416 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15765...  Training loss: 3.8357...  0.3431 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15766...  Training loss: 3.8721...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15767...  Training loss: 3.8557...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15768...  Training loss: 3.8446...  0.3438 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15769...  Training loss: 3.8961...  0.3414 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15770...  Training loss: 3.9238...  0.3413 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15771...  Training loss: 3.9423...  0.3416 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15772...  Training loss: 3.8483...  0.3428 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15773...  Training loss: 3.8753...  0.3396 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15774...  Training loss: 3.8283...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15775...  Training loss: 3.8871...  0.3418 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15776...  Training loss: 3.8048...  0.3413 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15777...  Training loss: 3.8021...  0.3418 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15778...  Training loss: 3.8762...  0.3420 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15779...  Training loss: 3.9053...  0.3403 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15780...  Training loss: 3.7612...  0.3407 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/100...  Training Step: 15781...  Training loss: 3.8555...  0.3391 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15782...  Training loss: 3.8729...  0.3441 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15783...  Training loss: 3.8167...  0.3400 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15784...  Training loss: 3.7934...  0.3411 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15785...  Training loss: 3.8601...  0.3435 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15786...  Training loss: 3.8689...  0.3389 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15787...  Training loss: 3.8450...  0.3407 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15788...  Training loss: 3.8487...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15789...  Training loss: 3.8608...  0.3426 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15790...  Training loss: 3.7670...  0.3424 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15791...  Training loss: 3.8124...  0.3448 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15792...  Training loss: 3.8213...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15793...  Training loss: 3.8192...  0.3405 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15794...  Training loss: 3.8940...  0.3398 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15795...  Training loss: 3.8912...  0.3428 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15796...  Training loss: 3.8955...  0.3422 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15797...  Training loss: 3.8625...  0.3409 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15798...  Training loss: 3.9564...  0.3424 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15799...  Training loss: 3.7695...  0.3433 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15800...  Training loss: 3.8247...  0.3415 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15801...  Training loss: 3.8463...  0.3426 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15802...  Training loss: 3.8493...  0.3418 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15803...  Training loss: 3.7975...  0.3439 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15804...  Training loss: 3.7464...  0.3425 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15805...  Training loss: 3.8348...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15806...  Training loss: 3.7777...  0.3399 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15807...  Training loss: 3.7790...  0.3426 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15808...  Training loss: 3.8463...  0.3394 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15809...  Training loss: 3.9028...  0.3417 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15810...  Training loss: 3.9092...  0.3389 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15811...  Training loss: 3.8317...  0.3389 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15812...  Training loss: 3.8242...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15813...  Training loss: 3.7493...  0.3388 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15814...  Training loss: 3.7584...  0.3411 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15815...  Training loss: 3.7207...  0.3389 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15816...  Training loss: 3.8379...  0.3401 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15817...  Training loss: 3.8511...  0.3412 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15818...  Training loss: 3.8183...  0.3407 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15819...  Training loss: 3.8442...  0.3407 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15820...  Training loss: 3.7732...  0.3400 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15821...  Training loss: 3.7713...  0.3408 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15822...  Training loss: 3.7889...  0.3410 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15823...  Training loss: 3.7864...  0.3402 sec/batch\n",
      "Epoch: 86/100...  Training Step: 15824...  Training loss: 3.8005...  0.3389 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15825...  Training loss: 3.7758...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15826...  Training loss: 3.5779...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15827...  Training loss: 3.5426...  0.3445 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15828...  Training loss: 3.6343...  0.3427 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15829...  Training loss: 3.6403...  0.3455 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15830...  Training loss: 3.5835...  0.3442 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15831...  Training loss: 3.7478...  0.3451 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15832...  Training loss: 3.7886...  0.3451 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15833...  Training loss: 3.7519...  0.3400 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15834...  Training loss: 3.7933...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15835...  Training loss: 3.7580...  0.3428 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15836...  Training loss: 3.7156...  0.3432 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15837...  Training loss: 3.7676...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15838...  Training loss: 3.7038...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15839...  Training loss: 3.7492...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15840...  Training loss: 3.7669...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15841...  Training loss: 3.8195...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15842...  Training loss: 3.7735...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15843...  Training loss: 3.8322...  0.3415 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15844...  Training loss: 3.8252...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15845...  Training loss: 3.7796...  0.3444 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15846...  Training loss: 3.6669...  0.3444 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15847...  Training loss: 3.7023...  0.3423 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15848...  Training loss: 3.7203...  0.3401 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15849...  Training loss: 3.8137...  0.3426 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15850...  Training loss: 3.7677...  0.3442 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15851...  Training loss: 3.8248...  0.3447 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15852...  Training loss: 3.7666...  0.3424 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15853...  Training loss: 3.8104...  0.3425 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15854...  Training loss: 3.9025...  0.3423 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15855...  Training loss: 3.8653...  0.3412 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15856...  Training loss: 3.8055...  0.3447 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15857...  Training loss: 3.6697...  0.3429 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15858...  Training loss: 3.7328...  0.3412 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15859...  Training loss: 3.8057...  0.3429 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15860...  Training loss: 3.8724...  0.3444 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15861...  Training loss: 3.8928...  0.3408 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15862...  Training loss: 3.8258...  0.3430 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15863...  Training loss: 3.8559...  0.3454 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15864...  Training loss: 3.8822...  0.3424 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15865...  Training loss: 3.8919...  0.3463 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15866...  Training loss: 3.9381...  0.3438 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15867...  Training loss: 3.9844...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15868...  Training loss: 3.9030...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15869...  Training loss: 3.8756...  0.3448 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15870...  Training loss: 3.8479...  0.3411 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15871...  Training loss: 3.9007...  0.3463 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15872...  Training loss: 3.8360...  0.3458 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15873...  Training loss: 3.8576...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15874...  Training loss: 3.8738...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15875...  Training loss: 3.9249...  0.3456 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15876...  Training loss: 3.9757...  0.3445 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100...  Training Step: 15877...  Training loss: 3.9398...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15878...  Training loss: 3.9222...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15879...  Training loss: 3.9249...  0.3431 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15880...  Training loss: 3.8835...  0.3420 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15881...  Training loss: 3.8403...  0.3453 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15882...  Training loss: 3.9013...  0.3430 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15883...  Training loss: 3.7835...  0.3443 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15884...  Training loss: 3.7817...  0.3458 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15885...  Training loss: 3.8446...  0.3409 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15886...  Training loss: 3.8934...  0.3428 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15887...  Training loss: 3.8627...  0.3437 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15888...  Training loss: 3.9225...  0.3441 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15889...  Training loss: 3.8368...  0.3400 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15890...  Training loss: 3.8300...  0.3424 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15891...  Training loss: 3.8475...  0.3467 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15892...  Training loss: 3.7743...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15893...  Training loss: 3.8295...  0.3436 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15894...  Training loss: 3.8666...  0.3458 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15895...  Training loss: 3.7882...  0.3445 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15896...  Training loss: 3.7575...  0.3405 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15897...  Training loss: 3.8081...  0.3422 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15898...  Training loss: 3.7838...  0.3452 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15899...  Training loss: 3.8100...  0.3439 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15900...  Training loss: 3.8168...  0.3447 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15901...  Training loss: 3.8309...  0.3457 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15902...  Training loss: 3.7672...  0.3449 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15903...  Training loss: 3.8166...  0.3424 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15904...  Training loss: 3.7722...  0.3455 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15905...  Training loss: 3.8377...  0.3434 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15906...  Training loss: 3.8217...  0.3466 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15907...  Training loss: 3.8153...  0.3453 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15908...  Training loss: 3.8091...  0.3443 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15909...  Training loss: 3.8462...  0.3462 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15910...  Training loss: 3.8893...  0.3415 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15911...  Training loss: 3.8117...  0.3414 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15912...  Training loss: 3.7202...  0.3459 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15913...  Training loss: 3.8486...  0.3406 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15914...  Training loss: 3.9231...  0.3419 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15915...  Training loss: 3.8383...  0.3436 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15916...  Training loss: 3.7842...  0.3425 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15917...  Training loss: 3.8262...  0.3460 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15918...  Training loss: 3.8183...  0.3434 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15919...  Training loss: 3.8788...  0.3456 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15920...  Training loss: 3.9271...  0.3463 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15921...  Training loss: 3.8290...  0.3452 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15922...  Training loss: 3.7915...  0.3406 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15923...  Training loss: 3.8048...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15924...  Training loss: 3.8099...  0.3465 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15925...  Training loss: 3.8382...  0.3461 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15926...  Training loss: 3.7861...  0.3466 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15927...  Training loss: 3.7637...  0.3415 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15928...  Training loss: 3.7030...  0.3478 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15929...  Training loss: 3.8198...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15930...  Training loss: 3.8090...  0.3430 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15931...  Training loss: 3.7943...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15932...  Training loss: 3.8158...  0.3419 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15933...  Training loss: 3.8261...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15934...  Training loss: 3.8246...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15935...  Training loss: 3.8993...  0.3416 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15936...  Training loss: 3.8104...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15937...  Training loss: 3.8151...  0.3412 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15938...  Training loss: 3.7658...  0.3429 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15939...  Training loss: 3.8392...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15940...  Training loss: 3.7903...  0.3453 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15941...  Training loss: 3.8186...  0.3491 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15942...  Training loss: 3.7636...  0.3462 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15943...  Training loss: 3.8432...  0.3431 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15944...  Training loss: 3.8407...  0.3417 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15945...  Training loss: 4.0034...  0.3440 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15946...  Training loss: 3.9408...  0.3419 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15947...  Training loss: 3.9434...  0.3468 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15948...  Training loss: 3.8046...  0.3436 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15949...  Training loss: 3.8547...  0.3465 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15950...  Training loss: 3.8939...  0.3453 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15951...  Training loss: 3.8534...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15952...  Training loss: 3.8253...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15953...  Training loss: 3.9163...  0.3456 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15954...  Training loss: 3.9136...  0.3449 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15955...  Training loss: 3.9067...  0.3431 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15956...  Training loss: 3.8324...  0.3450 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15957...  Training loss: 3.8712...  0.3454 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15958...  Training loss: 3.8315...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15959...  Training loss: 3.8706...  0.3432 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15960...  Training loss: 3.8277...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15961...  Training loss: 3.8095...  0.3446 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15962...  Training loss: 3.8644...  0.3437 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15963...  Training loss: 3.8645...  0.3428 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15964...  Training loss: 3.7155...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15965...  Training loss: 3.8823...  0.3429 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15966...  Training loss: 3.8450...  0.3409 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15967...  Training loss: 3.7792...  0.3407 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15968...  Training loss: 3.7938...  0.3415 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15969...  Training loss: 3.8226...  0.3426 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15970...  Training loss: 3.8596...  0.3439 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15971...  Training loss: 3.8141...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15972...  Training loss: 3.8555...  0.3401 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/100...  Training Step: 15973...  Training loss: 3.8672...  0.3406 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15974...  Training loss: 3.7728...  0.3406 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15975...  Training loss: 3.8106...  0.3408 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15976...  Training loss: 3.8191...  0.3430 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15977...  Training loss: 3.8253...  0.3435 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15978...  Training loss: 3.8476...  0.3402 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15979...  Training loss: 3.8672...  0.3401 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15980...  Training loss: 3.8916...  0.3396 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15981...  Training loss: 3.8579...  0.3437 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15982...  Training loss: 3.9246...  0.3405 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15983...  Training loss: 3.7520...  0.3414 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15984...  Training loss: 3.7915...  0.3420 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15985...  Training loss: 3.8457...  0.3409 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15986...  Training loss: 3.8240...  0.3418 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15987...  Training loss: 3.7673...  0.3445 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15988...  Training loss: 3.7586...  0.3421 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15989...  Training loss: 3.8341...  0.3388 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15990...  Training loss: 3.7596...  0.3408 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15991...  Training loss: 3.7695...  0.3403 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15992...  Training loss: 3.7815...  0.3398 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15993...  Training loss: 3.8930...  0.3416 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15994...  Training loss: 3.9080...  0.3414 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15995...  Training loss: 3.8275...  0.3430 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15996...  Training loss: 3.8151...  0.3392 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15997...  Training loss: 3.7552...  0.3404 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15998...  Training loss: 3.7412...  0.3401 sec/batch\n",
      "Epoch: 87/100...  Training Step: 15999...  Training loss: 3.6977...  0.3405 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16000...  Training loss: 3.8188...  0.3470 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16001...  Training loss: 3.8562...  0.3949 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16002...  Training loss: 3.8278...  0.3482 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16003...  Training loss: 3.8286...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16004...  Training loss: 3.7938...  0.3440 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16005...  Training loss: 3.7786...  0.3442 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16006...  Training loss: 3.7635...  0.3433 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16007...  Training loss: 3.7877...  0.3429 sec/batch\n",
      "Epoch: 87/100...  Training Step: 16008...  Training loss: 3.8087...  0.3440 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16009...  Training loss: 3.7370...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16010...  Training loss: 3.5520...  0.3409 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16011...  Training loss: 3.5636...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16012...  Training loss: 3.6165...  0.3434 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16013...  Training loss: 3.6195...  0.3441 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16014...  Training loss: 3.5456...  0.3418 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16015...  Training loss: 3.7312...  0.3431 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16016...  Training loss: 3.7787...  0.3431 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16017...  Training loss: 3.7541...  0.3412 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16018...  Training loss: 3.7905...  0.3420 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16019...  Training loss: 3.7531...  0.3426 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16020...  Training loss: 3.6952...  0.3415 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16021...  Training loss: 3.7123...  0.3434 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16022...  Training loss: 3.6889...  0.3426 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16023...  Training loss: 3.7324...  0.3416 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16024...  Training loss: 3.7556...  0.3435 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16025...  Training loss: 3.7744...  0.3442 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16026...  Training loss: 3.7538...  0.3458 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16027...  Training loss: 3.8251...  0.3422 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16028...  Training loss: 3.7894...  0.3442 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16029...  Training loss: 3.7523...  0.3442 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16030...  Training loss: 3.6513...  0.3415 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16031...  Training loss: 3.7077...  0.3460 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16032...  Training loss: 3.7162...  0.3457 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16033...  Training loss: 3.8057...  0.3411 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16034...  Training loss: 3.7667...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16035...  Training loss: 3.8171...  0.3437 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16036...  Training loss: 3.7641...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16037...  Training loss: 3.8109...  0.3445 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16038...  Training loss: 3.8757...  0.3410 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16039...  Training loss: 3.8354...  0.3424 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16040...  Training loss: 3.7732...  0.3455 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16041...  Training loss: 3.6821...  0.3444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16042...  Training loss: 3.7545...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16043...  Training loss: 3.8033...  0.3456 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16044...  Training loss: 3.8565...  0.3457 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16045...  Training loss: 3.8964...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16046...  Training loss: 3.8223...  0.3407 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16047...  Training loss: 3.8450...  0.3424 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16048...  Training loss: 3.8874...  0.3433 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16049...  Training loss: 3.8592...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16050...  Training loss: 3.9178...  0.3431 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16051...  Training loss: 3.9569...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16052...  Training loss: 3.9035...  0.3446 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16053...  Training loss: 3.8869...  0.3443 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16054...  Training loss: 3.8529...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16055...  Training loss: 3.8933...  0.3465 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16056...  Training loss: 3.8209...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16057...  Training loss: 3.8341...  0.3436 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16058...  Training loss: 3.8545...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16059...  Training loss: 3.9059...  0.3424 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16060...  Training loss: 3.9493...  0.3447 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16061...  Training loss: 3.9268...  0.3426 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16062...  Training loss: 3.9051...  0.3453 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16063...  Training loss: 3.8957...  0.3418 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16064...  Training loss: 3.8973...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16065...  Training loss: 3.8320...  0.3443 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16066...  Training loss: 3.8707...  0.3408 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16067...  Training loss: 3.7535...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16068...  Training loss: 3.7391...  0.3405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88/100...  Training Step: 16069...  Training loss: 3.8068...  0.3437 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16070...  Training loss: 3.8263...  0.3434 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16071...  Training loss: 3.8465...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16072...  Training loss: 3.8968...  0.3435 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16073...  Training loss: 3.8035...  0.3422 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16074...  Training loss: 3.8413...  0.3412 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16075...  Training loss: 3.8131...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16076...  Training loss: 3.7740...  0.3442 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16077...  Training loss: 3.8107...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16078...  Training loss: 3.8656...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16079...  Training loss: 3.7923...  0.3394 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16080...  Training loss: 3.7474...  0.3430 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16081...  Training loss: 3.7799...  0.3417 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16082...  Training loss: 3.7998...  0.3421 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16083...  Training loss: 3.8064...  0.3415 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16084...  Training loss: 3.8244...  0.3423 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16085...  Training loss: 3.8125...  0.3444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16086...  Training loss: 3.7394...  0.3423 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16087...  Training loss: 3.8262...  0.3429 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16088...  Training loss: 3.7698...  0.3447 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16089...  Training loss: 3.8050...  0.3431 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16090...  Training loss: 3.7917...  0.3414 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16091...  Training loss: 3.8287...  0.3461 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16092...  Training loss: 3.8173...  0.3419 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16093...  Training loss: 3.8230...  0.3424 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16094...  Training loss: 3.8822...  0.3433 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16095...  Training loss: 3.7806...  0.3416 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16096...  Training loss: 3.7333...  0.3406 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16097...  Training loss: 3.8293...  0.3389 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16098...  Training loss: 3.8773...  0.3433 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16099...  Training loss: 3.8249...  0.3432 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16100...  Training loss: 3.7428...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16101...  Training loss: 3.8004...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16102...  Training loss: 3.8014...  0.3441 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16103...  Training loss: 3.8730...  0.3433 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16104...  Training loss: 3.9037...  0.3430 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16105...  Training loss: 3.8426...  0.3422 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16106...  Training loss: 3.8014...  0.3453 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16107...  Training loss: 3.8209...  0.3437 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16108...  Training loss: 3.8345...  0.3450 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16109...  Training loss: 3.8358...  0.3453 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16110...  Training loss: 3.7748...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16111...  Training loss: 3.7502...  0.3466 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16112...  Training loss: 3.6547...  0.3419 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16113...  Training loss: 3.8278...  0.3453 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16114...  Training loss: 3.7899...  0.3446 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16115...  Training loss: 3.7514...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16116...  Training loss: 3.8230...  0.3408 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16117...  Training loss: 3.7906...  0.3404 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16118...  Training loss: 3.7912...  0.3429 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16119...  Training loss: 3.8873...  0.3453 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16120...  Training loss: 3.7653...  0.3440 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16121...  Training loss: 3.7623...  0.3412 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16122...  Training loss: 3.7512...  0.3433 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16123...  Training loss: 3.8298...  0.3430 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16124...  Training loss: 3.7726...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16125...  Training loss: 3.7986...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16126...  Training loss: 3.7355...  0.3410 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16127...  Training loss: 3.8284...  0.3431 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16128...  Training loss: 3.8385...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16129...  Training loss: 3.9260...  0.3411 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16130...  Training loss: 3.9620...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16131...  Training loss: 3.9347...  0.3420 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16132...  Training loss: 3.7941...  0.3438 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16133...  Training loss: 3.8788...  0.3466 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16134...  Training loss: 3.8719...  0.3450 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16135...  Training loss: 3.8550...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16136...  Training loss: 3.8466...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16137...  Training loss: 3.8667...  0.3418 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16138...  Training loss: 3.8916...  0.3441 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16139...  Training loss: 3.9126...  0.3414 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16140...  Training loss: 3.8389...  0.3440 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16141...  Training loss: 3.8546...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16142...  Training loss: 3.8160...  0.3452 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16143...  Training loss: 3.8717...  0.3444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16144...  Training loss: 3.8206...  0.3448 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16145...  Training loss: 3.8186...  0.3411 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16146...  Training loss: 3.8488...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16147...  Training loss: 3.8846...  0.3423 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16148...  Training loss: 3.7411...  0.3432 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16149...  Training loss: 3.8347...  0.3445 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16150...  Training loss: 3.8496...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16151...  Training loss: 3.8095...  0.3438 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16152...  Training loss: 3.7747...  0.3439 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16153...  Training loss: 3.8214...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16154...  Training loss: 3.8416...  0.3408 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16155...  Training loss: 3.8304...  0.3412 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16156...  Training loss: 3.8402...  0.3420 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16157...  Training loss: 3.8361...  0.3455 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16158...  Training loss: 3.7584...  0.3435 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16159...  Training loss: 3.8047...  0.3451 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16160...  Training loss: 3.8137...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16161...  Training loss: 3.8028...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16162...  Training loss: 3.8398...  0.3406 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16163...  Training loss: 3.8313...  0.3460 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16164...  Training loss: 3.8668...  0.3456 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88/100...  Training Step: 16165...  Training loss: 3.8580...  0.3466 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16166...  Training loss: 3.9009...  0.3444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16167...  Training loss: 3.7459...  0.3449 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16168...  Training loss: 3.8280...  0.3444 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16169...  Training loss: 3.8178...  0.3447 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16170...  Training loss: 3.8540...  0.3416 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16171...  Training loss: 3.7590...  0.3447 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16172...  Training loss: 3.7245...  0.3409 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16173...  Training loss: 3.8225...  0.3455 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16174...  Training loss: 3.7601...  0.3427 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16175...  Training loss: 3.7660...  0.3460 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16176...  Training loss: 3.7863...  0.3425 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16177...  Training loss: 3.8636...  0.3409 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16178...  Training loss: 3.8838...  0.3399 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16179...  Training loss: 3.8166...  0.3411 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16180...  Training loss: 3.7705...  0.3406 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16181...  Training loss: 3.7421...  0.3413 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16182...  Training loss: 3.7365...  0.3457 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16183...  Training loss: 3.6892...  0.3408 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16184...  Training loss: 3.8157...  0.3412 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16185...  Training loss: 3.8439...  0.3437 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16186...  Training loss: 3.8082...  0.3411 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16187...  Training loss: 3.7996...  0.3438 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16188...  Training loss: 3.7435...  0.3438 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16189...  Training loss: 3.7357...  0.3454 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16190...  Training loss: 3.8010...  0.3428 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16191...  Training loss: 3.7553...  0.3448 sec/batch\n",
      "Epoch: 88/100...  Training Step: 16192...  Training loss: 3.8009...  0.3459 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16193...  Training loss: 3.7234...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16194...  Training loss: 3.5525...  0.3467 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16195...  Training loss: 3.5349...  0.3446 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16196...  Training loss: 3.6104...  0.3438 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16197...  Training loss: 3.5840...  0.3439 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16198...  Training loss: 3.5661...  0.3432 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16199...  Training loss: 3.7186...  0.3448 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16200...  Training loss: 3.7670...  0.3456 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16201...  Training loss: 3.7173...  0.3421 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16202...  Training loss: 3.7546...  0.3403 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16203...  Training loss: 3.7400...  0.3401 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16204...  Training loss: 3.7008...  0.3433 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16205...  Training loss: 3.7212...  0.3447 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16206...  Training loss: 3.6858...  0.3459 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16207...  Training loss: 3.7238...  0.3444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16208...  Training loss: 3.7192...  0.3431 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16209...  Training loss: 3.7745...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16210...  Training loss: 3.6879...  0.3466 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16211...  Training loss: 3.7977...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16212...  Training loss: 3.8283...  0.3452 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16213...  Training loss: 3.7295...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16214...  Training loss: 3.6461...  0.3462 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16215...  Training loss: 3.6895...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16216...  Training loss: 3.7297...  0.3415 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16217...  Training loss: 3.7955...  0.3421 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16218...  Training loss: 3.7644...  0.3415 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16219...  Training loss: 3.8208...  0.3428 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16220...  Training loss: 3.7582...  0.3430 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16221...  Training loss: 3.7912...  0.3448 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16222...  Training loss: 3.8674...  0.3443 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16223...  Training loss: 3.8382...  0.3447 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16224...  Training loss: 3.7807...  0.3448 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16225...  Training loss: 3.6931...  0.3452 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16226...  Training loss: 3.7180...  0.3415 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16227...  Training loss: 3.7908...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16228...  Training loss: 3.8492...  0.3423 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16229...  Training loss: 3.8405...  0.3433 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16230...  Training loss: 3.7736...  0.3455 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16231...  Training loss: 3.8380...  0.3400 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16232...  Training loss: 3.8849...  0.3449 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16233...  Training loss: 3.8407...  0.3451 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16234...  Training loss: 3.8584...  0.3405 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16235...  Training loss: 3.9667...  0.3429 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16236...  Training loss: 3.8623...  0.3466 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16237...  Training loss: 3.9013...  0.3463 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16238...  Training loss: 3.8271...  0.3432 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16239...  Training loss: 3.8750...  0.3430 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16240...  Training loss: 3.8134...  0.3457 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16241...  Training loss: 3.8416...  0.3454 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16242...  Training loss: 3.8605...  0.3445 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16243...  Training loss: 3.8721...  0.3436 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16244...  Training loss: 3.9079...  0.3415 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16245...  Training loss: 3.9111...  0.3445 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16246...  Training loss: 3.8846...  0.3452 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16247...  Training loss: 3.8952...  0.3418 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16248...  Training loss: 3.8683...  0.3413 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16249...  Training loss: 3.8326...  0.3428 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16250...  Training loss: 3.8854...  0.3423 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16251...  Training loss: 3.7674...  0.3440 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16252...  Training loss: 3.7602...  0.3435 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16253...  Training loss: 3.8004...  0.3442 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16254...  Training loss: 3.8512...  0.3427 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16255...  Training loss: 3.8036...  0.3455 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16256...  Training loss: 3.8899...  0.3444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16257...  Training loss: 3.8077...  0.3440 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16258...  Training loss: 3.8157...  0.3448 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16259...  Training loss: 3.8033...  0.3463 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16260...  Training loss: 3.7720...  0.3446 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100...  Training Step: 16261...  Training loss: 3.8286...  0.3438 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16262...  Training loss: 3.8561...  0.3416 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16263...  Training loss: 3.7897...  0.3445 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16264...  Training loss: 3.7267...  0.3404 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16265...  Training loss: 3.7593...  0.3459 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16266...  Training loss: 3.7607...  0.3436 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16267...  Training loss: 3.7907...  0.3458 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16268...  Training loss: 3.7752...  0.3422 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16269...  Training loss: 3.8074...  0.3424 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16270...  Training loss: 3.7349...  0.3442 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16271...  Training loss: 3.8194...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16272...  Training loss: 3.7655...  0.3428 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16273...  Training loss: 3.8177...  0.3426 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16274...  Training loss: 3.7947...  0.3465 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16275...  Training loss: 3.7952...  0.3454 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16276...  Training loss: 3.8058...  0.3420 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16277...  Training loss: 3.8022...  0.3447 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16278...  Training loss: 3.8476...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16279...  Training loss: 3.7970...  0.3417 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16280...  Training loss: 3.7172...  0.3450 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16281...  Training loss: 3.8208...  0.3429 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16282...  Training loss: 3.9218...  0.3465 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16283...  Training loss: 3.8289...  0.3419 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16284...  Training loss: 3.7646...  0.3450 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16285...  Training loss: 3.7853...  0.3429 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16286...  Training loss: 3.7620...  0.3447 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16287...  Training loss: 3.8481...  0.3454 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16288...  Training loss: 3.8677...  0.3457 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16289...  Training loss: 3.7974...  0.3441 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16290...  Training loss: 3.7721...  0.3456 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16291...  Training loss: 3.8062...  0.3422 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16292...  Training loss: 3.8189...  0.3446 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16293...  Training loss: 3.8106...  0.3431 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16294...  Training loss: 3.7718...  0.3426 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16295...  Training loss: 3.7618...  0.3458 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16296...  Training loss: 3.6548...  0.3445 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16297...  Training loss: 3.8011...  0.3434 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16298...  Training loss: 3.7743...  0.3434 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16299...  Training loss: 3.7630...  0.3412 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16300...  Training loss: 3.7715...  0.3418 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16301...  Training loss: 3.7885...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16302...  Training loss: 3.8097...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16303...  Training loss: 3.9087...  0.3421 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16304...  Training loss: 3.8192...  0.3443 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16305...  Training loss: 3.7824...  0.3443 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16306...  Training loss: 3.7539...  0.3443 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16307...  Training loss: 3.8208...  0.3430 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16308...  Training loss: 3.7660...  0.3445 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16309...  Training loss: 3.7555...  0.3404 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16310...  Training loss: 3.7115...  0.3450 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16311...  Training loss: 3.8335...  0.3446 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16312...  Training loss: 3.7814...  0.3435 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16313...  Training loss: 3.9230...  0.3442 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16314...  Training loss: 3.9368...  0.3415 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16315...  Training loss: 3.9224...  0.3409 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16316...  Training loss: 3.7734...  0.3425 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16317...  Training loss: 3.8639...  0.3431 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16318...  Training loss: 3.8526...  0.3428 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16319...  Training loss: 3.8355...  0.3432 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16320...  Training loss: 3.8398...  0.3429 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16321...  Training loss: 3.8870...  0.3451 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16322...  Training loss: 3.8771...  0.3450 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16323...  Training loss: 3.8858...  0.3455 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16324...  Training loss: 3.8342...  0.3457 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16325...  Training loss: 3.8455...  0.3440 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16326...  Training loss: 3.8165...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16327...  Training loss: 3.8707...  0.3411 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16328...  Training loss: 3.8259...  0.3425 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16329...  Training loss: 3.8259...  0.3413 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16330...  Training loss: 3.8461...  0.3425 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16331...  Training loss: 3.8699...  0.3420 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16332...  Training loss: 3.7521...  0.3439 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16333...  Training loss: 3.8361...  0.3452 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16334...  Training loss: 3.8162...  0.3408 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16335...  Training loss: 3.7780...  0.3423 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16336...  Training loss: 3.7800...  0.3436 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16337...  Training loss: 3.8057...  0.3439 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16338...  Training loss: 3.8009...  0.3425 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16339...  Training loss: 3.8074...  0.3449 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16340...  Training loss: 3.8286...  0.3407 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16341...  Training loss: 3.8203...  0.3432 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16342...  Training loss: 3.7471...  0.3439 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16343...  Training loss: 3.8331...  0.3433 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16344...  Training loss: 3.7962...  0.3464 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16345...  Training loss: 3.7951...  0.3406 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16346...  Training loss: 3.8320...  0.3465 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16347...  Training loss: 3.8332...  0.3446 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16348...  Training loss: 3.8461...  0.3419 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16349...  Training loss: 3.8294...  0.3444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16350...  Training loss: 3.8939...  0.3440 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16351...  Training loss: 3.7355...  0.3397 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16352...  Training loss: 3.7690...  0.3423 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16353...  Training loss: 3.7836...  0.3422 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16354...  Training loss: 3.8287...  0.3441 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16355...  Training loss: 3.7744...  0.3441 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16356...  Training loss: 3.7289...  0.3446 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100...  Training Step: 16357...  Training loss: 3.8171...  0.3444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16358...  Training loss: 3.7562...  0.3414 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16359...  Training loss: 3.7257...  0.3442 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16360...  Training loss: 3.7824...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16361...  Training loss: 3.8739...  0.3440 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16362...  Training loss: 3.8601...  0.3454 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16363...  Training loss: 3.8138...  0.3429 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16364...  Training loss: 3.8005...  0.3453 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16365...  Training loss: 3.7004...  0.3467 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16366...  Training loss: 3.7351...  0.3434 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16367...  Training loss: 3.6751...  0.3444 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16368...  Training loss: 3.8176...  0.3451 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16369...  Training loss: 3.8370...  0.3450 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16370...  Training loss: 3.7871...  0.3437 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16371...  Training loss: 3.7986...  0.3412 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16372...  Training loss: 3.7613...  0.3454 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16373...  Training loss: 3.7294...  0.3409 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16374...  Training loss: 3.7358...  0.3428 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16375...  Training loss: 3.7619...  0.3432 sec/batch\n",
      "Epoch: 89/100...  Training Step: 16376...  Training loss: 3.7891...  0.3432 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16377...  Training loss: 3.6870...  0.3408 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16378...  Training loss: 3.5253...  0.3402 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16379...  Training loss: 3.5236...  0.3424 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16380...  Training loss: 3.6197...  0.3413 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16381...  Training loss: 3.5914...  0.3400 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16382...  Training loss: 3.5146...  0.3415 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16383...  Training loss: 3.7109...  0.3403 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16384...  Training loss: 3.7347...  0.3443 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16385...  Training loss: 3.7205...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16386...  Training loss: 3.7663...  0.3397 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16387...  Training loss: 3.7398...  0.3436 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16388...  Training loss: 3.7192...  0.3436 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16389...  Training loss: 3.7281...  0.3423 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16390...  Training loss: 3.6844...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16391...  Training loss: 3.7364...  0.3397 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16392...  Training loss: 3.7057...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16393...  Training loss: 3.7695...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16394...  Training loss: 3.7203...  0.3393 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16395...  Training loss: 3.7739...  0.3450 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16396...  Training loss: 3.8227...  0.3402 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16397...  Training loss: 3.7126...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16398...  Training loss: 3.6361...  0.3396 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16399...  Training loss: 3.6535...  0.3405 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16400...  Training loss: 3.6979...  0.3406 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16401...  Training loss: 3.7882...  0.3448 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16402...  Training loss: 3.7682...  0.3388 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16403...  Training loss: 3.7880...  0.3404 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16404...  Training loss: 3.7350...  0.3405 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16405...  Training loss: 3.7865...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16406...  Training loss: 3.8475...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16407...  Training loss: 3.8236...  0.3382 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16408...  Training loss: 3.7877...  0.3418 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16409...  Training loss: 3.6515...  0.3388 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16410...  Training loss: 3.7149...  0.3425 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16411...  Training loss: 3.7853...  0.3394 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16412...  Training loss: 3.8371...  0.3433 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16413...  Training loss: 3.8616...  0.3410 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16414...  Training loss: 3.7806...  0.3426 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16415...  Training loss: 3.8179...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16416...  Training loss: 3.8405...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16417...  Training loss: 3.8360...  0.3402 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16418...  Training loss: 3.8624...  0.3417 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16419...  Training loss: 3.9387...  0.3413 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16420...  Training loss: 3.8432...  0.3418 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16421...  Training loss: 3.8246...  0.3404 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16422...  Training loss: 3.8351...  0.3417 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16423...  Training loss: 3.8819...  0.3421 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16424...  Training loss: 3.8250...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16425...  Training loss: 3.7910...  0.3415 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16426...  Training loss: 3.8487...  0.3418 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16427...  Training loss: 3.9074...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16428...  Training loss: 3.9140...  0.3408 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16429...  Training loss: 3.9315...  0.3398 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16430...  Training loss: 3.8594...  0.3391 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16431...  Training loss: 3.8819...  0.3428 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16432...  Training loss: 3.8918...  0.3413 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16433...  Training loss: 3.8151...  0.3404 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16434...  Training loss: 3.8690...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16435...  Training loss: 3.7500...  0.3423 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16436...  Training loss: 3.7761...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16437...  Training loss: 3.8024...  0.3414 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16438...  Training loss: 3.8235...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16439...  Training loss: 3.7870...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16440...  Training loss: 3.8381...  0.3400 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16441...  Training loss: 3.8025...  0.3422 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16442...  Training loss: 3.7893...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16443...  Training loss: 3.8113...  0.3423 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16444...  Training loss: 3.7786...  0.3403 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16445...  Training loss: 3.8061...  0.3418 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16446...  Training loss: 3.8397...  0.3450 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16447...  Training loss: 3.7834...  0.3468 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16448...  Training loss: 3.7027...  0.3431 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16449...  Training loss: 3.7896...  0.3472 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16450...  Training loss: 3.7731...  0.3437 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16451...  Training loss: 3.7642...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16452...  Training loss: 3.7797...  0.3413 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90/100...  Training Step: 16453...  Training loss: 3.7883...  0.3435 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16454...  Training loss: 3.7358...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16455...  Training loss: 3.8028...  0.3423 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16456...  Training loss: 3.7332...  0.3406 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16457...  Training loss: 3.8118...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16458...  Training loss: 3.7929...  0.3422 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16459...  Training loss: 3.7962...  0.3437 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16460...  Training loss: 3.7724...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16461...  Training loss: 3.7765...  0.3429 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16462...  Training loss: 3.8195...  0.3400 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16463...  Training loss: 3.7640...  0.3417 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16464...  Training loss: 3.7166...  0.3442 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16465...  Training loss: 3.7887...  0.3446 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16466...  Training loss: 3.8911...  0.3428 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16467...  Training loss: 3.8261...  0.3446 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16468...  Training loss: 3.7439...  0.3422 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16469...  Training loss: 3.7612...  0.3458 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16470...  Training loss: 3.7530...  0.3418 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16471...  Training loss: 3.8556...  0.3413 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16472...  Training loss: 3.8609...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16473...  Training loss: 3.7965...  0.3404 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16474...  Training loss: 3.7877...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16475...  Training loss: 3.7930...  0.3397 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16476...  Training loss: 3.8165...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16477...  Training loss: 3.8255...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16478...  Training loss: 3.7484...  0.3384 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16479...  Training loss: 3.7695...  0.3411 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16480...  Training loss: 3.6887...  0.3397 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16481...  Training loss: 3.7638...  0.3400 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16482...  Training loss: 3.7619...  0.3408 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16483...  Training loss: 3.7215...  0.3410 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16484...  Training loss: 3.7360...  0.3453 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16485...  Training loss: 3.7713...  0.3424 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16486...  Training loss: 3.7882...  0.3414 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16487...  Training loss: 3.8860...  0.3392 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16488...  Training loss: 3.7812...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16489...  Training loss: 3.8122...  0.3404 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16490...  Training loss: 3.7443...  0.3436 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16491...  Training loss: 3.8677...  0.3440 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16492...  Training loss: 3.7636...  0.3438 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16493...  Training loss: 3.7934...  0.3442 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16494...  Training loss: 3.7055...  0.3451 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16495...  Training loss: 3.7913...  0.3434 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16496...  Training loss: 3.7852...  0.3455 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16497...  Training loss: 3.9026...  0.3468 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16498...  Training loss: 3.9140...  0.3452 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16499...  Training loss: 3.9013...  0.3453 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16500...  Training loss: 3.7665...  0.3441 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16501...  Training loss: 3.8211...  0.3410 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16502...  Training loss: 3.8427...  0.3399 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16503...  Training loss: 3.8489...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16504...  Training loss: 3.8474...  0.3415 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16505...  Training loss: 3.8775...  0.3437 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16506...  Training loss: 3.9029...  0.3440 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16507...  Training loss: 3.8905...  0.3426 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16508...  Training loss: 3.7864...  0.3449 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16509...  Training loss: 3.8516...  0.3451 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16510...  Training loss: 3.8200...  0.3465 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16511...  Training loss: 3.8534...  0.3444 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16512...  Training loss: 3.8066...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16513...  Training loss: 3.8102...  0.3438 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16514...  Training loss: 3.8288...  0.3454 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16515...  Training loss: 3.8861...  0.3403 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16516...  Training loss: 3.7623...  0.3430 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16517...  Training loss: 3.8384...  0.3413 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16518...  Training loss: 3.8121...  0.3421 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16519...  Training loss: 3.7892...  0.3442 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16520...  Training loss: 3.7579...  0.3416 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16521...  Training loss: 3.8347...  0.3466 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16522...  Training loss: 3.8172...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16523...  Training loss: 3.7587...  0.3440 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16524...  Training loss: 3.8320...  0.3462 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16525...  Training loss: 3.8251...  0.3423 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16526...  Training loss: 3.7333...  0.3431 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16527...  Training loss: 3.7907...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16528...  Training loss: 3.8081...  0.3444 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16529...  Training loss: 3.7771...  0.3431 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16530...  Training loss: 3.8333...  0.3449 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16531...  Training loss: 3.8220...  0.3409 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16532...  Training loss: 3.8378...  0.3453 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16533...  Training loss: 3.8290...  0.3450 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16534...  Training loss: 3.8614...  0.3427 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16535...  Training loss: 3.7211...  0.3434 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16536...  Training loss: 3.7714...  0.3411 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16537...  Training loss: 3.8011...  0.3441 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16538...  Training loss: 3.8198...  0.3446 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16539...  Training loss: 3.7636...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16540...  Training loss: 3.6994...  0.3419 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16541...  Training loss: 3.7755...  0.3448 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16542...  Training loss: 3.7349...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16543...  Training loss: 3.7267...  0.3419 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16544...  Training loss: 3.7667...  0.3445 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16545...  Training loss: 3.8670...  0.3436 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16546...  Training loss: 3.8644...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16547...  Training loss: 3.8026...  0.3441 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16548...  Training loss: 3.7763...  0.3440 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90/100...  Training Step: 16549...  Training loss: 3.7298...  0.3441 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16550...  Training loss: 3.7217...  0.3443 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16551...  Training loss: 3.6844...  0.3439 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16552...  Training loss: 3.7753...  0.3430 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16553...  Training loss: 3.8238...  0.3441 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16554...  Training loss: 3.7652...  0.3444 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16555...  Training loss: 3.7790...  0.3412 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16556...  Training loss: 3.7432...  0.3452 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16557...  Training loss: 3.6970...  0.3446 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16558...  Training loss: 3.7229...  0.3451 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16559...  Training loss: 3.7117...  0.3407 sec/batch\n",
      "Epoch: 90/100...  Training Step: 16560...  Training loss: 3.7728...  0.3459 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16561...  Training loss: 3.6918...  0.3446 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16562...  Training loss: 3.5094...  0.3428 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16563...  Training loss: 3.5098...  0.3448 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16564...  Training loss: 3.5951...  0.3449 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16565...  Training loss: 3.5951...  0.3437 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16566...  Training loss: 3.5291...  0.3448 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16567...  Training loss: 3.7017...  0.3425 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16568...  Training loss: 3.7624...  0.3410 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16569...  Training loss: 3.7049...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16570...  Training loss: 3.7385...  0.3444 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16571...  Training loss: 3.6641...  0.3402 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16572...  Training loss: 3.7006...  0.3451 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16573...  Training loss: 3.6873...  0.3428 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16574...  Training loss: 3.6799...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16575...  Training loss: 3.7035...  0.3440 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16576...  Training loss: 3.7281...  0.3451 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16577...  Training loss: 3.7564...  0.3397 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16578...  Training loss: 3.7163...  0.3418 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16579...  Training loss: 3.7634...  0.3420 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16580...  Training loss: 3.7926...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16581...  Training loss: 3.7135...  0.3454 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16582...  Training loss: 3.5928...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16583...  Training loss: 3.6447...  0.3440 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16584...  Training loss: 3.6721...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16585...  Training loss: 3.7498...  0.3430 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16586...  Training loss: 3.7510...  0.3425 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16587...  Training loss: 3.7839...  0.3428 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16588...  Training loss: 3.7556...  0.3404 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16589...  Training loss: 3.7804...  0.3397 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16590...  Training loss: 3.8467...  0.3406 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16591...  Training loss: 3.8020...  0.3449 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16592...  Training loss: 3.7814...  0.3426 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16593...  Training loss: 3.6636...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16594...  Training loss: 3.6952...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16595...  Training loss: 3.7842...  0.3400 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16596...  Training loss: 3.8285...  0.3410 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16597...  Training loss: 3.8398...  0.3408 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16598...  Training loss: 3.7777...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16599...  Training loss: 3.8439...  0.3449 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16600...  Training loss: 3.8624...  0.3409 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16601...  Training loss: 3.8321...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16602...  Training loss: 3.8529...  0.3394 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16603...  Training loss: 3.8902...  0.3420 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16604...  Training loss: 3.8457...  0.3420 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16605...  Training loss: 3.8163...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16606...  Training loss: 3.8086...  0.3423 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16607...  Training loss: 3.8418...  0.3419 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16608...  Training loss: 3.7898...  0.3442 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16609...  Training loss: 3.7796...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16610...  Training loss: 3.8369...  0.3406 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16611...  Training loss: 3.8488...  0.3423 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16612...  Training loss: 3.9029...  0.3419 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16613...  Training loss: 3.9057...  0.3464 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16614...  Training loss: 3.8683...  0.3448 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16615...  Training loss: 3.8838...  0.3434 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16616...  Training loss: 3.8719...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16617...  Training loss: 3.8092...  0.3410 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16618...  Training loss: 3.8604...  0.3403 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16619...  Training loss: 3.7360...  0.3422 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16620...  Training loss: 3.7312...  0.3416 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16621...  Training loss: 3.7791...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16622...  Training loss: 3.7874...  0.3415 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16623...  Training loss: 3.8059...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16624...  Training loss: 3.8752...  0.3426 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16625...  Training loss: 3.7744...  0.3429 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16626...  Training loss: 3.7722...  0.3442 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16627...  Training loss: 3.7869...  0.3446 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16628...  Training loss: 3.7466...  0.3416 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16629...  Training loss: 3.8185...  0.3431 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16630...  Training loss: 3.8389...  0.3446 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16631...  Training loss: 3.7717...  0.3440 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16632...  Training loss: 3.7276...  0.3453 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16633...  Training loss: 3.7372...  0.3436 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16634...  Training loss: 3.7582...  0.3448 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16635...  Training loss: 3.7773...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16636...  Training loss: 3.7827...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16637...  Training loss: 3.7796...  0.3445 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16638...  Training loss: 3.7264...  0.3412 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16639...  Training loss: 3.7924...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16640...  Training loss: 3.7325...  0.3415 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16641...  Training loss: 3.8016...  0.3445 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16642...  Training loss: 3.7822...  0.3404 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16643...  Training loss: 3.7855...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16644...  Training loss: 3.7616...  0.3414 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100...  Training Step: 16645...  Training loss: 3.7959...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16646...  Training loss: 3.8286...  0.3432 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16647...  Training loss: 3.7484...  0.3411 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16648...  Training loss: 3.7173...  0.3451 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16649...  Training loss: 3.8242...  0.3415 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16650...  Training loss: 3.8876...  0.3422 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16651...  Training loss: 3.8176...  0.3456 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16652...  Training loss: 3.7427...  0.3464 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16653...  Training loss: 3.7619...  0.3459 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16654...  Training loss: 3.7461...  0.3457 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16655...  Training loss: 3.8240...  0.3423 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16656...  Training loss: 3.8414...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16657...  Training loss: 3.7816...  0.3421 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16658...  Training loss: 3.7456...  0.3462 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16659...  Training loss: 3.7753...  0.3450 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16660...  Training loss: 3.8040...  0.3462 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16661...  Training loss: 3.8189...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16662...  Training loss: 3.7643...  0.3421 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16663...  Training loss: 3.7572...  0.3424 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16664...  Training loss: 3.6614...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16665...  Training loss: 3.7542...  0.3418 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16666...  Training loss: 3.7853...  0.3460 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16667...  Training loss: 3.6913...  0.3394 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16668...  Training loss: 3.7500...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16669...  Training loss: 3.7248...  0.3416 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16670...  Training loss: 3.7574...  0.3449 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16671...  Training loss: 3.8710...  0.3442 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16672...  Training loss: 3.7936...  0.3422 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16673...  Training loss: 3.7662...  0.3440 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16674...  Training loss: 3.7452...  0.3452 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16675...  Training loss: 3.8435...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16676...  Training loss: 3.7727...  0.3442 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16677...  Training loss: 3.7669...  0.3435 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16678...  Training loss: 3.7124...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16679...  Training loss: 3.7961...  0.3411 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16680...  Training loss: 3.7758...  0.3431 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16681...  Training loss: 3.9295...  0.3429 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16682...  Training loss: 3.8927...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16683...  Training loss: 3.8896...  0.3452 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16684...  Training loss: 3.7555...  0.3436 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16685...  Training loss: 3.8578...  0.3430 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16686...  Training loss: 3.8702...  0.3406 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16687...  Training loss: 3.8563...  0.3407 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16688...  Training loss: 3.8234...  0.3410 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16689...  Training loss: 3.8501...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16690...  Training loss: 3.8888...  0.3450 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16691...  Training loss: 3.8623...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16692...  Training loss: 3.7837...  0.3442 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16693...  Training loss: 3.8374...  0.3418 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16694...  Training loss: 3.7900...  0.3409 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16695...  Training loss: 3.8265...  0.3418 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16696...  Training loss: 3.7739...  0.3433 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16697...  Training loss: 3.7969...  0.3416 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16698...  Training loss: 3.8127...  0.3398 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16699...  Training loss: 3.8883...  0.3424 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16700...  Training loss: 3.7278...  0.3461 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16701...  Training loss: 3.8499...  0.3454 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16702...  Training loss: 3.8460...  0.3441 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16703...  Training loss: 3.7905...  0.3429 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16704...  Training loss: 3.7738...  0.3419 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16705...  Training loss: 3.8295...  0.3420 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16706...  Training loss: 3.8001...  0.3428 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16707...  Training loss: 3.7563...  0.3433 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16708...  Training loss: 3.8119...  0.3396 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16709...  Training loss: 3.8394...  0.3447 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16710...  Training loss: 3.7557...  0.3467 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16711...  Training loss: 3.8097...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16712...  Training loss: 3.8061...  0.3433 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16713...  Training loss: 3.7962...  0.3403 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16714...  Training loss: 3.8129...  0.3414 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16715...  Training loss: 3.8175...  0.3427 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16716...  Training loss: 3.8468...  0.3408 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16717...  Training loss: 3.8061...  0.3404 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16718...  Training loss: 3.8781...  0.3444 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16719...  Training loss: 3.7125...  0.3413 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16720...  Training loss: 3.7608...  0.3418 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16721...  Training loss: 3.7822...  0.3436 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16722...  Training loss: 3.8413...  0.3431 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16723...  Training loss: 3.7522...  0.3432 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16724...  Training loss: 3.7202...  0.3432 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16725...  Training loss: 3.7570...  0.3403 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16726...  Training loss: 3.7701...  0.3439 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16727...  Training loss: 3.7232...  0.3426 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16728...  Training loss: 3.7265...  0.3463 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16729...  Training loss: 3.8183...  0.3432 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16730...  Training loss: 3.8253...  0.3422 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16731...  Training loss: 3.7625...  0.3453 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16732...  Training loss: 3.7465...  0.3449 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16733...  Training loss: 3.6983...  0.3438 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16734...  Training loss: 3.7151...  0.3431 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16735...  Training loss: 3.6818...  0.3404 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16736...  Training loss: 3.8085...  0.3423 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16737...  Training loss: 3.8154...  0.3444 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16738...  Training loss: 3.7530...  0.3433 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16739...  Training loss: 3.7385...  0.3421 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16740...  Training loss: 3.7378...  0.3435 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/100...  Training Step: 16741...  Training loss: 3.6722...  0.3430 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16742...  Training loss: 3.7417...  0.3443 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16743...  Training loss: 3.7461...  0.3448 sec/batch\n",
      "Epoch: 91/100...  Training Step: 16744...  Training loss: 3.7555...  0.3435 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16745...  Training loss: 3.6931...  0.3435 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16746...  Training loss: 3.4992...  0.3419 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16747...  Training loss: 3.4792...  0.3446 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16748...  Training loss: 3.5894...  0.3421 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16749...  Training loss: 3.5582...  0.3406 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16750...  Training loss: 3.5222...  0.3414 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16751...  Training loss: 3.6806...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16752...  Training loss: 3.7292...  0.3420 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16753...  Training loss: 3.6715...  0.3402 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16754...  Training loss: 3.7160...  0.3434 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16755...  Training loss: 3.6998...  0.3449 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16756...  Training loss: 3.6566...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16757...  Training loss: 3.7166...  0.3422 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16758...  Training loss: 3.6558...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16759...  Training loss: 3.6778...  0.3427 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16760...  Training loss: 3.6811...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16761...  Training loss: 3.7595...  0.3430 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16762...  Training loss: 3.7049...  0.3400 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16763...  Training loss: 3.7216...  0.3452 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16764...  Training loss: 3.7335...  0.3461 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16765...  Training loss: 3.7153...  0.3414 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16766...  Training loss: 3.6111...  0.3419 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16767...  Training loss: 3.6069...  0.3439 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16768...  Training loss: 3.6553...  0.3452 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16769...  Training loss: 3.7328...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16770...  Training loss: 3.7541...  0.3439 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16771...  Training loss: 3.7790...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16772...  Training loss: 3.7196...  0.3427 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16773...  Training loss: 3.7457...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16774...  Training loss: 3.8318...  0.3429 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16775...  Training loss: 3.8148...  0.3437 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16776...  Training loss: 3.7405...  0.3444 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16777...  Training loss: 3.6457...  0.3424 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16778...  Training loss: 3.6950...  0.3417 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16779...  Training loss: 3.7583...  0.3425 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16780...  Training loss: 3.8185...  0.3429 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16781...  Training loss: 3.8365...  0.3400 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16782...  Training loss: 3.7907...  0.3429 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16783...  Training loss: 3.8187...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16784...  Training loss: 3.8618...  0.3431 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16785...  Training loss: 3.8266...  0.3432 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16786...  Training loss: 3.8800...  0.3411 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16787...  Training loss: 3.9112...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16788...  Training loss: 3.8370...  0.3418 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16789...  Training loss: 3.7932...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16790...  Training loss: 3.8247...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16791...  Training loss: 3.8406...  0.3432 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16792...  Training loss: 3.7812...  0.3410 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16793...  Training loss: 3.7888...  0.3411 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16794...  Training loss: 3.8323...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16795...  Training loss: 3.8650...  0.3454 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16796...  Training loss: 3.9150...  0.3429 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16797...  Training loss: 3.8929...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16798...  Training loss: 3.8387...  0.3448 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16799...  Training loss: 3.8674...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16800...  Training loss: 3.8590...  0.3434 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16801...  Training loss: 3.7859...  0.3411 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16802...  Training loss: 3.8271...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16803...  Training loss: 3.7482...  0.3435 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16804...  Training loss: 3.7181...  0.3434 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16805...  Training loss: 3.7918...  0.3421 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16806...  Training loss: 3.8170...  0.3433 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16807...  Training loss: 3.7901...  0.3453 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16808...  Training loss: 3.8420...  0.3414 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16809...  Training loss: 3.7938...  0.3413 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16810...  Training loss: 3.7634...  0.3434 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16811...  Training loss: 3.7427...  0.3410 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16812...  Training loss: 3.7397...  0.3422 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16813...  Training loss: 3.7840...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16814...  Training loss: 3.8147...  0.3454 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16815...  Training loss: 3.7260...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16816...  Training loss: 3.7003...  0.3444 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16817...  Training loss: 3.7477...  0.3418 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16818...  Training loss: 3.7296...  0.3406 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16819...  Training loss: 3.7466...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16820...  Training loss: 3.7584...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16821...  Training loss: 3.7787...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16822...  Training loss: 3.7182...  0.3424 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16823...  Training loss: 3.7728...  0.3419 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16824...  Training loss: 3.7082...  0.3417 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16825...  Training loss: 3.7844...  0.3424 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16826...  Training loss: 3.7445...  0.3407 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16827...  Training loss: 3.7554...  0.3434 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16828...  Training loss: 3.7399...  0.3461 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16829...  Training loss: 3.7822...  0.3419 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16830...  Training loss: 3.8589...  0.3431 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16831...  Training loss: 3.7751...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16832...  Training loss: 3.6907...  0.3430 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16833...  Training loss: 3.7793...  0.3444 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16834...  Training loss: 3.8693...  0.3437 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16835...  Training loss: 3.7648...  0.3443 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16836...  Training loss: 3.7006...  0.3444 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92/100...  Training Step: 16837...  Training loss: 3.7155...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16838...  Training loss: 3.7238...  0.3422 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16839...  Training loss: 3.8239...  0.3418 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16840...  Training loss: 3.8122...  0.3424 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16841...  Training loss: 3.7528...  0.3426 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16842...  Training loss: 3.7476...  0.3446 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16843...  Training loss: 3.7520...  0.3450 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16844...  Training loss: 3.7944...  0.3429 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16845...  Training loss: 3.8248...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16846...  Training loss: 3.7343...  0.3443 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16847...  Training loss: 3.7570...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16848...  Training loss: 3.6486...  0.3433 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16849...  Training loss: 3.7529...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16850...  Training loss: 3.7514...  0.3465 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16851...  Training loss: 3.6932...  0.3427 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16852...  Training loss: 3.7318...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16853...  Training loss: 3.7340...  0.3449 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16854...  Training loss: 3.7430...  0.3422 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16855...  Training loss: 3.8329...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16856...  Training loss: 3.7770...  0.3454 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16857...  Training loss: 3.7638...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16858...  Training loss: 3.7517...  0.3404 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16859...  Training loss: 3.8264...  0.3420 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16860...  Training loss: 3.7693...  0.3430 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16861...  Training loss: 3.7823...  0.3453 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16862...  Training loss: 3.6992...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16863...  Training loss: 3.8132...  0.3446 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16864...  Training loss: 3.7565...  0.3425 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16865...  Training loss: 3.8734...  0.3425 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16866...  Training loss: 3.8713...  0.3410 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16867...  Training loss: 3.8695...  0.3436 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16868...  Training loss: 3.7484...  0.3443 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16869...  Training loss: 3.8260...  0.3448 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16870...  Training loss: 3.8480...  0.3425 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16871...  Training loss: 3.8158...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16872...  Training loss: 3.8035...  0.3412 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16873...  Training loss: 3.8617...  0.3409 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16874...  Training loss: 3.8848...  0.3407 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16875...  Training loss: 3.8848...  0.3403 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16876...  Training loss: 3.7930...  0.3402 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16877...  Training loss: 3.7987...  0.3436 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16878...  Training loss: 3.7932...  0.3442 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16879...  Training loss: 3.8303...  0.3398 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16880...  Training loss: 3.7549...  0.3417 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16881...  Training loss: 3.7351...  0.3437 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16882...  Training loss: 3.7966...  0.3460 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16883...  Training loss: 3.8583...  0.3445 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16884...  Training loss: 3.6974...  0.3452 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16885...  Training loss: 3.8181...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16886...  Training loss: 3.8394...  0.3413 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16887...  Training loss: 3.8046...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16888...  Training loss: 3.7741...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16889...  Training loss: 3.8315...  0.3410 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16890...  Training loss: 3.7946...  0.3427 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16891...  Training loss: 3.7833...  0.3401 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16892...  Training loss: 3.8065...  0.3448 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16893...  Training loss: 3.8079...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16894...  Training loss: 3.7245...  0.3414 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16895...  Training loss: 3.8137...  0.3447 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16896...  Training loss: 3.7998...  0.3414 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16897...  Training loss: 3.7807...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16898...  Training loss: 3.8438...  0.3416 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16899...  Training loss: 3.7988...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16900...  Training loss: 3.8204...  0.3427 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16901...  Training loss: 3.8241...  0.3432 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16902...  Training loss: 3.8516...  0.3397 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16903...  Training loss: 3.7004...  0.3439 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16904...  Training loss: 3.7372...  0.3411 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16905...  Training loss: 3.7760...  0.3405 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16906...  Training loss: 3.7599...  0.3440 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16907...  Training loss: 3.7195...  0.3432 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16908...  Training loss: 3.6985...  0.3441 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16909...  Training loss: 3.8042...  0.3423 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16910...  Training loss: 3.7411...  0.3415 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16911...  Training loss: 3.7334...  0.3448 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16912...  Training loss: 3.7472...  0.3442 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16913...  Training loss: 3.8606...  0.3422 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16914...  Training loss: 3.8473...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16915...  Training loss: 3.7655...  0.3418 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16916...  Training loss: 3.7316...  0.3409 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16917...  Training loss: 3.7118...  0.3408 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16918...  Training loss: 3.7040...  0.3419 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16919...  Training loss: 3.6668...  0.3453 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16920...  Training loss: 3.7955...  0.3437 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16921...  Training loss: 3.8007...  0.3438 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16922...  Training loss: 3.7692...  0.3448 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16923...  Training loss: 3.7788...  0.3420 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16924...  Training loss: 3.7479...  0.3428 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16925...  Training loss: 3.7085...  0.3416 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16926...  Training loss: 3.7290...  0.3451 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16927...  Training loss: 3.7208...  0.3444 sec/batch\n",
      "Epoch: 92/100...  Training Step: 16928...  Training loss: 3.7693...  0.3445 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16929...  Training loss: 3.6849...  0.3444 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16930...  Training loss: 3.5203...  0.3445 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16931...  Training loss: 3.5036...  0.3436 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16932...  Training loss: 3.5923...  0.3450 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100...  Training Step: 16933...  Training loss: 3.5473...  0.3449 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16934...  Training loss: 3.5608...  0.3413 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16935...  Training loss: 3.6903...  0.3398 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16936...  Training loss: 3.7312...  0.3414 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16937...  Training loss: 3.6833...  0.3414 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16938...  Training loss: 3.7386...  0.3444 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16939...  Training loss: 3.6558...  0.3417 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16940...  Training loss: 3.6564...  0.3417 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16941...  Training loss: 3.6840...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16942...  Training loss: 3.6176...  0.3449 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16943...  Training loss: 3.6702...  0.3446 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16944...  Training loss: 3.6632...  0.3439 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16945...  Training loss: 3.7320...  0.3452 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16946...  Training loss: 3.6565...  0.3438 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16947...  Training loss: 3.7512...  0.3448 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16948...  Training loss: 3.7548...  0.3454 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16949...  Training loss: 3.6841...  0.3440 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16950...  Training loss: 3.5853...  0.3453 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16951...  Training loss: 3.6473...  0.3423 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16952...  Training loss: 3.6514...  0.3422 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16953...  Training loss: 3.7452...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16954...  Training loss: 3.7138...  0.3447 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16955...  Training loss: 3.7480...  0.3452 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16956...  Training loss: 3.7222...  0.3412 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16957...  Training loss: 3.7472...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16958...  Training loss: 3.8347...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16959...  Training loss: 3.8020...  0.3437 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16960...  Training loss: 3.7371...  0.3407 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16961...  Training loss: 3.6501...  0.3414 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16962...  Training loss: 3.6576...  0.3403 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16963...  Training loss: 3.7507...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16964...  Training loss: 3.8327...  0.3445 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16965...  Training loss: 3.7982...  0.3443 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16966...  Training loss: 3.7394...  0.3444 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16967...  Training loss: 3.7780...  0.3428 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16968...  Training loss: 3.8123...  0.3465 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16969...  Training loss: 3.8019...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16970...  Training loss: 3.8359...  0.3450 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16971...  Training loss: 3.8865...  0.3417 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16972...  Training loss: 3.8342...  0.3424 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16973...  Training loss: 3.8372...  0.3447 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16974...  Training loss: 3.7885...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16975...  Training loss: 3.8363...  0.3427 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16976...  Training loss: 3.7977...  0.3429 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16977...  Training loss: 3.7850...  0.3442 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16978...  Training loss: 3.8379...  0.3465 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16979...  Training loss: 3.8532...  0.3437 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16980...  Training loss: 3.8982...  0.3442 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16981...  Training loss: 3.9111...  0.3442 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16982...  Training loss: 3.8657...  0.3446 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16983...  Training loss: 3.8879...  0.3439 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16984...  Training loss: 3.8797...  0.3448 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16985...  Training loss: 3.8152...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16986...  Training loss: 3.8377...  0.3438 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16987...  Training loss: 3.7385...  0.3442 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16988...  Training loss: 3.7206...  0.3457 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16989...  Training loss: 3.7819...  0.3456 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16990...  Training loss: 3.7968...  0.3431 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16991...  Training loss: 3.7929...  0.3443 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16992...  Training loss: 3.8478...  0.3406 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16993...  Training loss: 3.7711...  0.3435 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16994...  Training loss: 3.7595...  0.3421 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16995...  Training loss: 3.7689...  0.3451 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16996...  Training loss: 3.7209...  0.3437 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16997...  Training loss: 3.7590...  0.3419 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16998...  Training loss: 3.7865...  0.3400 sec/batch\n",
      "Epoch: 93/100...  Training Step: 16999...  Training loss: 3.7286...  0.3450 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17000...  Training loss: 3.7081...  0.3405 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17001...  Training loss: 3.7450...  0.3844 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17002...  Training loss: 3.7318...  0.3465 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17003...  Training loss: 3.7567...  0.3437 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17004...  Training loss: 3.7432...  0.3436 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17005...  Training loss: 3.7690...  0.3439 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17006...  Training loss: 3.6688...  0.3400 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17007...  Training loss: 3.7458...  0.3421 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17008...  Training loss: 3.7244...  0.3422 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17009...  Training loss: 3.7811...  0.3409 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17010...  Training loss: 3.7569...  0.3402 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17011...  Training loss: 3.7903...  0.3406 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17012...  Training loss: 3.7385...  0.3384 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17013...  Training loss: 3.7754...  0.3437 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17014...  Training loss: 3.8267...  0.3434 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17015...  Training loss: 3.7564...  0.3405 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17016...  Training loss: 3.6745...  0.3436 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17017...  Training loss: 3.7534...  0.3399 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17018...  Training loss: 3.8537...  0.3428 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17019...  Training loss: 3.7774...  0.3394 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17020...  Training loss: 3.7169...  0.3423 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17021...  Training loss: 3.7757...  0.3388 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17022...  Training loss: 3.7196...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17023...  Training loss: 3.7950...  0.3428 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17024...  Training loss: 3.8052...  0.3394 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17025...  Training loss: 3.7415...  0.3417 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17026...  Training loss: 3.7078...  0.3421 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17027...  Training loss: 3.6930...  0.3393 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17028...  Training loss: 3.7526...  0.3395 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100...  Training Step: 17029...  Training loss: 3.7871...  0.3408 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17030...  Training loss: 3.7099...  0.3434 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17031...  Training loss: 3.7434...  0.3441 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17032...  Training loss: 3.6445...  0.3421 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17033...  Training loss: 3.7677...  0.3425 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17034...  Training loss: 3.7338...  0.3448 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17035...  Training loss: 3.7018...  0.3413 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17036...  Training loss: 3.7331...  0.3411 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17037...  Training loss: 3.7235...  0.3423 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17038...  Training loss: 3.7131...  0.3428 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17039...  Training loss: 3.7879...  0.3433 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17040...  Training loss: 3.6939...  0.3409 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17041...  Training loss: 3.7018...  0.3402 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17042...  Training loss: 3.7166...  0.3406 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17043...  Training loss: 3.8328...  0.3415 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17044...  Training loss: 3.7680...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17045...  Training loss: 3.7478...  0.3438 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17046...  Training loss: 3.7135...  0.3412 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17047...  Training loss: 3.7514...  0.3407 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17048...  Training loss: 3.7028...  0.3408 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17049...  Training loss: 3.8921...  0.3427 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17050...  Training loss: 3.8247...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17051...  Training loss: 3.8466...  0.3409 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17052...  Training loss: 3.7487...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17053...  Training loss: 3.8139...  0.3402 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17054...  Training loss: 3.8319...  0.3411 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17055...  Training loss: 3.7952...  0.3439 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17056...  Training loss: 3.7993...  0.3418 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17057...  Training loss: 3.8304...  0.3393 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17058...  Training loss: 3.8591...  0.3425 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17059...  Training loss: 3.8703...  0.3405 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17060...  Training loss: 3.7929...  0.3443 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17061...  Training loss: 3.7992...  0.3399 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17062...  Training loss: 3.7988...  0.3405 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17063...  Training loss: 3.8310...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17064...  Training loss: 3.7786...  0.3407 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17065...  Training loss: 3.7272...  0.3397 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17066...  Training loss: 3.7928...  0.3435 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17067...  Training loss: 3.8344...  0.3434 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17068...  Training loss: 3.7078...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17069...  Training loss: 3.7944...  0.3398 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17070...  Training loss: 3.8561...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17071...  Training loss: 3.8149...  0.3399 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17072...  Training loss: 3.7809...  0.3438 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17073...  Training loss: 3.8306...  0.3433 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17074...  Training loss: 3.7887...  0.3407 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17075...  Training loss: 3.7541...  0.3385 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17076...  Training loss: 3.7642...  0.3433 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17077...  Training loss: 3.8007...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17078...  Training loss: 3.7356...  0.3412 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17079...  Training loss: 3.7892...  0.3423 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17080...  Training loss: 3.7936...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17081...  Training loss: 3.7883...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17082...  Training loss: 3.8218...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17083...  Training loss: 3.8261...  0.3410 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17084...  Training loss: 3.8400...  0.3449 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17085...  Training loss: 3.8018...  0.3429 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17086...  Training loss: 3.8613...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17087...  Training loss: 3.6796...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17088...  Training loss: 3.7372...  0.3420 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17089...  Training loss: 3.7339...  0.3412 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17090...  Training loss: 3.7865...  0.3387 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17091...  Training loss: 3.7159...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17092...  Training loss: 3.7045...  0.3383 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17093...  Training loss: 3.7635...  0.3421 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17094...  Training loss: 3.7176...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17095...  Training loss: 3.7222...  0.3396 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17096...  Training loss: 3.7569...  0.3428 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17097...  Training loss: 3.8608...  0.3440 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17098...  Training loss: 3.8477...  0.3416 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17099...  Training loss: 3.7644...  0.3432 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17100...  Training loss: 3.7091...  0.3432 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17101...  Training loss: 3.6928...  0.3445 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17102...  Training loss: 3.6733...  0.3407 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17103...  Training loss: 3.6351...  0.3409 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17104...  Training loss: 3.7913...  0.3423 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17105...  Training loss: 3.8032...  0.3433 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17106...  Training loss: 3.7626...  0.3426 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17107...  Training loss: 3.7748...  0.3413 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17108...  Training loss: 3.7413...  0.3430 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17109...  Training loss: 3.6813...  0.3394 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17110...  Training loss: 3.7358...  0.3425 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17111...  Training loss: 3.6968...  0.3433 sec/batch\n",
      "Epoch: 93/100...  Training Step: 17112...  Training loss: 3.7615...  0.3426 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17113...  Training loss: 3.6614...  0.3425 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17114...  Training loss: 3.4961...  0.3441 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17115...  Training loss: 3.4648...  0.3413 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17116...  Training loss: 3.5972...  0.3403 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17117...  Training loss: 3.5659...  0.3399 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17118...  Training loss: 3.5250...  0.3412 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17119...  Training loss: 3.6761...  0.3427 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17120...  Training loss: 3.7298...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17121...  Training loss: 3.6507...  0.3426 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17122...  Training loss: 3.7052...  0.3421 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17123...  Training loss: 3.7050...  0.3408 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17124...  Training loss: 3.6691...  0.3402 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/100...  Training Step: 17125...  Training loss: 3.6767...  0.3408 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17126...  Training loss: 3.6344...  0.3434 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17127...  Training loss: 3.6663...  0.3428 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17128...  Training loss: 3.6630...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17129...  Training loss: 3.7158...  0.3447 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17130...  Training loss: 3.6759...  0.3397 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17131...  Training loss: 3.7172...  0.3426 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17132...  Training loss: 3.7226...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17133...  Training loss: 3.6721...  0.3416 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17134...  Training loss: 3.5830...  0.3397 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17135...  Training loss: 3.6288...  0.3431 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17136...  Training loss: 3.6576...  0.3397 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17137...  Training loss: 3.7412...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17138...  Training loss: 3.7078...  0.3434 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17139...  Training loss: 3.7474...  0.3399 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17140...  Training loss: 3.6911...  0.3425 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17141...  Training loss: 3.7498...  0.3441 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17142...  Training loss: 3.8340...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17143...  Training loss: 3.7886...  0.3406 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17144...  Training loss: 3.7252...  0.3430 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17145...  Training loss: 3.6258...  0.3432 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17146...  Training loss: 3.6534...  0.3447 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17147...  Training loss: 3.7014...  0.3419 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17148...  Training loss: 3.7621...  0.3414 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17149...  Training loss: 3.7991...  0.3400 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17150...  Training loss: 3.7448...  0.3393 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17151...  Training loss: 3.7623...  0.3427 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17152...  Training loss: 3.8075...  0.3418 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17153...  Training loss: 3.7862...  0.3435 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17154...  Training loss: 3.8399...  0.3399 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17155...  Training loss: 3.8651...  0.3425 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17156...  Training loss: 3.8319...  0.3393 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17157...  Training loss: 3.7980...  0.3401 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17158...  Training loss: 3.7785...  0.3391 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17159...  Training loss: 3.7825...  0.3390 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17160...  Training loss: 3.7186...  0.3449 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17161...  Training loss: 3.7566...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17162...  Training loss: 3.7764...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17163...  Training loss: 3.8157...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17164...  Training loss: 3.8800...  0.3435 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17165...  Training loss: 3.8801...  0.3439 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17166...  Training loss: 3.8731...  0.3428 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17167...  Training loss: 3.8780...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17168...  Training loss: 3.8388...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17169...  Training loss: 3.8070...  0.3420 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17170...  Training loss: 3.8178...  0.3403 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17171...  Training loss: 3.7365...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17172...  Training loss: 3.7071...  0.3429 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17173...  Training loss: 3.7629...  0.3419 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17174...  Training loss: 3.7928...  0.3394 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17175...  Training loss: 3.8218...  0.3432 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17176...  Training loss: 3.8597...  0.3399 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17177...  Training loss: 3.8055...  0.3388 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17178...  Training loss: 3.7658...  0.3411 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17179...  Training loss: 3.7658...  0.3429 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17180...  Training loss: 3.7072...  0.3397 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17181...  Training loss: 3.7471...  0.3405 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17182...  Training loss: 3.7958...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17183...  Training loss: 3.7346...  0.3412 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17184...  Training loss: 3.6799...  0.3418 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17185...  Training loss: 3.7191...  0.3388 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17186...  Training loss: 3.7335...  0.3408 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17187...  Training loss: 3.7219...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17188...  Training loss: 3.7434...  0.3414 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17189...  Training loss: 3.7360...  0.3422 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17190...  Training loss: 3.6816...  0.3430 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17191...  Training loss: 3.7776...  0.3417 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17192...  Training loss: 3.6950...  0.3438 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17193...  Training loss: 3.7808...  0.3446 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17194...  Training loss: 3.7390...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17195...  Training loss: 3.7348...  0.3428 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17196...  Training loss: 3.6973...  0.3443 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17197...  Training loss: 3.7865...  0.3404 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17198...  Training loss: 3.8059...  0.3413 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17199...  Training loss: 3.7360...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17200...  Training loss: 3.6798...  0.3440 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17201...  Training loss: 3.7749...  0.3394 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17202...  Training loss: 3.8302...  0.3396 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17203...  Training loss: 3.7615...  0.3438 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17204...  Training loss: 3.6898...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17205...  Training loss: 3.7125...  0.3396 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17206...  Training loss: 3.7403...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17207...  Training loss: 3.7886...  0.3432 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17208...  Training loss: 3.7937...  0.3446 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17209...  Training loss: 3.7431...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17210...  Training loss: 3.7308...  0.3431 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17211...  Training loss: 3.7124...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17212...  Training loss: 3.7482...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17213...  Training loss: 3.7551...  0.3431 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17214...  Training loss: 3.7384...  0.3392 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17215...  Training loss: 3.7339...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17216...  Training loss: 3.6312...  0.3418 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17217...  Training loss: 3.7319...  0.3429 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17218...  Training loss: 3.7166...  0.3444 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17219...  Training loss: 3.6810...  0.3442 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17220...  Training loss: 3.7365...  0.3427 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94/100...  Training Step: 17221...  Training loss: 3.7215...  0.3431 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17222...  Training loss: 3.7196...  0.3399 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17223...  Training loss: 3.7958...  0.3430 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17224...  Training loss: 3.7121...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17225...  Training loss: 3.7096...  0.3387 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17226...  Training loss: 3.7031...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17227...  Training loss: 3.8021...  0.3385 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17228...  Training loss: 3.7195...  0.3422 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17229...  Training loss: 3.7386...  0.3388 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17230...  Training loss: 3.6838...  0.3442 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17231...  Training loss: 3.7950...  0.3390 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17232...  Training loss: 3.7292...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17233...  Training loss: 3.8605...  0.3440 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17234...  Training loss: 3.8705...  0.3424 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17235...  Training loss: 3.8393...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17236...  Training loss: 3.7045...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17237...  Training loss: 3.7839...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17238...  Training loss: 3.7733...  0.3391 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17239...  Training loss: 3.7741...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17240...  Training loss: 3.7842...  0.3433 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17241...  Training loss: 3.8134...  0.3400 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17242...  Training loss: 3.8169...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17243...  Training loss: 3.8395...  0.3396 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17244...  Training loss: 3.7710...  0.3403 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17245...  Training loss: 3.8280...  0.3405 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17246...  Training loss: 3.7754...  0.3406 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17247...  Training loss: 3.8042...  0.3437 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17248...  Training loss: 3.7473...  0.3435 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17249...  Training loss: 3.7317...  0.3426 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17250...  Training loss: 3.7414...  0.3434 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17251...  Training loss: 3.8280...  0.3393 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17252...  Training loss: 3.6820...  0.3413 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17253...  Training loss: 3.8188...  0.3406 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17254...  Training loss: 3.8272...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17255...  Training loss: 3.7395...  0.3415 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17256...  Training loss: 3.7572...  0.3411 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17257...  Training loss: 3.7796...  0.3428 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17258...  Training loss: 3.7741...  0.3424 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17259...  Training loss: 3.7848...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17260...  Training loss: 3.7962...  0.3424 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17261...  Training loss: 3.7845...  0.3422 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17262...  Training loss: 3.7148...  0.3414 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17263...  Training loss: 3.7464...  0.3404 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17264...  Training loss: 3.7811...  0.3392 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17265...  Training loss: 3.7893...  0.3418 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17266...  Training loss: 3.8281...  0.3436 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17267...  Training loss: 3.8279...  0.3398 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17268...  Training loss: 3.8278...  0.3395 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17269...  Training loss: 3.7999...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17270...  Training loss: 3.8555...  0.3415 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17271...  Training loss: 3.6782...  0.3434 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17272...  Training loss: 3.7076...  0.3438 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17273...  Training loss: 3.7301...  0.3438 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17274...  Training loss: 3.7411...  0.3428 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17275...  Training loss: 3.6851...  0.3390 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17276...  Training loss: 3.6712...  0.3417 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17277...  Training loss: 3.7725...  0.3407 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17278...  Training loss: 3.7451...  0.3400 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17279...  Training loss: 3.7270...  0.3425 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17280...  Training loss: 3.7211...  0.3444 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17281...  Training loss: 3.8250...  0.3444 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17282...  Training loss: 3.8613...  0.3426 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17283...  Training loss: 3.7710...  0.3410 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17284...  Training loss: 3.7090...  0.3390 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17285...  Training loss: 3.7129...  0.3423 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17286...  Training loss: 3.6896...  0.3402 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17287...  Training loss: 3.6520...  0.3400 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17288...  Training loss: 3.7630...  0.3421 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17289...  Training loss: 3.7561...  0.3393 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17290...  Training loss: 3.7619...  0.3400 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17291...  Training loss: 3.7558...  0.3415 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17292...  Training loss: 3.7368...  0.3440 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17293...  Training loss: 3.7281...  0.3422 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17294...  Training loss: 3.7583...  0.3441 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17295...  Training loss: 3.7236...  0.3414 sec/batch\n",
      "Epoch: 94/100...  Training Step: 17296...  Training loss: 3.7500...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17297...  Training loss: 3.6527...  0.3419 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17298...  Training loss: 3.4667...  0.3412 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17299...  Training loss: 3.4942...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17300...  Training loss: 3.5692...  0.3406 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17301...  Training loss: 3.5267...  0.3440 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17302...  Training loss: 3.4908...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17303...  Training loss: 3.6805...  0.3402 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17304...  Training loss: 3.7009...  0.3421 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17305...  Training loss: 3.6648...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17306...  Training loss: 3.6891...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17307...  Training loss: 3.6787...  0.3389 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17308...  Training loss: 3.6651...  0.3394 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17309...  Training loss: 3.6709...  0.3441 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17310...  Training loss: 3.6260...  0.3412 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17311...  Training loss: 3.6889...  0.3431 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17312...  Training loss: 3.6438...  0.3439 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17313...  Training loss: 3.7203...  0.3428 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17314...  Training loss: 3.7109...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17315...  Training loss: 3.7198...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17316...  Training loss: 3.7451...  0.3420 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100...  Training Step: 17317...  Training loss: 3.6843...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17318...  Training loss: 3.5463...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17319...  Training loss: 3.6122...  0.3428 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17320...  Training loss: 3.6297...  0.3387 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17321...  Training loss: 3.7227...  0.3411 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17322...  Training loss: 3.7100...  0.3409 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17323...  Training loss: 3.7475...  0.3421 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17324...  Training loss: 3.6888...  0.3416 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17325...  Training loss: 3.7667...  0.3403 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17326...  Training loss: 3.8079...  0.3410 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17327...  Training loss: 3.7805...  0.3427 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17328...  Training loss: 3.7291...  0.3419 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17329...  Training loss: 3.6282...  0.3387 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17330...  Training loss: 3.6589...  0.3416 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17331...  Training loss: 3.7338...  0.3434 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17332...  Training loss: 3.7731...  0.3421 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17333...  Training loss: 3.7984...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17334...  Training loss: 3.7383...  0.3429 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17335...  Training loss: 3.7611...  0.3418 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17336...  Training loss: 3.7991...  0.3386 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17337...  Training loss: 3.7884...  0.3436 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17338...  Training loss: 3.8264...  0.3410 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17339...  Training loss: 3.8948...  0.3382 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17340...  Training loss: 3.8083...  0.3409 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17341...  Training loss: 3.7793...  0.3431 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17342...  Training loss: 3.7701...  0.3426 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17343...  Training loss: 3.8301...  0.3406 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17344...  Training loss: 3.7387...  0.3408 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17345...  Training loss: 3.7539...  0.3429 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17346...  Training loss: 3.7948...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17347...  Training loss: 3.7985...  0.3426 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17348...  Training loss: 3.8725...  0.3402 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17349...  Training loss: 3.8673...  0.3395 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17350...  Training loss: 3.8586...  0.3398 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17351...  Training loss: 3.8712...  0.3422 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17352...  Training loss: 3.8686...  0.3440 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17353...  Training loss: 3.8093...  0.3440 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17354...  Training loss: 3.8029...  0.3403 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17355...  Training loss: 3.7042...  0.3428 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17356...  Training loss: 3.6910...  0.3437 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17357...  Training loss: 3.7569...  0.3438 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17358...  Training loss: 3.7716...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17359...  Training loss: 3.7476...  0.3394 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17360...  Training loss: 3.8592...  0.3409 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17361...  Training loss: 3.7738...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17362...  Training loss: 3.7629...  0.3392 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17363...  Training loss: 3.7687...  0.3396 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17364...  Training loss: 3.7307...  0.3422 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17365...  Training loss: 3.7770...  0.3431 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17366...  Training loss: 3.7962...  0.3410 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17367...  Training loss: 3.7261...  0.3387 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17368...  Training loss: 3.6874...  0.3445 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17369...  Training loss: 3.7019...  0.3398 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17370...  Training loss: 3.7035...  0.3413 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17371...  Training loss: 3.7277...  0.3431 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17372...  Training loss: 3.7774...  0.3411 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17373...  Training loss: 3.7387...  0.3423 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17374...  Training loss: 3.6761...  0.3426 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17375...  Training loss: 3.7528...  0.3408 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17376...  Training loss: 3.6761...  0.3402 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17377...  Training loss: 3.7613...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17378...  Training loss: 3.7080...  0.3403 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17379...  Training loss: 3.7275...  0.3423 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17380...  Training loss: 3.7197...  0.3401 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17381...  Training loss: 3.7393...  0.3442 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17382...  Training loss: 3.7858...  0.3405 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17383...  Training loss: 3.7252...  0.3429 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17384...  Training loss: 3.6885...  0.3440 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17385...  Training loss: 3.7810...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17386...  Training loss: 3.8345...  0.3396 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17387...  Training loss: 3.7459...  0.3392 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17388...  Training loss: 3.7273...  0.3417 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17389...  Training loss: 3.7462...  0.3432 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17390...  Training loss: 3.7155...  0.3401 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17391...  Training loss: 3.7789...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17392...  Training loss: 3.7862...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17393...  Training loss: 3.7210...  0.3397 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17394...  Training loss: 3.7043...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17395...  Training loss: 3.7000...  0.3406 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17396...  Training loss: 3.7418...  0.3425 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17397...  Training loss: 3.7496...  0.3442 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17398...  Training loss: 3.7251...  0.3432 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17399...  Training loss: 3.7165...  0.3419 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17400...  Training loss: 3.6144...  0.3436 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17401...  Training loss: 3.7387...  0.3394 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17402...  Training loss: 3.6987...  0.3418 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17403...  Training loss: 3.7024...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17404...  Training loss: 3.6970...  0.3421 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17405...  Training loss: 3.7216...  0.3441 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17406...  Training loss: 3.7096...  0.3443 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17407...  Training loss: 3.7587...  0.3432 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17408...  Training loss: 3.6536...  0.3424 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17409...  Training loss: 3.6636...  0.3405 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17410...  Training loss: 3.6574...  0.3426 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17411...  Training loss: 3.7633...  0.3418 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17412...  Training loss: 3.7167...  0.3410 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/100...  Training Step: 17413...  Training loss: 3.7252...  0.3439 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17414...  Training loss: 3.6921...  0.3404 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17415...  Training loss: 3.7683...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17416...  Training loss: 3.7311...  0.3391 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17417...  Training loss: 3.8784...  0.3410 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17418...  Training loss: 3.8445...  0.3409 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17419...  Training loss: 3.8288...  0.3399 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17420...  Training loss: 3.7001...  0.3413 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17421...  Training loss: 3.7815...  0.3388 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17422...  Training loss: 3.7981...  0.3425 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17423...  Training loss: 3.7440...  0.3402 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17424...  Training loss: 3.7849...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17425...  Training loss: 3.7685...  0.3395 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17426...  Training loss: 3.8371...  0.3417 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17427...  Training loss: 3.8555...  0.3398 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17428...  Training loss: 3.7565...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17429...  Training loss: 3.8078...  0.3423 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17430...  Training loss: 3.7895...  0.3422 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17431...  Training loss: 3.8050...  0.3388 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17432...  Training loss: 3.7508...  0.3434 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17433...  Training loss: 3.7132...  0.3397 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17434...  Training loss: 3.7447...  0.3397 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17435...  Training loss: 3.7952...  0.3410 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17436...  Training loss: 3.6788...  0.3434 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17437...  Training loss: 3.7729...  0.3383 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17438...  Training loss: 3.7833...  0.3432 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17439...  Training loss: 3.7607...  0.3406 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17440...  Training loss: 3.7043...  0.3416 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17441...  Training loss: 3.7885...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17442...  Training loss: 3.7503...  0.3426 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17443...  Training loss: 3.7412...  0.3429 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17444...  Training loss: 3.7565...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17445...  Training loss: 3.7734...  0.3395 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17446...  Training loss: 3.6823...  0.3436 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17447...  Training loss: 3.7283...  0.3417 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17448...  Training loss: 3.7501...  0.3434 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17449...  Training loss: 3.7296...  0.3393 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17450...  Training loss: 3.7981...  0.3422 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17451...  Training loss: 3.8136...  0.3415 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17452...  Training loss: 3.8412...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17453...  Training loss: 3.7863...  0.3422 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17454...  Training loss: 3.8287...  0.3423 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17455...  Training loss: 3.6872...  0.3420 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17456...  Training loss: 3.7353...  0.3417 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17457...  Training loss: 3.7299...  0.3435 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17458...  Training loss: 3.7468...  0.3433 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17459...  Training loss: 3.6962...  0.3391 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17460...  Training loss: 3.6791...  0.3402 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17461...  Training loss: 3.7711...  0.3415 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17462...  Training loss: 3.6957...  0.3406 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17463...  Training loss: 3.6803...  0.3401 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17464...  Training loss: 3.7081...  0.3397 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17465...  Training loss: 3.8041...  0.3423 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17466...  Training loss: 3.8421...  0.3418 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17467...  Training loss: 3.8050...  0.3432 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17468...  Training loss: 3.7330...  0.3443 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17469...  Training loss: 3.6397...  0.3407 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17470...  Training loss: 3.6483...  0.3393 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17471...  Training loss: 3.5975...  0.3431 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17472...  Training loss: 3.7148...  0.3391 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17473...  Training loss: 3.7684...  0.3430 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17474...  Training loss: 3.7258...  0.3395 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17475...  Training loss: 3.7468...  0.3399 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17476...  Training loss: 3.7276...  0.3437 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17477...  Training loss: 3.7102...  0.3392 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17478...  Training loss: 3.7316...  0.3400 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17479...  Training loss: 3.6784...  0.3407 sec/batch\n",
      "Epoch: 95/100...  Training Step: 17480...  Training loss: 3.7707...  0.3429 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17481...  Training loss: 3.6599...  0.3420 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17482...  Training loss: 3.4354...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17483...  Training loss: 3.4716...  0.3412 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17484...  Training loss: 3.5566...  0.3379 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17485...  Training loss: 3.5233...  0.3379 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17486...  Training loss: 3.4909...  0.3383 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17487...  Training loss: 3.6831...  0.3390 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17488...  Training loss: 3.7250...  0.3387 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17489...  Training loss: 3.6348...  0.3391 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17490...  Training loss: 3.6600...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17491...  Training loss: 3.6687...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17492...  Training loss: 3.6408...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17493...  Training loss: 3.6290...  0.3377 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17494...  Training loss: 3.6431...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17495...  Training loss: 3.6668...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17496...  Training loss: 3.6747...  0.3404 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17497...  Training loss: 3.6949...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17498...  Training loss: 3.7017...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17499...  Training loss: 3.7270...  0.3406 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17500...  Training loss: 3.7298...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17501...  Training loss: 3.6364...  0.3367 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17502...  Training loss: 3.5804...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17503...  Training loss: 3.6044...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17504...  Training loss: 3.6074...  0.3396 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17505...  Training loss: 3.6906...  0.3376 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17506...  Training loss: 3.6887...  0.3412 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17507...  Training loss: 3.7107...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17508...  Training loss: 3.6557...  0.3412 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/100...  Training Step: 17509...  Training loss: 3.7229...  0.3418 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17510...  Training loss: 3.8073...  0.3428 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17511...  Training loss: 3.7782...  0.3407 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17512...  Training loss: 3.7112...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17513...  Training loss: 3.6237...  0.3405 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17514...  Training loss: 3.6425...  0.3404 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17515...  Training loss: 3.7060...  0.3413 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17516...  Training loss: 3.7635...  0.3381 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17517...  Training loss: 3.8009...  0.3421 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17518...  Training loss: 3.7329...  0.3389 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17519...  Training loss: 3.7431...  0.3374 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17520...  Training loss: 3.7958...  0.3405 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17521...  Training loss: 3.7655...  0.3401 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17522...  Training loss: 3.8011...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17523...  Training loss: 3.8681...  0.3391 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17524...  Training loss: 3.7791...  0.3401 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17525...  Training loss: 3.7825...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17526...  Training loss: 3.7393...  0.3396 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17527...  Training loss: 3.7756...  0.3404 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17528...  Training loss: 3.7128...  0.3386 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17529...  Training loss: 3.7750...  0.3373 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17530...  Training loss: 3.7682...  0.3401 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17531...  Training loss: 3.7697...  0.3419 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17532...  Training loss: 3.8498...  0.3379 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17533...  Training loss: 3.8237...  0.3394 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17534...  Training loss: 3.8375...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17535...  Training loss: 3.8035...  0.3386 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17536...  Training loss: 3.8662...  0.3378 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17537...  Training loss: 3.7624...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17538...  Training loss: 3.8227...  0.3410 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17539...  Training loss: 3.7252...  0.3415 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17540...  Training loss: 3.7041...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17541...  Training loss: 3.7691...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17542...  Training loss: 3.7561...  0.3379 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17543...  Training loss: 3.7670...  0.3391 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17544...  Training loss: 3.8396...  0.3378 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17545...  Training loss: 3.7668...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17546...  Training loss: 3.7485...  0.3379 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17547...  Training loss: 3.7486...  0.3383 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17548...  Training loss: 3.7254...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17549...  Training loss: 3.7632...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17550...  Training loss: 3.7829...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17551...  Training loss: 3.7504...  0.3413 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17552...  Training loss: 3.6846...  0.3389 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17553...  Training loss: 3.6981...  0.3383 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17554...  Training loss: 3.7072...  0.3415 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17555...  Training loss: 3.6927...  0.3406 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17556...  Training loss: 3.7333...  0.3408 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17557...  Training loss: 3.7250...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17558...  Training loss: 3.6305...  0.3429 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17559...  Training loss: 3.7274...  0.3416 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17560...  Training loss: 3.6902...  0.3422 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17561...  Training loss: 3.7226...  0.3424 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17562...  Training loss: 3.7437...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17563...  Training loss: 3.7366...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17564...  Training loss: 3.7081...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17565...  Training loss: 3.6929...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17566...  Training loss: 3.7666...  0.3380 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17567...  Training loss: 3.7112...  0.3386 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17568...  Training loss: 3.6617...  0.3369 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17569...  Training loss: 3.7484...  0.3427 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17570...  Training loss: 3.8294...  0.3406 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17571...  Training loss: 3.7289...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17572...  Training loss: 3.7049...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17573...  Training loss: 3.7413...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17574...  Training loss: 3.7229...  0.3418 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17575...  Training loss: 3.7674...  0.3381 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17576...  Training loss: 3.7872...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17577...  Training loss: 3.7388...  0.3428 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17578...  Training loss: 3.6837...  0.3406 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17579...  Training loss: 3.6950...  0.3401 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17580...  Training loss: 3.7240...  0.3384 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17581...  Training loss: 3.7484...  0.3378 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17582...  Training loss: 3.6768...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17583...  Training loss: 3.6967...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17584...  Training loss: 3.6000...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17585...  Training loss: 3.7161...  0.3396 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17586...  Training loss: 3.6909...  0.3384 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17587...  Training loss: 3.6505...  0.3382 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17588...  Training loss: 3.7105...  0.3377 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17589...  Training loss: 3.7048...  0.3400 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17590...  Training loss: 3.7082...  0.3417 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17591...  Training loss: 3.7525...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17592...  Training loss: 3.6796...  0.3386 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17593...  Training loss: 3.6656...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17594...  Training loss: 3.6059...  0.3405 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17595...  Training loss: 3.7509...  0.3391 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17596...  Training loss: 3.7010...  0.3401 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17597...  Training loss: 3.6986...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17598...  Training loss: 3.6624...  0.3406 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17599...  Training loss: 3.7294...  0.3390 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17600...  Training loss: 3.7147...  0.3382 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17601...  Training loss: 3.8544...  0.3386 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17602...  Training loss: 3.8337...  0.3382 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17603...  Training loss: 3.8392...  0.3389 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17604...  Training loss: 3.7207...  0.3390 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/100...  Training Step: 17605...  Training loss: 3.7696...  0.3391 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17606...  Training loss: 3.7745...  0.3404 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17607...  Training loss: 3.7588...  0.3382 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17608...  Training loss: 3.7598...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17609...  Training loss: 3.8134...  0.3417 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17610...  Training loss: 3.8349...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17611...  Training loss: 3.8383...  0.3400 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17612...  Training loss: 3.7533...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17613...  Training loss: 3.8149...  0.3387 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17614...  Training loss: 3.7814...  0.3370 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17615...  Training loss: 3.8110...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17616...  Training loss: 3.7504...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17617...  Training loss: 3.7520...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17618...  Training loss: 3.7329...  0.3389 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17619...  Training loss: 3.7655...  0.3419 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17620...  Training loss: 3.6326...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17621...  Training loss: 3.7815...  0.3405 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17622...  Training loss: 3.7594...  0.3414 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17623...  Training loss: 3.7272...  0.3390 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17624...  Training loss: 3.7332...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17625...  Training loss: 3.7742...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17626...  Training loss: 3.7549...  0.3371 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17627...  Training loss: 3.7247...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17628...  Training loss: 3.7583...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17629...  Training loss: 3.7707...  0.3422 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17630...  Training loss: 3.6862...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17631...  Training loss: 3.7216...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17632...  Training loss: 3.7492...  0.3381 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17633...  Training loss: 3.7276...  0.3404 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17634...  Training loss: 3.7723...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17635...  Training loss: 3.7500...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17636...  Training loss: 3.8408...  0.3400 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17637...  Training loss: 3.8117...  0.3427 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17638...  Training loss: 3.8497...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17639...  Training loss: 3.6669...  0.3399 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17640...  Training loss: 3.7375...  0.3398 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17641...  Training loss: 3.7258...  0.3405 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17642...  Training loss: 3.7301...  0.3430 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17643...  Training loss: 3.6979...  0.3407 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17644...  Training loss: 3.6381...  0.3392 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17645...  Training loss: 3.7283...  0.3410 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17646...  Training loss: 3.6691...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17647...  Training loss: 3.6706...  0.3375 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17648...  Training loss: 3.6915...  0.3378 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17649...  Training loss: 3.8267...  0.3396 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17650...  Training loss: 3.8178...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17651...  Training loss: 3.7487...  0.3385 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17652...  Training loss: 3.7013...  0.3416 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17653...  Training loss: 3.6780...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17654...  Training loss: 3.6451...  0.3402 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17655...  Training loss: 3.6079...  0.3403 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17656...  Training loss: 3.7483...  0.3418 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17657...  Training loss: 3.7573...  0.3381 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17658...  Training loss: 3.7141...  0.3396 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17659...  Training loss: 3.7372...  0.3388 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17660...  Training loss: 3.7228...  0.3395 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17661...  Training loss: 3.6777...  0.3424 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17662...  Training loss: 3.6962...  0.3397 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17663...  Training loss: 3.6758...  0.3418 sec/batch\n",
      "Epoch: 96/100...  Training Step: 17664...  Training loss: 3.7546...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17665...  Training loss: 3.6513...  0.3430 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17666...  Training loss: 3.4562...  0.3371 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17667...  Training loss: 3.4593...  0.3388 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17668...  Training loss: 3.5444...  0.3447 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17669...  Training loss: 3.5047...  0.3418 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17670...  Training loss: 3.4837...  0.3419 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17671...  Training loss: 3.6526...  0.3386 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17672...  Training loss: 3.6715...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17673...  Training loss: 3.6510...  0.3395 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17674...  Training loss: 3.6547...  0.3423 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17675...  Training loss: 3.6354...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17676...  Training loss: 3.6256...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17677...  Training loss: 3.6884...  0.3386 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17678...  Training loss: 3.6297...  0.3403 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17679...  Training loss: 3.6656...  0.3392 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17680...  Training loss: 3.6646...  0.3390 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17681...  Training loss: 3.6947...  0.3419 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17682...  Training loss: 3.6405...  0.3423 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17683...  Training loss: 3.7103...  0.3404 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17684...  Training loss: 3.7339...  0.3422 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17685...  Training loss: 3.6658...  0.3399 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17686...  Training loss: 3.5549...  0.3382 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17687...  Training loss: 3.5711...  0.3417 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17688...  Training loss: 3.6114...  0.3391 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17689...  Training loss: 3.6848...  0.3397 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17690...  Training loss: 3.6773...  0.3382 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17691...  Training loss: 3.6950...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17692...  Training loss: 3.6625...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17693...  Training loss: 3.7187...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17694...  Training loss: 3.7916...  0.3390 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17695...  Training loss: 3.7507...  0.3387 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17696...  Training loss: 3.7022...  0.3376 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17697...  Training loss: 3.6151...  0.3390 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17698...  Training loss: 3.6280...  0.3427 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17699...  Training loss: 3.6985...  0.3392 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17700...  Training loss: 3.7566...  0.3392 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/100...  Training Step: 17701...  Training loss: 3.7498...  0.3383 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17702...  Training loss: 3.6879...  0.3380 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17703...  Training loss: 3.7581...  0.3381 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17704...  Training loss: 3.8156...  0.3400 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17705...  Training loss: 3.7653...  0.3388 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17706...  Training loss: 3.8191...  0.3386 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17707...  Training loss: 3.8914...  0.3420 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17708...  Training loss: 3.7755...  0.3401 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17709...  Training loss: 3.7402...  0.3422 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17710...  Training loss: 3.7234...  0.3379 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17711...  Training loss: 3.8163...  0.3377 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17712...  Training loss: 3.7032...  0.3391 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17713...  Training loss: 3.7291...  0.3385 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17714...  Training loss: 3.7711...  0.3391 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17715...  Training loss: 3.7982...  0.3404 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17716...  Training loss: 3.8543...  0.3412 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17717...  Training loss: 3.8552...  0.3391 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17718...  Training loss: 3.8172...  0.3404 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17719...  Training loss: 3.8237...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17720...  Training loss: 3.8377...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17721...  Training loss: 3.7945...  0.3390 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17722...  Training loss: 3.8033...  0.3403 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17723...  Training loss: 3.6961...  0.3400 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17724...  Training loss: 3.7029...  0.3395 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17725...  Training loss: 3.7288...  0.3391 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17726...  Training loss: 3.7538...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17727...  Training loss: 3.7581...  0.3385 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17728...  Training loss: 3.8083...  0.3383 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17729...  Training loss: 3.7183...  0.3406 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17730...  Training loss: 3.7407...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17731...  Training loss: 3.7308...  0.3413 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17732...  Training loss: 3.7212...  0.3392 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17733...  Training loss: 3.7633...  0.3397 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17734...  Training loss: 3.7682...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17735...  Training loss: 3.7489...  0.3403 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17736...  Training loss: 3.7040...  0.3366 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17737...  Training loss: 3.6965...  0.3390 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17738...  Training loss: 3.6822...  0.3394 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17739...  Training loss: 3.6914...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17740...  Training loss: 3.6895...  0.3384 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17741...  Training loss: 3.7167...  0.3395 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17742...  Training loss: 3.6557...  0.3397 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17743...  Training loss: 3.7322...  0.3408 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17744...  Training loss: 3.6792...  0.3410 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17745...  Training loss: 3.7502...  0.3389 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17746...  Training loss: 3.7206...  0.3395 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17747...  Training loss: 3.7572...  0.3416 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17748...  Training loss: 3.6771...  0.3411 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17749...  Training loss: 3.7066...  0.3425 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17750...  Training loss: 3.7831...  0.3417 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17751...  Training loss: 3.6783...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17752...  Training loss: 3.6093...  0.3388 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17753...  Training loss: 3.7088...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17754...  Training loss: 3.8044...  0.3392 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17755...  Training loss: 3.7226...  0.3401 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17756...  Training loss: 3.6695...  0.3426 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17757...  Training loss: 3.6993...  0.3416 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17758...  Training loss: 3.6882...  0.3416 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17759...  Training loss: 3.7888...  0.3401 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17760...  Training loss: 3.7644...  0.3402 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17761...  Training loss: 3.7041...  0.3394 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17762...  Training loss: 3.6788...  0.3389 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17763...  Training loss: 3.6944...  0.3411 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17764...  Training loss: 3.7005...  0.3422 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17765...  Training loss: 3.7611...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17766...  Training loss: 3.6904...  0.3415 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17767...  Training loss: 3.6918...  0.3425 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17768...  Training loss: 3.5853...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17769...  Training loss: 3.7119...  0.3435 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17770...  Training loss: 3.7040...  0.3386 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17771...  Training loss: 3.6714...  0.3372 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17772...  Training loss: 3.7275...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17773...  Training loss: 3.6830...  0.3425 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17774...  Training loss: 3.6966...  0.3389 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17775...  Training loss: 3.7558...  0.3420 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17776...  Training loss: 3.6728...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17777...  Training loss: 3.6762...  0.3389 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17778...  Training loss: 3.6324...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17779...  Training loss: 3.7493...  0.3374 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17780...  Training loss: 3.6680...  0.3410 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17781...  Training loss: 3.6968...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17782...  Training loss: 3.6513...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17783...  Training loss: 3.7090...  0.3382 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17784...  Training loss: 3.7123...  0.3396 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17785...  Training loss: 3.8333...  0.3401 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17786...  Training loss: 3.8132...  0.3396 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17787...  Training loss: 3.8376...  0.3403 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17788...  Training loss: 3.7010...  0.3393 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17789...  Training loss: 3.7636...  0.3388 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17790...  Training loss: 3.7574...  0.3416 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17791...  Training loss: 3.7606...  0.3408 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17792...  Training loss: 3.7638...  0.3415 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17793...  Training loss: 3.7716...  0.3422 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17794...  Training loss: 3.8086...  0.3408 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17795...  Training loss: 3.8260...  0.3449 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17796...  Training loss: 3.7566...  0.3423 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97/100...  Training Step: 17797...  Training loss: 3.8237...  0.3421 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17798...  Training loss: 3.7802...  0.3449 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17799...  Training loss: 3.7926...  0.3411 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17800...  Training loss: 3.7686...  0.3399 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17801...  Training loss: 3.7103...  0.3394 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17802...  Training loss: 3.7698...  0.3444 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17803...  Training loss: 3.8014...  0.3431 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17804...  Training loss: 3.6373...  0.3405 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17805...  Training loss: 3.7668...  0.3443 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17806...  Training loss: 3.7433...  0.3438 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17807...  Training loss: 3.6997...  0.3402 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17808...  Training loss: 3.7110...  0.3432 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17809...  Training loss: 3.7551...  0.3440 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17810...  Training loss: 3.7381...  0.3447 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17811...  Training loss: 3.7074...  0.3400 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17812...  Training loss: 3.7508...  0.3414 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17813...  Training loss: 3.7668...  0.3440 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17814...  Training loss: 3.6770...  0.3430 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17815...  Training loss: 3.7392...  0.3433 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17816...  Training loss: 3.7159...  0.3470 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17817...  Training loss: 3.7008...  0.3445 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17818...  Training loss: 3.7505...  0.3421 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17819...  Training loss: 3.7706...  0.3426 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17820...  Training loss: 3.7788...  0.3397 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17821...  Training loss: 3.7335...  0.3438 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17822...  Training loss: 3.8385...  0.3392 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17823...  Training loss: 3.6983...  0.3448 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17824...  Training loss: 3.7211...  0.3417 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17825...  Training loss: 3.7452...  0.3412 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17826...  Training loss: 3.7403...  0.3398 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17827...  Training loss: 3.6759...  0.3406 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17828...  Training loss: 3.6382...  0.3436 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17829...  Training loss: 3.6971...  0.3410 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17830...  Training loss: 3.6702...  0.3438 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17831...  Training loss: 3.6726...  0.3408 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17832...  Training loss: 3.6820...  0.3389 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17833...  Training loss: 3.7850...  0.3413 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17834...  Training loss: 3.7844...  0.3414 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17835...  Training loss: 3.7429...  0.3444 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17836...  Training loss: 3.7133...  0.3422 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17837...  Training loss: 3.6558...  0.3439 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17838...  Training loss: 3.6492...  0.3437 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17839...  Training loss: 3.5893...  0.3430 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17840...  Training loss: 3.7091...  0.3444 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17841...  Training loss: 3.7491...  0.3400 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17842...  Training loss: 3.7079...  0.3421 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17843...  Training loss: 3.7018...  0.3424 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17844...  Training loss: 3.6798...  0.3404 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17845...  Training loss: 3.6442...  0.3407 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17846...  Training loss: 3.6732...  0.3438 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17847...  Training loss: 3.6318...  0.3438 sec/batch\n",
      "Epoch: 97/100...  Training Step: 17848...  Training loss: 3.7242...  0.3432 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17849...  Training loss: 3.6415...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17850...  Training loss: 3.4534...  0.3434 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17851...  Training loss: 3.4099...  0.3396 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17852...  Training loss: 3.5088...  0.3387 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17853...  Training loss: 3.5042...  0.3437 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17854...  Training loss: 3.4530...  0.3400 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17855...  Training loss: 3.6276...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17856...  Training loss: 3.6656...  0.3399 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17857...  Training loss: 3.6176...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17858...  Training loss: 3.6331...  0.3394 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17859...  Training loss: 3.6328...  0.3453 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17860...  Training loss: 3.6093...  0.3425 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17861...  Training loss: 3.6261...  0.3445 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17862...  Training loss: 3.5984...  0.3387 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17863...  Training loss: 3.6362...  0.3432 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17864...  Training loss: 3.6297...  0.3426 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17865...  Training loss: 3.6826...  0.3410 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17866...  Training loss: 3.6502...  0.3421 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17867...  Training loss: 3.6919...  0.3381 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17868...  Training loss: 3.7229...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17869...  Training loss: 3.6506...  0.3422 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17870...  Training loss: 3.5339...  0.3435 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17871...  Training loss: 3.5556...  0.3409 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17872...  Training loss: 3.5686...  0.3446 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17873...  Training loss: 3.6840...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17874...  Training loss: 3.6756...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17875...  Training loss: 3.7004...  0.3425 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17876...  Training loss: 3.6349...  0.3426 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17877...  Training loss: 3.6926...  0.3415 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17878...  Training loss: 3.7850...  0.3442 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17879...  Training loss: 3.7076...  0.3431 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17880...  Training loss: 3.6674...  0.3430 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17881...  Training loss: 3.6418...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17882...  Training loss: 3.6037...  0.3388 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17883...  Training loss: 3.7268...  0.3427 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17884...  Training loss: 3.7308...  0.3404 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17885...  Training loss: 3.7578...  0.3402 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17886...  Training loss: 3.6904...  0.3400 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17887...  Training loss: 3.7586...  0.3390 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17888...  Training loss: 3.7706...  0.3398 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17889...  Training loss: 3.7631...  0.3414 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17890...  Training loss: 3.7921...  0.3427 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17891...  Training loss: 3.8632...  0.3376 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17892...  Training loss: 3.7603...  0.3405 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100...  Training Step: 17893...  Training loss: 3.7567...  0.3396 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17894...  Training loss: 3.7308...  0.3407 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17895...  Training loss: 3.7531...  0.3421 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17896...  Training loss: 3.7251...  0.3443 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17897...  Training loss: 3.7271...  0.3410 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17898...  Training loss: 3.7601...  0.3426 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17899...  Training loss: 3.7989...  0.3412 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17900...  Training loss: 3.8447...  0.3400 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17901...  Training loss: 3.8359...  0.3409 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17902...  Training loss: 3.8195...  0.3395 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17903...  Training loss: 3.8333...  0.3434 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17904...  Training loss: 3.8489...  0.3405 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17905...  Training loss: 3.7813...  0.3405 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17906...  Training loss: 3.8192...  0.3445 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17907...  Training loss: 3.6988...  0.3440 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17908...  Training loss: 3.6888...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17909...  Training loss: 3.7414...  0.3418 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17910...  Training loss: 3.7654...  0.3420 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17911...  Training loss: 3.7330...  0.3418 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17912...  Training loss: 3.7649...  0.3401 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17913...  Training loss: 3.7359...  0.3451 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17914...  Training loss: 3.7144...  0.3419 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17915...  Training loss: 3.7181...  0.3404 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17916...  Training loss: 3.6800...  0.3415 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17917...  Training loss: 3.7574...  0.3406 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17918...  Training loss: 3.7742...  0.3431 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17919...  Training loss: 3.7165...  0.3409 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17920...  Training loss: 3.6596...  0.3438 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17921...  Training loss: 3.7165...  0.3432 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17922...  Training loss: 3.6719...  0.3443 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17923...  Training loss: 3.6882...  0.3394 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17924...  Training loss: 3.6876...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17925...  Training loss: 3.6952...  0.3447 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17926...  Training loss: 3.6201...  0.3427 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17927...  Training loss: 3.7238...  0.3439 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17928...  Training loss: 3.6557...  0.3381 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17929...  Training loss: 3.6991...  0.3391 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17930...  Training loss: 3.7034...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17931...  Training loss: 3.7361...  0.3424 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17932...  Training loss: 3.6995...  0.3421 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17933...  Training loss: 3.7257...  0.3438 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17934...  Training loss: 3.7894...  0.3424 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17935...  Training loss: 3.6859...  0.3427 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17936...  Training loss: 3.6356...  0.3403 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17937...  Training loss: 3.6970...  0.3403 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17938...  Training loss: 3.8203...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17939...  Training loss: 3.6906...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17940...  Training loss: 3.6597...  0.3391 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17941...  Training loss: 3.6859...  0.3422 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17942...  Training loss: 3.6766...  0.3430 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17943...  Training loss: 3.7659...  0.3417 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17944...  Training loss: 3.7710...  0.3437 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17945...  Training loss: 3.6951...  0.3429 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17946...  Training loss: 3.7047...  0.3438 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17947...  Training loss: 3.6983...  0.3403 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17948...  Training loss: 3.7458...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17949...  Training loss: 3.7339...  0.3395 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17950...  Training loss: 3.6588...  0.3392 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17951...  Training loss: 3.6647...  0.3433 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17952...  Training loss: 3.5816...  0.3408 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17953...  Training loss: 3.7074...  0.3439 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17954...  Training loss: 3.6904...  0.3413 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17955...  Training loss: 3.6567...  0.3396 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17956...  Training loss: 3.6721...  0.3446 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17957...  Training loss: 3.6861...  0.3426 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17958...  Training loss: 3.6825...  0.3433 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17959...  Training loss: 3.7321...  0.3438 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17960...  Training loss: 3.6619...  0.3446 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17961...  Training loss: 3.6707...  0.3408 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17962...  Training loss: 3.6134...  0.3424 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17963...  Training loss: 3.7820...  0.3399 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17964...  Training loss: 3.6539...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17965...  Training loss: 3.6751...  0.3429 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17966...  Training loss: 3.6526...  0.3410 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17967...  Training loss: 3.6916...  0.3405 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17968...  Training loss: 3.6945...  0.3445 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17969...  Training loss: 3.8082...  0.3397 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17970...  Training loss: 3.7872...  0.3405 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17971...  Training loss: 3.7960...  0.3389 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17972...  Training loss: 3.6687...  0.3395 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17973...  Training loss: 3.7718...  0.3448 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17974...  Training loss: 3.7770...  0.3459 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17975...  Training loss: 3.7478...  0.3443 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17976...  Training loss: 3.7281...  0.3426 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17977...  Training loss: 3.7467...  0.3433 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17978...  Training loss: 3.8005...  0.3388 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17979...  Training loss: 3.8058...  0.3419 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17980...  Training loss: 3.7192...  0.3454 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17981...  Training loss: 3.8015...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17982...  Training loss: 3.7853...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17983...  Training loss: 3.8116...  0.3446 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17984...  Training loss: 3.7515...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17985...  Training loss: 3.7006...  0.3404 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17986...  Training loss: 3.7221...  0.3404 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17987...  Training loss: 3.8074...  0.3433 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17988...  Training loss: 3.6465...  0.3394 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/100...  Training Step: 17989...  Training loss: 3.7370...  0.3434 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17990...  Training loss: 3.7761...  0.3428 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17991...  Training loss: 3.7144...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17992...  Training loss: 3.6896...  0.3393 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17993...  Training loss: 3.7509...  0.3424 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17994...  Training loss: 3.7124...  0.3398 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17995...  Training loss: 3.7414...  0.3433 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17996...  Training loss: 3.7591...  0.3403 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17997...  Training loss: 3.7742...  0.3413 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17998...  Training loss: 3.6792...  0.3406 sec/batch\n",
      "Epoch: 98/100...  Training Step: 17999...  Training loss: 3.7239...  0.3407 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18000...  Training loss: 3.7566...  0.3449 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18001...  Training loss: 3.7093...  0.3950 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18002...  Training loss: 3.7819...  0.3458 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18003...  Training loss: 3.7539...  0.3447 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18004...  Training loss: 3.7541...  0.3404 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18005...  Training loss: 3.7347...  0.3437 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18006...  Training loss: 3.7757...  0.3447 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18007...  Training loss: 3.6727...  0.3413 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18008...  Training loss: 3.7480...  0.3432 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18009...  Training loss: 3.7223...  0.3459 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18010...  Training loss: 3.7425...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18011...  Training loss: 3.7183...  0.3437 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18012...  Training loss: 3.6469...  0.3425 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18013...  Training loss: 3.6951...  0.3439 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18014...  Training loss: 3.6578...  0.3439 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18015...  Training loss: 3.6363...  0.3455 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18016...  Training loss: 3.6497...  0.3469 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18017...  Training loss: 3.7884...  0.3450 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18018...  Training loss: 3.7954...  0.3425 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18019...  Training loss: 3.7504...  0.3423 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18020...  Training loss: 3.7056...  0.3452 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18021...  Training loss: 3.6449...  0.3420 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18022...  Training loss: 3.6254...  0.3450 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18023...  Training loss: 3.5903...  0.3454 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18024...  Training loss: 3.7360...  0.3446 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18025...  Training loss: 3.7291...  0.3436 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18026...  Training loss: 3.7353...  0.3445 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18027...  Training loss: 3.7291...  0.3416 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18028...  Training loss: 3.6860...  0.3410 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18029...  Training loss: 3.6400...  0.3412 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18030...  Training loss: 3.6363...  0.3424 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18031...  Training loss: 3.6381...  0.3415 sec/batch\n",
      "Epoch: 98/100...  Training Step: 18032...  Training loss: 3.7260...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18033...  Training loss: 3.6308...  0.3414 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18034...  Training loss: 3.4243...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18035...  Training loss: 3.4468...  0.3417 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18036...  Training loss: 3.5243...  0.3407 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18037...  Training loss: 3.5183...  0.3455 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18038...  Training loss: 3.4388...  0.3408 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18039...  Training loss: 3.6181...  0.3421 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18040...  Training loss: 3.6467...  0.3454 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18041...  Training loss: 3.6059...  0.3408 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18042...  Training loss: 3.6220...  0.3401 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18043...  Training loss: 3.5974...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18044...  Training loss: 3.5946...  0.3412 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18045...  Training loss: 3.6282...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18046...  Training loss: 3.5856...  0.3460 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18047...  Training loss: 3.6085...  0.3444 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18048...  Training loss: 3.6287...  0.3430 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18049...  Training loss: 3.7021...  0.3423 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18050...  Training loss: 3.6459...  0.3418 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18051...  Training loss: 3.7176...  0.3433 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18052...  Training loss: 3.7349...  0.3452 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18053...  Training loss: 3.6624...  0.3435 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18054...  Training loss: 3.5430...  0.3442 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18055...  Training loss: 3.6148...  0.3435 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18056...  Training loss: 3.6144...  0.3447 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18057...  Training loss: 3.6840...  0.3426 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18058...  Training loss: 3.6973...  0.3437 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18059...  Training loss: 3.7247...  0.3430 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18060...  Training loss: 3.6513...  0.3426 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18061...  Training loss: 3.7325...  0.3444 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18062...  Training loss: 3.7988...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18063...  Training loss: 3.7724...  0.3444 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18064...  Training loss: 3.7276...  0.3453 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18065...  Training loss: 3.6040...  0.3414 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18066...  Training loss: 3.6273...  0.3429 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18067...  Training loss: 3.6891...  0.3418 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18068...  Training loss: 3.7346...  0.3448 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18069...  Training loss: 3.7675...  0.3458 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18070...  Training loss: 3.6988...  0.3414 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18071...  Training loss: 3.7140...  0.3398 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18072...  Training loss: 3.7517...  0.3424 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18073...  Training loss: 3.7397...  0.3414 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18074...  Training loss: 3.7924...  0.3417 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18075...  Training loss: 3.8160...  0.3416 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18076...  Training loss: 3.7525...  0.3452 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18077...  Training loss: 3.7400...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18078...  Training loss: 3.7353...  0.3426 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18079...  Training loss: 3.7767...  0.3463 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18080...  Training loss: 3.7006...  0.3474 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18081...  Training loss: 3.7046...  0.3419 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18082...  Training loss: 3.7677...  0.3417 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18083...  Training loss: 3.8003...  0.3400 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18084...  Training loss: 3.8324...  0.3427 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100...  Training Step: 18085...  Training loss: 3.8065...  0.3414 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18086...  Training loss: 3.7839...  0.3407 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18087...  Training loss: 3.7979...  0.3452 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18088...  Training loss: 3.8141...  0.3449 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18089...  Training loss: 3.7589...  0.3431 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18090...  Training loss: 3.7936...  0.3456 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18091...  Training loss: 3.7061...  0.3449 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18092...  Training loss: 3.6718...  0.3412 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18093...  Training loss: 3.7408...  0.3415 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18094...  Training loss: 3.7496...  0.3431 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18095...  Training loss: 3.7367...  0.3459 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18096...  Training loss: 3.8133...  0.3427 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18097...  Training loss: 3.7391...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18098...  Training loss: 3.7159...  0.3454 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18099...  Training loss: 3.7050...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18100...  Training loss: 3.6624...  0.3451 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18101...  Training loss: 3.7079...  0.3436 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18102...  Training loss: 3.7608...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18103...  Training loss: 3.6932...  0.3412 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18104...  Training loss: 3.6913...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18105...  Training loss: 3.6937...  0.3407 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18106...  Training loss: 3.6670...  0.3418 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18107...  Training loss: 3.6663...  0.3411 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18108...  Training loss: 3.6759...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18109...  Training loss: 3.6842...  0.3448 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18110...  Training loss: 3.6367...  0.3427 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18111...  Training loss: 3.7052...  0.3419 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18112...  Training loss: 3.6011...  0.3461 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18113...  Training loss: 3.6973...  0.3440 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18114...  Training loss: 3.7114...  0.3435 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18115...  Training loss: 3.7373...  0.3437 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18116...  Training loss: 3.7090...  0.3446 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18117...  Training loss: 3.6922...  0.3406 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18118...  Training loss: 3.7754...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18119...  Training loss: 3.6949...  0.3427 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18120...  Training loss: 3.6551...  0.3419 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18121...  Training loss: 3.7002...  0.3421 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18122...  Training loss: 3.7997...  0.3417 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18123...  Training loss: 3.6661...  0.3435 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18124...  Training loss: 3.6596...  0.3425 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18125...  Training loss: 3.6775...  0.3439 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18126...  Training loss: 3.6892...  0.3460 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18127...  Training loss: 3.7488...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18128...  Training loss: 3.7669...  0.3424 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18129...  Training loss: 3.6923...  0.3425 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18130...  Training loss: 3.7159...  0.3450 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18131...  Training loss: 3.6886...  0.3429 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18132...  Training loss: 3.7185...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18133...  Training loss: 3.7056...  0.3458 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18134...  Training loss: 3.7016...  0.3446 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18135...  Training loss: 3.6413...  0.3427 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18136...  Training loss: 3.5309...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18137...  Training loss: 3.6779...  0.3411 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18138...  Training loss: 3.6572...  0.3469 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18139...  Training loss: 3.6213...  0.3437 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18140...  Training loss: 3.6604...  0.3454 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18141...  Training loss: 3.6503...  0.3411 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18142...  Training loss: 3.6607...  0.3415 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18143...  Training loss: 3.7306...  0.3415 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18144...  Training loss: 3.6269...  0.3416 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18145...  Training loss: 3.6607...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18146...  Training loss: 3.6383...  0.3427 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18147...  Training loss: 3.7223...  0.3434 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18148...  Training loss: 3.6668...  0.3424 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18149...  Training loss: 3.6566...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18150...  Training loss: 3.6471...  0.3456 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18151...  Training loss: 3.6753...  0.3458 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18152...  Training loss: 3.6451...  0.3447 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18153...  Training loss: 3.8223...  0.3418 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18154...  Training loss: 3.7836...  0.3457 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18155...  Training loss: 3.7807...  0.3413 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18156...  Training loss: 3.6508...  0.3434 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18157...  Training loss: 3.7405...  0.3447 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18158...  Training loss: 3.7656...  0.3453 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18159...  Training loss: 3.7380...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18160...  Training loss: 3.7400...  0.3425 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18161...  Training loss: 3.7912...  0.3440 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18162...  Training loss: 3.7797...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18163...  Training loss: 3.7915...  0.3432 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18164...  Training loss: 3.7088...  0.3423 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18165...  Training loss: 3.7537...  0.3432 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18166...  Training loss: 3.7544...  0.3426 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18167...  Training loss: 3.7722...  0.3429 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18168...  Training loss: 3.7356...  0.3439 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18169...  Training loss: 3.6726...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18170...  Training loss: 3.7358...  0.3412 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18171...  Training loss: 3.7784...  0.3434 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18172...  Training loss: 3.5975...  0.3466 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18173...  Training loss: 3.7312...  0.3411 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18174...  Training loss: 3.7482...  0.3432 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18175...  Training loss: 3.6934...  0.3432 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18176...  Training loss: 3.6674...  0.3428 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18177...  Training loss: 3.7261...  0.3409 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18178...  Training loss: 3.7098...  0.3406 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18179...  Training loss: 3.7180...  0.3407 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18180...  Training loss: 3.7395...  0.3422 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99/100...  Training Step: 18181...  Training loss: 3.7486...  0.3426 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18182...  Training loss: 3.6699...  0.3440 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18183...  Training loss: 3.7264...  0.3464 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18184...  Training loss: 3.7236...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18185...  Training loss: 3.7107...  0.3450 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18186...  Training loss: 3.7634...  0.3408 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18187...  Training loss: 3.7602...  0.3416 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18188...  Training loss: 3.7945...  0.3422 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18189...  Training loss: 3.7239...  0.3428 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18190...  Training loss: 3.7957...  0.3417 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18191...  Training loss: 3.6429...  0.3413 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18192...  Training loss: 3.7205...  0.3446 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18193...  Training loss: 3.7036...  0.3424 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18194...  Training loss: 3.7523...  0.3450 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18195...  Training loss: 3.6647...  0.3431 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18196...  Training loss: 3.6276...  0.3412 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18197...  Training loss: 3.7034...  0.3435 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18198...  Training loss: 3.6648...  0.3429 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18199...  Training loss: 3.6562...  0.3431 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18200...  Training loss: 3.6782...  0.3409 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18201...  Training loss: 3.7627...  0.3415 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18202...  Training loss: 3.7850...  0.3405 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18203...  Training loss: 3.7102...  0.3410 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18204...  Training loss: 3.6894...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18205...  Training loss: 3.6603...  0.3404 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18206...  Training loss: 3.6611...  0.3441 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18207...  Training loss: 3.6131...  0.3455 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18208...  Training loss: 3.7326...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18209...  Training loss: 3.7596...  0.3411 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18210...  Training loss: 3.7076...  0.3447 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18211...  Training loss: 3.7253...  0.3408 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18212...  Training loss: 3.6772...  0.3458 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18213...  Training loss: 3.6180...  0.3420 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18214...  Training loss: 3.6486...  0.3445 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18215...  Training loss: 3.6609...  0.3468 sec/batch\n",
      "Epoch: 99/100...  Training Step: 18216...  Training loss: 3.6901...  0.3452 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18217...  Training loss: 3.5879...  0.3437 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18218...  Training loss: 3.4217...  0.3437 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18219...  Training loss: 3.4402...  0.3427 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18220...  Training loss: 3.5197...  0.3427 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18221...  Training loss: 3.5123...  0.3429 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18222...  Training loss: 3.4358...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18223...  Training loss: 3.6028...  0.3438 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18224...  Training loss: 3.6667...  0.3412 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18225...  Training loss: 3.5965...  0.3423 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18226...  Training loss: 3.6241...  0.3436 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18227...  Training loss: 3.5869...  0.3405 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18228...  Training loss: 3.5525...  0.3414 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18229...  Training loss: 3.6158...  0.3405 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18230...  Training loss: 3.5626...  0.3415 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18231...  Training loss: 3.5810...  0.3429 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18232...  Training loss: 3.6476...  0.3415 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18233...  Training loss: 3.6446...  0.3429 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18234...  Training loss: 3.6281...  0.3447 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18235...  Training loss: 3.6464...  0.3421 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18236...  Training loss: 3.6824...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18237...  Training loss: 3.6179...  0.3396 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18238...  Training loss: 3.5243...  0.3385 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18239...  Training loss: 3.5731...  0.3409 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18240...  Training loss: 3.6301...  0.3426 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18241...  Training loss: 3.6720...  0.3433 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18242...  Training loss: 3.6313...  0.3414 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18243...  Training loss: 3.6618...  0.3399 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18244...  Training loss: 3.6239...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18245...  Training loss: 3.6767...  0.3415 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18246...  Training loss: 3.7437...  0.3400 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18247...  Training loss: 3.7453...  0.3423 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18248...  Training loss: 3.6600...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18249...  Training loss: 3.5792...  0.3431 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18250...  Training loss: 3.6150...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18251...  Training loss: 3.6798...  0.3422 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18252...  Training loss: 3.7413...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18253...  Training loss: 3.7396...  0.3406 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18254...  Training loss: 3.6802...  0.3401 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18255...  Training loss: 3.7092...  0.3388 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18256...  Training loss: 3.7626...  0.3390 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18257...  Training loss: 3.7106...  0.3401 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18258...  Training loss: 3.7351...  0.3404 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18259...  Training loss: 3.8048...  0.3411 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18260...  Training loss: 3.7444...  0.3453 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18261...  Training loss: 3.7217...  0.3428 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18262...  Training loss: 3.6915...  0.3406 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18263...  Training loss: 3.7254...  0.3408 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18264...  Training loss: 3.6808...  0.3440 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18265...  Training loss: 3.6950...  0.3402 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18266...  Training loss: 3.7093...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18267...  Training loss: 3.7672...  0.3419 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18268...  Training loss: 3.7930...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18269...  Training loss: 3.8084...  0.3405 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18270...  Training loss: 3.7905...  0.3407 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18271...  Training loss: 3.7861...  0.3432 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18272...  Training loss: 3.7862...  0.3406 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18273...  Training loss: 3.7485...  0.3399 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18274...  Training loss: 3.7906...  0.3396 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18275...  Training loss: 3.6776...  0.3406 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100...  Training Step: 18276...  Training loss: 3.6642...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18277...  Training loss: 3.7261...  0.3444 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18278...  Training loss: 3.7540...  0.3428 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18279...  Training loss: 3.7448...  0.3388 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18280...  Training loss: 3.7821...  0.3402 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18281...  Training loss: 3.7097...  0.3436 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18282...  Training loss: 3.7069...  0.3445 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18283...  Training loss: 3.6887...  0.3404 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18284...  Training loss: 3.6678...  0.3409 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18285...  Training loss: 3.7282...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18286...  Training loss: 3.7640...  0.3389 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18287...  Training loss: 3.6799...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18288...  Training loss: 3.6520...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18289...  Training loss: 3.6898...  0.3420 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18290...  Training loss: 3.6726...  0.3435 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18291...  Training loss: 3.7046...  0.3408 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18292...  Training loss: 3.6844...  0.3449 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18293...  Training loss: 3.6605...  0.3425 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18294...  Training loss: 3.5929...  0.3408 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18295...  Training loss: 3.6857...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18296...  Training loss: 3.6088...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18297...  Training loss: 3.6744...  0.3425 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18298...  Training loss: 3.7102...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18299...  Training loss: 3.7383...  0.3419 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18300...  Training loss: 3.6958...  0.3385 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18301...  Training loss: 3.6894...  0.3399 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18302...  Training loss: 3.7848...  0.3413 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18303...  Training loss: 3.7129...  0.3448 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18304...  Training loss: 3.6278...  0.3427 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18305...  Training loss: 3.7265...  0.3392 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18306...  Training loss: 3.7818...  0.3405 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18307...  Training loss: 3.6863...  0.3409 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18308...  Training loss: 3.6525...  0.3396 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18309...  Training loss: 3.6849...  0.3409 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18310...  Training loss: 3.6976...  0.3417 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18311...  Training loss: 3.7580...  0.3423 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18312...  Training loss: 3.7910...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18313...  Training loss: 3.7299...  0.3386 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18314...  Training loss: 3.7230...  0.3438 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18315...  Training loss: 3.7312...  0.3436 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18316...  Training loss: 3.7625...  0.3447 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18317...  Training loss: 3.7432...  0.3395 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18318...  Training loss: 3.6990...  0.3440 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18319...  Training loss: 3.6627...  0.3413 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18320...  Training loss: 3.5436...  0.3423 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18321...  Training loss: 3.6905...  0.3420 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18322...  Training loss: 3.6588...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18323...  Training loss: 3.6490...  0.3397 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18324...  Training loss: 3.6893...  0.3427 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18325...  Training loss: 3.6856...  0.3432 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18326...  Training loss: 3.6689...  0.3402 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18327...  Training loss: 3.7520...  0.3419 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18328...  Training loss: 3.6912...  0.3444 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18329...  Training loss: 3.6913...  0.3440 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18330...  Training loss: 3.6416...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18331...  Training loss: 3.7737...  0.3431 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18332...  Training loss: 3.6952...  0.3407 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18333...  Training loss: 3.6573...  0.3400 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18334...  Training loss: 3.6517...  0.3413 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18335...  Training loss: 3.7015...  0.3404 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18336...  Training loss: 3.6744...  0.3401 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18337...  Training loss: 3.8245...  0.3423 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18338...  Training loss: 3.7816...  0.3399 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18339...  Training loss: 3.7886...  0.3382 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18340...  Training loss: 3.6651...  0.3429 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18341...  Training loss: 3.7310...  0.3426 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18342...  Training loss: 3.7904...  0.3398 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18343...  Training loss: 3.7495...  0.3404 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18344...  Training loss: 3.7635...  0.3425 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18345...  Training loss: 3.7324...  0.3428 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18346...  Training loss: 3.8011...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18347...  Training loss: 3.8236...  0.3424 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18348...  Training loss: 3.7247...  0.3387 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18349...  Training loss: 3.7344...  0.3418 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18350...  Training loss: 3.7313...  0.3411 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18351...  Training loss: 3.7589...  0.3411 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18352...  Training loss: 3.7366...  0.3412 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18353...  Training loss: 3.7109...  0.3410 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18354...  Training loss: 3.7661...  0.3414 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18355...  Training loss: 3.8097...  0.3393 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18356...  Training loss: 3.6309...  0.3422 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18357...  Training loss: 3.7337...  0.3438 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18358...  Training loss: 3.7917...  0.3446 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18359...  Training loss: 3.6961...  0.3440 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18360...  Training loss: 3.7091...  0.3421 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18361...  Training loss: 3.7577...  0.3432 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18362...  Training loss: 3.7138...  0.3413 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18363...  Training loss: 3.7176...  0.3421 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18364...  Training loss: 3.7869...  0.3451 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18365...  Training loss: 3.7725...  0.3435 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18366...  Training loss: 3.6950...  0.3422 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18367...  Training loss: 3.7284...  0.3397 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18368...  Training loss: 3.7187...  0.3424 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18369...  Training loss: 3.7177...  0.3413 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18370...  Training loss: 3.7822...  0.3424 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100/100...  Training Step: 18371...  Training loss: 3.7679...  0.3412 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18372...  Training loss: 3.8026...  0.3447 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18373...  Training loss: 3.7478...  0.3416 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18374...  Training loss: 3.7725...  0.3403 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18375...  Training loss: 3.6509...  0.3402 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18376...  Training loss: 3.7214...  0.3400 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18377...  Training loss: 3.7156...  0.3400 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18378...  Training loss: 3.7085...  0.3438 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18379...  Training loss: 3.6613...  0.3434 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18380...  Training loss: 3.6372...  0.3448 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18381...  Training loss: 3.6983...  0.3447 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18382...  Training loss: 3.6842...  0.3431 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18383...  Training loss: 3.6292...  0.3445 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18384...  Training loss: 3.6899...  0.3453 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18385...  Training loss: 3.7782...  0.3434 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18386...  Training loss: 3.7865...  0.3437 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18387...  Training loss: 3.7259...  0.3422 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18388...  Training loss: 3.6906...  0.3435 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18389...  Training loss: 3.6746...  0.3442 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18390...  Training loss: 3.6277...  0.3440 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18391...  Training loss: 3.6198...  0.3407 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18392...  Training loss: 3.7654...  0.3437 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18393...  Training loss: 3.7491...  0.3437 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18394...  Training loss: 3.7414...  0.3442 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18395...  Training loss: 3.7220...  0.3420 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18396...  Training loss: 3.6818...  0.3457 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18397...  Training loss: 3.6351...  0.3448 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18398...  Training loss: 3.6482...  0.3431 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18399...  Training loss: 3.6622...  0.3469 sec/batch\n",
      "Epoch: 100/100...  Training Step: 18400...  Training loss: 3.7246...  0.3427 sec/batch\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        #todo: \n",
    "        #用 sess 去得到初始 state 保存在 new_state 中\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        loss = 0\n",
    "        dataset = build_data_generator(train_set, batch_size, True)\n",
    "#         print(\"dataset:\\n\", dataset)\n",
    "        \n",
    "        for x, y in dataset():\n",
    "            counter += 1\n",
    "#             print(\"x:\\n\", x)\n",
    "#             print(\"y:\\n\", y)\n",
    "            start = time.time()\n",
    "            \n",
    "            #todo:\n",
    "            # 构造 feed_dict\n",
    "            # 这里, 我们需要得到model.inputs, model.targets, model.keep_prob, model.initial_state的输入\n",
    "            # 需要将上一步得到的state作为这一步的model.initial_state\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            #todo:\n",
    "            # 使用上面定义的 feed_dict, 运行 session, 获得当前 batch 的 loss, state, 并运行 model.optimizer\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/checkpoints/i{}_l{}.ckpt\".format(counter, rnn_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, rnn_size, batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以查看在 checkpoints 中所有保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i18400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i4000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i5000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i6000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i7000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i8000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i9000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i10000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i11000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i12000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i13000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i14000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i15000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i16000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i17000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/checkpoints/i18000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18400_l512.ckpt\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练完成之后, 可以从 checkpoints 中恢复模型, 然后我们再给网络输入一个字符, 再让 CharRNN 不断生成新的字符, 也就是让神经网络\"写诗\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让输出的字符更加丰富随意, 这里我们从模型输出的概率向量中随机选取前几个中的一个作为最后的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在是时候看看我们训练的神经网络能够写出什么样的诗了, 这里封装成了一个 sample 函数, 需要完成下面的 #todo 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意, 我们在用 CharRNN 写诗的时候, 需要先用输入的字符对网络进行\"预热\", 这个过程我们不采用网络输出字符的结果, 但是可以得到一个更好的状态, 然后就可以用这个状态作为后续生成新字符的初始状态, 从而获得更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, rnn_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    \n",
    "    # 构造CharRNN模型\n",
    "    # 注意, 这里我们处于测试状态, batch_size 和 n_step 都应为1\n",
    "    model = CharRNN(convert.vocab_size, rnn_size=rnn_size, sampling=True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        #todo:\n",
    "        # 得到初始 state\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        # 这一步我们先将输入的几个字符对网络进行\"预热\", \n",
    "        # 这样可以得到更好的 state\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = convert.word_to_int(c)\n",
    "            \n",
    "            #todo:\n",
    "            # 像之前一样设定feed_dict\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            #todo:\n",
    "            # 得到概率输出以及当前输出状态\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, convert.vocab_size)\n",
    "        samples.append(convert.int_to_word(c))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            \n",
    "            #todo:\n",
    "            # 像之前一样设定feed_dict\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            #todo:\n",
    "            # 得到概率输出以及当前输出状态\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, convert.vocab_size)\n",
    "            samples.append(convert.int_to_word(c))\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下训练时长最长的模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i18400_l512.ckpt\n",
      "天青色等烟雨无看年夜  在  何人有路心  不为学天人  故国上云人  凭山宋叶衣  危葭落夕云  庭河上望云  吾思寄何游  话  长人入上声  谁看上塞人  空看楚塞秋月风  莫叹如兹时  知君亦为襟  大禽飞水下  兹尺学金饰  素  天山在为心  萧洲逼蜀时  花山动白楼  春风落落风  江山上雾新霭风  如知圣柄明  当朝帝帝骢  明时意时行  何夜上云尘  愿言命上来  故门多此人  寒门满暮楼  更来天月深  吾君载古心  自知千涧云   翩盖降车列  共受丰中年  金旆在文台  徒此圣花开  圣光方大回  金飞织管空        风  秋  春风一不心  一怜支署微  还来思落山  山泉生竹生  寒吟向夏门  孤风隔海桥  无看入雪山  何堪白里中  谢君策水深  萧桡思钓然  登余入海庭  江流隔汉城  几朝杨桂尘  裴条有此情  空看映蕙文  佳传奉彩文  六媒四表书  清飘玉帐舒  含门意应知  何能素妾情  妾声看芳叶  相对花光清月同  妾妾折罗台月    光妾娇  上袖纤妆叶  拂舞含光扇  虚乐表秦恩  仙  天 何日上中开  清首拂花声  新门绝火风  无逢老此风  君言黄士来  因朝杨月来  关林月月寒  还因比化衣  金驾大川城上人  东山在水楼  春风雁路深  江峰洒洞衣  楚人过顶稀  茅冰落晚寒山心  一念在中情  相言白道时  青袍入石篱  终来得古年  眼冰桃玉冰空     风山映尘发  王条意幽曲月香  红粉拂飞衣  乍彩湿花飞  无  何去是无人  一窥鹓塞尘  平山不在时为风  早  明日复如里  自言尘贵玉不风  春看发花人  一为赴塞人  烟河落树斜  孤馆信无行  天枫北里来  行来发水楼  春风吹不行  一从出上中  谁知车下春    □风逐天发  因当二宰人  生精礼圣知  朝天渭水来  凭将逐镜台  花看映翠风  风风散彩罗  六士敌鱼时  吾君重恶簪  圣然游钓邑人归  不著入荆枝  结听黄风襟  周符神礼馀  天晨意中新  香  何风不如归  清看缑殿衣  江山有望飞山山  一是雪花时  莫是见君人  江看西远城  东城游雁归  故日戎铜衣人   风得映兰萼  愿嗟夫子国  文代清光暮  不见此人情  此事是师人  此日任于尘  懒觉映龙风春不中  谁人长此深  谁为访君名  今知入江思  金崖燎绮裘  如将文李心  皇然属皇君  峥迤七器然  文圣凤舜仪  至物致皇宣德情  五游表方玉  吾贤德义来  行年寄上情  庙花盘角风荷春  天雨落云声  一拜式士时  千风歌太君  日是牛衣辉  空卧抱中心  众  自山风物人何春  坐得过花人  昨日在悠然  万载在王洲  江山楚塞寒  烟潮海木飞风中  渐觉朱衣声  一向白头      春来映相月  愿年无太年  寒云云径风  兹士敌玄人  大德列皇才  羽历列书风  香步夜云空  文命动君台  凤树挂寒秋  欲  天日在如风  长见钓罗衣  谁恐弃体尘  愿君迩道行  三王符武骢下人  离年此夜人  何知是此人  何因到别行  渔洲湘钓峰  谁人见者身  他知道道情  见  此有中  不人长所时  一妾拂罗花  不似玉香台  红质飘罗杯  谁以为开风  神袖命咸水  从将泛上日  长为谢群去  一见不郎然  何是说君来  出  此花听上存  如如赵肉衣  病酒骨罗球  晓  此  日日向花风  独见山城人  征人发此心  高峰倚水时  湖风过雪衣  相人问故时  唯逢有苦劳  如逢大悟书  山风有月来无有死  不如尘上语  不个染金错  积月卧中生不清  我妾祸忠君  不是有花心  莫是洞秋春  何思见白楼  杉声宿亦行  江江草水人  一知月照人  空来入月春  谁知骨德仙  唯看历圣文一风树无月  不得看金色  一作绮鸡客  风  无影落花声  玉息征荆林天秋  白馆入云楼  建骑拜瑶州  含为见迁回  翳氏亦能防  为作比门年  故思思汉踪  南山山路间  空山听第深  相思信归水未风  遥看剑殿声日  何日有花饰  谁我学其泪  素节澹如古人心  一使出门城  白雪宿声寒  不为骨吾踪  吾名学者人  黄门向水中  如应入物情  终朝入水行  秋风野塞山  寒流草树云  吾言任九成  西山秋月流  飘  天水出兰人  承为命瑁游  光疏入暖风  花花拂玉弦  遥歌四羽月山风  一此春上时  红花半漏寒  空来黄桂尘  裴恩临太台  三随摇气清  游恩汉兴行在云曲 风  一月出中山  一见入鱼中  空首出花深  故山秋鸟低  青斋宿乱楼  蒹洲归自情  月风摇绿花歌风  香气在花城  秋落闭罗衣  何忆从山阙  一种青波后         含文入树中  今有上中人风人  不得风花台  不是金镜情  妾道折花里  何得赏生机  山吹塞阳深  今行无别时  春风吹杜声  凤哉秦客情  西思务幽游  清雀有素声  谁年得此游  露红贮导轻檀归  影  今君无帝衣  离家思别心  江山芳壑中  芳策在先心  还年入汉情  一旗陈羽草风风  白马看华颜  圣佛比皇回  天臣降尧天  旌驾佐秦云  天乘风英明天风  谁有升君人  风色入寒泉  高登白畔声  何逢入水人  何因白国中  殊花入淮阳  千帆见远林  青波江月间  相年住有宽  何知见路时  空看黄玉弦  江上洛门城  月暮白声风  春云虏路生不高国山心  独望望山流  此人殊吾子  蹉勤仲仙水  春风起水人  樵田过雨云飞云水云月  相说为芳节  何知叹酒君  上林鸣鼓稀  一得妒肠情  水落重芳阁风宿  清风波蒲色  悠舟有幽里  北听山映息  谁为荡禾熟  空逢偶亦吟无寒  形干掌云中  荷月出西流  云峰洒上衣  山上白城天  万日思清尘  应为赴阳城  高霞入楚流  大风宴汉门  波疏满户船  春山满落衣  何朝物信人  一师尘月间  空山入暮中  他知入影深  何人吟白阴  几人著者风  高泉在石庭  闲来逐石流  山声落汉山  吾年征阙吟水闲  青峰傍水时  经山吹雪尘  松  不人一上头  我知弃太时人生  万里知君心  一作离阳衣  不览傍氤巾  吾令白人开  自言春自归人长  地  一年有中时  何能免君发  今思迟中期  万道寄东程  春月夜痕风  灼面戏腰去  白语亦成风  一知游襟心见清  心  此日入山人  还风朔塞新  鸣装积树楼  江山古木中  曾知蹑面时  何年起月峰  危声动古门  波霞金盖飞山尘  凤玉荷花风  圣命在荣诗  金媛敞龙衣  清风知人归  天楼出御筵  阳池映水清龙光  长为荆海年  单车近素虚  承楼象水台  还看入羽来  参飘拂翠移  霜光吹自箭  大摘陈雕殿 萦威洽承书接君心  谁为上里飞   云映罗龙黄  清户入阳开  汉子愁生望风水  谁年无此日  无看荡衣草  万里来消变 神  何日出中情  不见布陵衣  长载入陵关  鸟  天日  不  不年见似人  不言入人时  妾时任不尘  天怀竞镜心  浮歌含凤池云时  谁见命香风  徒为涉陵游  云吹映上云  日知  非  不人不不时  妾人祸奋素  从朝折芳阁  手手盈郸衣  今人有为此人  何思入风深  谁是白枝开  一见愁心客山风  一是海门山  为与尘士人  只知玉物人  空时照舞飞  团  人人亦清水  儒  山日有花峰  何得寄高涯  影飒含光花月时  神  何思有衣妆  何言有妾情  匈  白日在君心  莫以屈皇襟  大世贤已歇  子  自人无鬼心  但为游上言  红  不人不殿声  谁言无隐人  结是凌金花  空是咏朝游  岂有临龙阁  楚子忘鱼发  浮道感为发  清哉起名苦  世道望不里  莫是叹于不  日  不此得人时  不见山上开  一言生水时  吾年不尔心  春花过上迷  江思接寇旗  方山战字新  簿戈亦有欺云尘  曾为屈华游  高轩出玉弦  三飘拂翠舒  风光红影声  今年羽景阑  春风吹塞花  临风落复情  还堪怀物生中埃  三仰弄春风  一驱边草阴  沙波映木寒  何知得隐名  长事在明年 声  一里有天台  空为竞蝶时  登波依古阙  小  一日有成情  一将泛汉阳  微风拂水云  春云云树风  诸哉道子情  幽首在清河  明色吹云迟  一为忘远忧  儒泠映月云  贞精玉庶玉文中  圣景有花回  有胜作大花    花  不来过未衰  江人应到花  如知费目生  禅房入石松不知  一为邻骨骨金裳  四门波草流  高吟绕洞庭  湖云江木中  半风松上声  荆帆入塞楼  相峰杨叶花人衣  清月拂花声  不为双李心  也之精圣德心恩  欲想在门心  张  自山得人人  一  天日入花眉  一如芳管清  白  白知有生水  别望在何处  谁为贤藿捐吾宗  时  春花日自死人   歌花覆 春寒 不一行  相为尘  映  无风流  白  何知人  向 日风秋  一与中  讵  自生流  讵此尊  不沾明  受  一生  一与深  不相襟  独作安与家  寄傥诗  睿 愧一中  可人生  水 堤此年  大 色连枝叶 霜一 马正时  英 里砥诸里  不用纤神  陶 君新中  一剑  水雪  一与山  有 堤一分草  经生阆  白 怜此风见  白叶迟  插 马不生  不行流  旧 知无风人  还不逢  有 襟不生色  可无生 色白不过  几山生  清经山  亦不成  寒 鹿不不深  自分金  不与贤  何知何  一 色连风  月风玉  含麋车妆 疑 筵满山  自风生  不作成  有 时经风  自 襟无人色  不生明  上  何风生  不 歌新秋色  自 歌长中  自长新  水 知正分达  不生生  时  不难人  剃 庙新秋液  愿悲风连  荷 襟一风  不知襟  阶 歌新蔡分  人生有  讵用师  纷 色一云  一生中  圣 堤不风色  倘中  不不一  一相轮  白 马经人  不知寒  插 歌申风里  周 牛何不时  不生生  一 吏春寒  入下  一生心  玉  不时人  不 拳锵新  伫与戎  含章  转广 森百 知不山达  此人知  采 堤不风  一 拳无一生  一知流  月 风正人  水 场不行去  何知尘  水一章  插 场新霜  伫用 歌正剑  有枝同  玉转  一人分  一 堤新不分  两 襟无风色  故生心  白 色一行分  自知师  雨  一人生  一见分  水  一分时  旧  相风风  伫与章 载春中  更 水砥天食 愧才用  旧  相生  不有  不难羞  受  不生生  一 赋秋山  不长生  一生风  月 愧一中风  入此分  不上纤  空时破  场 色白行风  远天章  玉 狖一时  已知深  结与生  一风时  庙与文  一成分  珮化冰枝  云 色三风色  不生同  一辍  作姓生  蟾 歌连寒  不长轮  一 色一山  不清风  解 筵新中  转 拳新风  耻是光  玉盈牛  柳 愧已春  转  有难生  入 襟一风  可 赋春风  自同流  插转睿  一生  莫此风  不看生  不 襟 载白 拳一一  自人生  柳 筵正中  三天  未人  莫雪迟  水 知不新  无名有 拳无用  杳自知  麝 羞一时知  不与知  风风  不多轮  水 怜不新  不人生  不生裛  有如生  自知狂  不上新  蝮亦谁见  莫觉空明草  为叹圣章近  清清尊  不春中  伫 马生时  一有寒  如山冰  结 堤生襟  一有风  一生结  玉露霞  水来  插清分  一 拳知用  莫 知无春  月 时无秋色  清 人一春  一风流  水是皇华  琼鼓段  为医得  独作流  不分  不分风  月 襟一年  不人生  插 吏生草风  大进歌  英 筵插不枝  幽 色  故人年  寒  一人风  把 赐新春  水 筵一风  三文纤  有  无中风  共是尊  寒纤瀛  一作用君  不相襆  何是  岂得轮  不 草申风里  知 襟碎难枝  霜夏山  春来  自新轮  独 色生中  不来  自生轮  水  不年生  水 堤不纤  自过过  月相妆  森罽纤  玉  不姓生  蝮此无事  独此红耳  中陇字  上  更分生  旧 堤春枝  月  一生生  玉 筵锵尘液  有 筵春春  长  不不光  惜年心  赐 从三春  不 堤经难段  莫与光 粢插用  已 筵一山色  不此生  白分襟  玉 堤生章  一 堤生海  旧 筵锵天液  水 赋满草  场香  圣人  已雪   伴 狖无山枝  故人归  不有分  已如纤  山 狖不中里  一怜同甲      玉光娇  山  不中风  愧有章筵  汉  一行风  一向寒  不 色一相时  不与知  无 色插一色 风禁鼓  直沾文  一 马生人  不有 名春不  大清章  一 予生春人  欲与清  马 堤无轺草 寒不 怜新家  雨  无知期  解向清埃  还锵鼓  鼓 筵正中  一分迟  独此兼玉欢  含  三春风  受  惭\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下刚刚开始训练时模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoints/i1000_l512.ckpt\n",
      "天青色等烟雨人山人  何是自中中  不山不上时  今风自自春  何风入自人  今  山人一自风       人山入自人      风风入人时  今人一路风                   风风有自时                山年有自人  不风不不风  何山一不春  不风入上中    何风一相人  山日不人归  不山不上人  风风见不时                        风人入不中  今  山人入相时  何日一人中    不年入自中                        春山不不风        山人入相中  何日不相春  一风有不春  山风一不人  何日不山中    何年一山人  何  不人入自人  山日有相人  何水入山归  山  山  春  何风有不春                     风山不自中                                                  风山入路人  今人有上中  今人不路人  今人不上中  何风不不人  山山有自人  何风不不风  今山有水人  不人不水春    何日不山中          春人不上人  何日自相时  何  山风入不稀  不风不不风  一山无水生  今人不上风                       山人不不时  何风有自春  风人不自人  何人一自中  不  山风入人时  山人见自稀  何  何人不水人  风  何人一路归                                  风年不不风  不年入上人  不人入路中      何人一不时       风年入上春    风风有不春无山上  不年一山中  不人不不人                                 山山不不人  何山不路时       人山在上人                                风山有路年  不山有水中  风年入上人       山生一不时  今风入水中                                                                     山山入不人  何风不上时  不风不水人  风山不自中                                                                            山人入不人  何山见自人          春人不山春                                                                                                           风年见相中  何风不自人  今人一上春  何人不水时  今  何风一路人  风风不路生  风人有自人                  风年一上人    风山有自人  何  风人不不春  今山一路中    山山不相春  山风入水时  今  春山一自春         风山在不中    空山有相中  不年有水人  何风有上中          春风入不春  何风入不人  何风不自时  山风一不人  何  不        风人有人春  山风一不人  今风一水生                                                                      风                                            风  山人一自时  何  风  山人入不时                                                            山人有相人    不山入不风  山  春人入不稀  何日入山时                            山山不自人  何山有水稀  风人入自稀                山年不不中                                春风不上风          不山有人人  不风自不春               人人入自中  何  何人不上人      春  风  何  春  山风不路中                             风风见不风  山山有上中                                                                          山山入不风  山日一人人  不人不自时  不山一上人  何风一上人  今风见水人                             人山有不时    不日不云中  何月入山人  不人自上春        风人一相春  不风一自风       风年不自人  今山入自春                     山年不自人  山山入自时    不人有人人    何日不中风                                                                           春年一相时  何人不不中  何山入水稀  今  山  何山不不春  何山不自春            人山不自时  何  山年自上时       人风入路时  今  不年有上归  何  山  山  何  何日入云春  一日不中中                                                                                                                          山人一不时  今人有上中  何人一路中  不人入不时  今年不路人  何人一自中  何日无人归  不人不上人          山山一不春  何风有自人                  风山不自人  山人有上稀  风年入路时  何  风  山人自自人    不风不不中  不风一水人            风风入不风  何  何人自自人      何年不自中          春风一不风               人年在相风                                                                                                        人人见人中  山风不自风                                  春山见相人    空山一不时  何山入水生  风  春人入不稀  不人入自春  今山一路时      何风一自时                                  风年有上时  风  何风入不时                     风风有上年  何人不路人           风云一相人    空山一不人    不人不相中  山风不不中  今风有路中    风日有人人  一水不相春                                                    人人不相春    何  山人自自时  不人入上生  今  何  风风一自中    山风入不稀  何风不上人  不人不路时  不风自自人  何  不  不  风风入自春  今  山人不路中  今风有自稀    山  何  风年有水春  不  风  风日一相人    风人入相人                                    山  风日有相春                                                                                       人山有不中                 风年有上时  何  何山不自风          人风有相时  何风不路人  何山有上中  风山见上时  今  春风入水人          山山一相中  一  春风一自人  何山不上中                         山风一上春  不年有不人  何日不云风  一日不山归  何水有相春                                 风风不自风  今  山日自山人  不日入相春  一  何  不年有水人                                         春山不自人  今山有自稀              春风入不人  今  山山有不春         山人在自风  山风见自时             山人见人中  何风自不春                  人  山  春山入不春  不  何风有自春    风日入中风      山  何山自自人  何日一云人  山  山年一自时          人山有自风  今山不自人    不  山  风  不年不自人                      人年不不人  何  风年自上稀                           山风见人人  山人不自稀    不  何人自上春        人日有云中  不日有相春  何  春风一上人  何山入水生  风人见自稀  何  山  何日自云归  一风入不稀  山  不  风  何人一不稀       风风有自风    何人一相中  何人一水人  何人见上稀                           山山有上风    不山有相春                             风年有上时    不风一相人  今风不不春  山山见不人  何人一水人          风风有相人      空风不不中  不  风人自自中                                                                                   山  何  风山不水时                                         风  风风不不人          山人入人人  今人自上稀  今风不不风    何人入人中  何风自自人                                 春年入相春  今日一山时  山水有相中  山人入不中                               风年入不人  山山一不稀  不风一自中  何  不山入路人                          风山入不风                风人入自风                                                 人人入相中            人年有上中  风风不上人  今山见水人  何山不路风    不风一人风  不山不自人      风人不自人                    风山有相风  不风一路时  风年不路中  风山入水人  风人一路风  风风一路人                                                                                               风人有一时  何日不山风  何人自不风                山风入上年                       人风入不时      何                                                                                         山年见相时    空日不中中  何月自云中  不  不年入上人                                                              风风入相春  何人一自时                  春人见人风  何  何人自自时  不  不风不相春            山山在相时           人人一相山  山日有山时  不风有路中  今人一水春      风  山风入水风    何人有山人  今人入路人                                           山山入不人  何山入不人  何风一路人    不人一相风                                                                     山年见上人  不  何山不人人                      人年见一人           风年不上风                              \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/checkpoints/i1000_l512.ckpt'\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoints/i5000_l512.ckpt\n",
      "天青色等烟雨  花歌玉月开  金山玉日筵    不中不天水  天山不自中  天人何日归  风云出不人  何山日月人  何人日上人  山山入不人  何人日水中  长来日日衣  何来日不中  长风出不来  何人出上归  云来不上心  无山无日心  无风日水归  风山日月归不不里      风山日风    青人风    山人不水衣  天来不不中  何山日水人  风云日上中  山来不上归  山风不不中  风人有水归  风风不未深  谁君何上心  不君无不来  长人望上来  山来有月人  空来入月人  长山入上来  云山有上中  无山日日来  山山日日归  空云入上流  长风不未中  山山无里人  风来不月归  风云有不中  何人不月心  还风不未人  不人一月来  长云日不中  何风日上来  山人一上中  长人入水来  山云入不中  山来入上人  长风一水归何中去  月  风风一云树  山山何不人  何人有水归  山山不水中  何风入水中  空山不不中  长山入不时  何人日不人  何来出月人  风云日不来  山人望上归  山山不水来  云云玉上来  山山入不深  何山日水人  长人有日来  何来入月来  空云日日归  山人不日深  山山何里心  何山日上深  无人无月心  空山落月流  风山日月人  山来入水深  长人何月来  何来不未深  江风一未来  江风不上心  相山日不深  山人有月归  空山日上来  风人日日中  长来不上深  山山无上来  何来落水来  山来不未归  长人入不人  江人不不人  不风有水来  山来不上归  山山落月归  风风不上深  长人不上来  风山出水人  云风日路人  山云落上开  江人不日深  山人一水深  长山有里人  江山不不深  山来不不归  长来不不归  何年日不春  高山有不时  不山有水来  山山入月归  山风不不春  风人不不春  风风不水人  风人日月来  长人入月人  空云不日归  山人不不人  山来不未人  何来何月来  何人不北人  何来出不人  风人不不春  风来不水人  风风日未中  山风日上来  云风有月人  云来不上春  风来入不时  何来不不来  何风入水回  云风入水来  山人不不归  风山有月归无山月云归  不山不未中  一知何所归  不花春上人  长人何水人  山风不水归  空山不上人  长知入不中  山人不未归  高年入月心  云云日上深无有人云归  一里入相归  天来无上来  山花出不春  空风落上开  云人入日深何未去  风人何所发  不山日水人  江风不水来  无人不未归  长来一上归  风人日上归  江风不不深  江年不不归  何人有月人  何云不水人  空来有上来  风来出日来  江风不水流  风风日上来  江风有不来  何风有月回  山山日日归  空来日月归  风山日路人  山风日月人  何山有不人  山人有不时  空风不月来  山来玉水春  何云不上归  何来不月来  长风入水来  山山不日人  何人有日来  长来有上开  山人不日中  何山日水来  山风入水人  山风入上开  空云不月归  长人不月开  云山不水中  风人不不来  长知不未来  山来不月人  山  不山无不归  何人有水人  长来有上春  何人无月归  长风不未归  何人入上人  长风有上来  山云入日归  空山日水人  风来日上流  无来有日归何有月云归  何来一未深  不来有上归  风人入上来  风人日不归  空来出上来  何人不月人  何来日水人  何来日日来  风来不月来  山云不月来  风来入不来  空风玉日开  山风日上人  无风入月归  长人入水归  何来入月中  空山不里归  山山不水归  风来有水来  风云入日归  山  春山一云人  山  风来有人人  江风不云心  山人无不人  山山无日归不风里不人  秋山无上人  何来日未来  风风望上归  长人落不来  空人出月回  山  风风有山发  不人何不月  何日有人来  何人一上心  何山不未深  谁知不上时  何风无不人  不人一月来  风人落月来  空人入上归  空山不上人  高年入水归  风山出路衣  何来日不来  山人不上人  山人不不中  山风有上归  长风有月归  空人出树来  何人出月归  空人入路来  何风日水回  何山出水中  何风不月来  山山入月人  山风有日衣  空山入里衣  风来入不来  风风落水人  山来日不中  长山日月来  云来入上人  风年日水来  云山不不人  何来入里开  空人出月开何相里山人  一此在云人  何人有不归  高年日月流  何来落水中  山风日日中  云山不月来  长风落水来  云来日上人  何来入日中  空人入上开  云云日里来有不日人归  日  山人无不时  山人不未时  长知有未人  不人有上时  山人不月归  空来玉树开  山云出上回  云来入上中  无来落上来  长来日未归  空风不上归  空山不上人  风风不不人  长来有不归  长风不不人  山来有不来  何山入不人  长风有不时  山风有月人  风风入里衣  云人出水中  山来不上心  相风不未深  一山不未来  何君一未归  何来日上来  长山有不中  山山有月人  长人日上人  风山不上归  山来入上来  长来入不来  山风不月衣  何山入月人  风风落月开  金风玉日衣  山山出不归  风山有水来  山风入上来  风风落日归不相上山风  不日有人来  山里入花流  山风不云月  谁来不相发  不来春未人  一月下春开  日  何人春不人  长山不云来  何人入水人  长风落上人  天来入云人  风来不水人  空来不不归  长风出水来  云云日上回  山人下上归何山去人中  不日一相归  天月不云心  不人不不归  山来不上来  风人落水来  云人下月人  空山万不归  长人不水人  何山有月人  风风日月衣  空来出日人  何人入水深  长山入不归  风风不未人  不山无不中  不风一月人  何来不未开  山人入日来  山  不山何不人  不山不所流  不来不月来  山山不未归  何人不未来  何人有上心  不来一此来  山风有不人  谁山日里归归风去山风日  不日风相情  山日一人来  山里落云来  一月入花人  何年有月来  风风玉月归  风来入上归  空风万上中  云风不不来  何来日月回  空风入水衣  何知不水心  相来不水时  江山入水人  无山不上归  江来不不来  无人不不春  山山日水人  山来不日中  无山一月中  何云落路开  山来出上来  山风日上来  风云落水回  长风不水人  山山不不人  风风落月开  山云日树人  山风不水流  山风入不归  山人出上中  长山不水心  何人不不来  风人入上人  空风日水人  空山不水来  何来入月来  空云落树人  云山出上深  高来无月来  风人入上归  长山出水来  山来落上人  江年日上来  风人不不来  山年入上来  何山不月来  风风入路来  风云日上衣  江风有上来  风风有上人  山风不水归  秋山落月归  山来出上人  无人有不中  山山不水来  无人有月人  风风不上归  风人入不深  长山有不中  高人无月归  山风落路来  山人不月人  山人落不中  还年不不人  不知入不人  山来有月归  天云入不开  金风玉月归  山风出水深  江来有不深  长来无月心  无风落上人  山山日上来  何山有不春  山人有不来  山风不不归  长来落上人  长山入月人  风  不来出上心  何来不水人  何花玉不归  云花不未开  山风不里深  山山不不人  长来不月人  山风不未归  何年入上人  山年不不来  长云有水人  何云日月归  云人落水人  山人不上人  山来不未人  无年无上来  空人不上深  无山无月归  何山日不归  山风有不人  谁人无月来  山山日上人  风山有月人  风山不未人  不人何水人  山来不不人  何来不水中  长风不水人  山年日月来  云风落月人  空人出不归  长知入上人  云人出日人  山  山人无月心  风山何水归  山年不水深  长风不上来  高人何上归  江人日上人  山山入里来         春风月云发  长花出月衣  何来有水心  长来不上中  长风有月来  风人日未归  长人有水人  江山不上归  江人不上人  何来日不时  空山日上开  空风落日人  江山入不中  空云出月来  空人不月归  云人日上衣  云山不月人  山来入日开  山人入水归  何人不不中  江山不月归  何风日不人  长来望不春  长云不上来  风风有上来  云人出水开  空来出月人  空风日不人  何人不不归  山来出不人  山人望月开不山去不人  何日有风回  山有有中心  日日一山来  风风落未开  山风出月人  山  春风无不中  何年入上开  风云出不开  山风日上人  云风出上回  金云日上回  风人入不春  山人不未来  长来不未来  长知有不人  何知不不中  山来不水人  空云入日开  云来日上归有无日云春  不有有人人  天里有云来  日见入山人  风日有人来  山人何水来  江来月水中  风云落水人  还风落不来  山山出水回  山人落日衣  江人不上人  江来入月来  江来月月来  长人入日人  云年日不中  山风日里来  云  山风一上时  何风无上人  何年入水来  风来日不人  江山有日开  山来不月中  空人不日归  山人一日来  何风入水流  山山入月来  山人出日来  风风落未深  谁来不上心  不来何不中  谁来何不来  风来日未归不云月日风  空花出上归  山云不月人  空风日上归  山山不上来  山风不不人  长风日月开  空人出树衣  山来一此人  江来有月中  风风日上中  山山不水心  何来不上中  长来不不中  风山不未人  何人不未时  山人何水人  不人有月开  山山不上人  风年落日开  山云不月来  何山落上人  江年不不中  山知不水来  山风日上人  山风出月归  云来日上归  空来入路来  风云出日回  江来落水深  何人出水来  云云不水中  风风入里人  云来入上来  云山有水人  山山入水来  山山不不人  江年日上归  空山不月深  空人入上开  云风不不归  何风有里流  空人不月中  山知出上心  何来不上人  山来日日人  山山有日归  何风落上深  长人春水归  云  春人一不中  何来日水中  何来落月来  云山不上人  江风不未来  何来入上归  空人入路来  空云不上来  风云有上中  无人不不来  何山入里来  云山日上来  风人日月中  山人日不归  山山日水来  江风落不中  山来何水来  何人不月来  空来日上归  长来日不人  空年出月人  山  春风出不时  风云不未中  不人不月归  风山入上衣  山年日月人  山来不上来  长人有不来  空山不月来  风风日上中  江风日上深  山人何月人  无人有上流  何山出里开  何山落上来  江山日里来                 戋歌览池春  青山出海中  春风出不深  何山不上中  何山日未心  山来无不时  山来有日人  山人入日来  风山入日衣  云来日水中  空云玉上开  云山入里开  云云日水来  风风不月归  云云落月归无何日人人  日有有云人  山见有风里  何来何不水  谁人不相发  不人不不水  长人无不里  何知自如里  山来无未中  何山一不中  长山有不人  风知望上回  风山玉月开  风花出月来  空人出上深  云知不水归何风去不时  山人一月人  山人日日人  空山下月回  长山不不来  风人有日来  风人日水人  山山有月中  山风入水中  何风日水中  长山有未来  长风不不中  何年何上来  何来不上来  长风入不春  云人出水来  风来不日中  何山日上归  云云不未来  长人望水来  云风落不来  山知一月来  何来日水来  何山落不归  高人有上深  江来不未人  何人有水来  山风不月来  风人入树人  空山出不深  山人一上深  无年无月归  长来入水流  何云日上归  山人不未深  不来无未心  何君无不归  不山有水来  风山入不人  山风不水中  山风日里归  风山不水中  空年不里人  何人不不春  江山有不来  长风日水人  空来入不深  长人入水归  山风日水深  无山无上中  长来一水深  山山日水来  山来不月来  山山日树回  山人落日中  山来有水人  山山有水来  风山不上流  长人入未中  长风日月心  何来不未人  山山无月来                 笙风入云衣  天山入路衣  春风出上人  天山不月情  风风日上衣  何风出上来  风人日月来  云风入日来  山云日不中  何人入水归  何山入月深  山知不水\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/checkpoints/i5000_l512.ckpt'\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看模型的中间结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/checkpoints/i10000_l512.ckpt\n",
      "天青色等烟雨  含花意月流  青山日素生龙归  不与山山心  云日见花深  江色宿人间    弱马白云存  一事有人衣  白日出山山  野发见山人  野  何此风已流  不为无此归  春云见路深  白山山水间山云  不在不沾城  山人日云路人人  独里夜云人  不雪暮风人  相知白上深  山门秋水心不人  不风风水归  江云发路稀  孤风日水深  月山山路间  山山入木中    风  何日不云山  江城雨雨深  山山见不人  江山过客深  高山独暮山  江风不客归有花暮人山  不月有关归  山日意何人  山花月叶清  寒声入客人  故人思路稀  春云日雨归  江山草水稀  江花草雪云有花路人长  江事有山秋  云雨入云流  相君无不人    风君此不云  寒山日木中  江花云雪长长山  一日风山山  不与不门城  月月过知过人人  山月夜人城  云月落花水未人  风风积竹山    鸟  不来不未见  山波不不时  孤风不自人    云风何未时  风山花水花  野家山故生  何思有山归无山雨人人  一月有人深  江思人不春  江风落月中    马  白日有山见  归思思不时  野风吹白移  不知云远间  春风月水间  山帆水木人  江山见一年  风河白木深  江风积竹流    水  不君思不时  离风此见时  山风风海人  江来日路人  江城日海山  山云白路深  白风春上山  云风入草深  江山天水来   山上上人人  山风水海云  江人见帝臣  相时白路时  风花落照声  莫君秋草间  东山秋草间  东江秋自人  山花出水流    风山白不死  孤公就白行人归  云草出山城  野日过山下自归  寒山入色来   风见有无时  幽上独东山  风望一风深  何有整门中  素烛带新阴自山  山为白沈间  一月白山时  高风积水低   山雪竹新衣  山风竹木流  山山水照新山云月  不暮空不发  自日惜麟心  野  山云见不春  山光落水中  江山水水深  白心不不归有人  风花白紫香  高条芳节稀  山花带月时  江云白雨阴  禅门不水人  月人寒上来云山  自人秋路时  江山过雪船  风云雪上山  有来寒上生  有为不人人  何夜入云人  江山草雨深  离山独不时  渔流秋月间  江风云客山             春云拂不时    月色入云香  今见寒玉枝  春山独故云  山风吹柳流  东风独路时  风声雪雨深    危  春人日不远清枝  不风日不人不山  此人无路人  孤人独水人  山风白水人  故山江草山云山  今见山上归  不望暮云人  何为秋海深  江山草草流  江风积马林  霜花草水深  自怜青海城如云  无为万日城    弱  风水青云月  春山白山人  风山白色中     寒  不日一春人  春山见水中  风云水水云  啾书宠自时谢中事相难  不年人日人  一夜白云时  风风蓟不斜  白  不君见自人  春山春月深  风山归夜来  江光紫凤衣  江风郢海归    宿  风人风无月人中  野见青人人山生  一见山云时  无见心人人  一日有云山  云日不夜深  山  云人一上人  江山海水深  高风云上山  山云吹水多风人地 山风不不山  山风见水人  江山白雨山  山  不日不为月  江云独上山  春门吹有云  风山草鼓阳  江风水上云    清风白中耳  江云过草深  江亭出水归无山外  水马山中水  不怅此人时  春山意已深  江年无处人  风月白云山  春门日不时  风光照水香  江风入鼓低   花风雨玉阳  江山万苑人   风色不难知  高风意水稀  云云草发深云春  何与一人心  草水带青山  风望白衣生  风光竹木长   山鸟落花风  日  山客人山过  野日白人心  白日夜山时  高  何日不云城  江云草路深  高山见不归  江风雨海楼  江山入水城   花水雨云深  野雪寒青云云人  一此为归时  风风恨一移  一门山夜时  风声鸟竹花  春风白鼓阳  野息天故山云人  一作风夫归  不见世山人  日里白花山  山思见不心  风风秋月深望云远  春山春相里  玉马鸳鸯食  白  风日在春人  春山入色来   花水上花声  山  不月有山人  春人吹树来  繁光积已寒  微风不有愁  江时草海人  山风日不春  山花草木归多不上云云  更望为云风  何见天路时  风风水不多  江年旧木深  一知云鼓颠  江山隐水人    封  一客不山衣  一夜春人心  一见空人时  归人不未穷  江风月雪稀  江风细鼓云  何为有荆山无人远人人  野里有江归  风水草中流  一日一年归  月  何日有人里  空风江水归  江云见水山  白人云路人山云  对与青林人  春花独未心  江风见路深  江云白雨愁  山风落夕山  云风过月多  高人不有人  何时出水深  云风风复人  不山云草间  东风不处来  风风入露寒  江山海雨人   旌风水此人  山人白国深山不去  白  春山不不水  云人知自多  风霞见一过风城  一日有人深  江月白山深  山山草路深  江山水白深  高山思路来   风月石中  山云白月云  不风过竹城新中路云月        风风引风水  春  春日一山城                                           风花玉兰发  青旌蕙节知  炎马时上字  寒光映中发  空云白发人  不雪有前天  空云白水稀  山山山木间云风  不见不中心  江见过人山  细雪入云山  山云草鼓城  几夜见云人雨归  山  不君不上山  不人秋雨深  客知枫白归  不日一山深  细日风心重人心  何望山云人  春人见路心  江时水水人    宿  风山有无客    云花入马风  池花汉气衣  江时山上人  一首不相人  山云落影风不人客山人  白水故山城  白水春声动上归  山风宿木来   风绿鸟红枝  春风过水深  竹门多有深人生一天远 山  山日不春人  江花白水低   风风鼓鼓微船  江为橘海稀  江山草水云云山  不为山水间  白山水客云山春  江水宿中风  山雨汉书稀  江夜草中人  江山白水间  江山过草山草不远  江风山水闲  江云江海深  山城忆上云  江帆水水山  云山不故人  江云入草深山中  风草海云山  不悟青人水山风  风云草水山  何思一上心  山风草雨间  江城见远人  野山云客时  江山不未人  江山海水深  山门日上时  山  山日一人客  春  不日不人风  江风水水云  江云水雪深  江门见未山  山风过树云山春  不有风人山  一见一花深  山日忆如山  独此寒人心  山上白时心  白见一山山人月人有时  白  一日心上月  何日不如归  一日不云深  不月不江时  山  山月见云心  云暮一山间  江见不云客  孤人忆不发  黄阁入人心  相见心心去  甘子山人心  山鸟故人山  江月上关城云风月  不见秋自月  一此人不时  自上一不人  牛衣何复时    云雨不相年  山鸟白山客草深  何人云上心  江舟白路生  云风出绶中  江云积水流  江江上不山  高君独白乡  江帆水水云  江云吹水深  白马生人过  不见南云人  不与故山归  野雨见东山  江日不云时  江思出不阳  江风不暮深  东风何路心  孤云过路行无不远人风  白日故江归  野里白人人  白日长山秋人心  不见无复春  一月不山心  江云秋水深  青山云路人                                         风风白幽碧  清色亦中发  江山古客人  云中月白山  青山山雨间  不斋见不时  山风独不归  江门入水深  不人秋雨间  青风日暮阳  山风见月归  江江隔木稀  江山积水飞    鸟  一人何未屋  暮  山山无山下人人  独里有云人  月 风云白一人  春云草水深  一山山水归  孤山江水山  云风过水稀  风云白树云  一知在故人  风山白水来  江江海水山  野林秋白愁在春  因来过路深  江山白暮人  江风过水云不山月人山  不有一乡山  云水更人心  江雨汉阳人  山  今日一相发  山花亦有心  山山独水时   风  一人见上人  江云白路深长人  山  一日日人人  野月一山深  野  不日见相深  万里人中生  一日人中心  一上春山心  云云云草间  春风日水归  风风入水稀  楚风万草稀    月  春风何已雪         春风入玉人  春风草木行  江山桂雨深  孤人见不归  江帆白客楼  山洲春水间  江秋水暮山  风心看有春  江秋草客还不云远不人  一君山远人   花水一无人  山草白云山  相山春水归  野风淮水稀不风  离人愁上山  一知秋水人  山城水水归山未人何人  昨向故人中  白色白人客山山  不为独路时  风花积月斜  春人见有年  春流日水流山人雨不山  云书入木深  青风江水深  江家云客稀  江帆去远归  春山路水深  一人江上云路人  野公抱客生山深  山风生自人  风上青云水            山风几人水  白马不人人  江人水上流  白人云水人人人  不望洞阳衣  何事有山心  山帆竹木深  江山过白归  江山带海楼   山草客来人  江风汉马阳  春风带雨间  春山草水人  一离云路过山云  不日不人衣  何日见山衣  山山天木生  风山春自心  一人春自生  自君多不时  山风日有人  相年白路深  江门汉鼓云  风门万汉人  青山汉苑城  气香霜水来人人  何日不云时  江山独水楼  云城水客深  江山思不心  柴风过夜深  相思人不时  春云雪不归  江山云海流  风风水路行  江花隔上深  江风云雨山    误  不知在人客相人  独日不人春  风色水山深  不见有中归  白  何山无不时  山山多不还  风城秋草时  孤人无有知有云月    风山入不人    草  风月过风路人心  今思风上人  山门人夜人  江花草草深  云光紫戍中  风江白水中  竹人随桂深不人  离心山不人  故君人自人  野为白人生人山  山日一人深  白雪白山生  山  谁日不不道  春风日日时  风山草路稀  江山汉木新  江山不夜山  无山风不山  不君归自人    误色白如年  清风满树深  山帘犹有山山归  白与青云山自归  高人忆白人  风江白水山  野风云海归长山  江云草雨间  江山近水云                                                          风风自天绿  江月独人多  山父风不适不深  不人何不时  君人不隐时  春山有不心  江山古水人  江山不上山长秋  江云白雨时  山山不不期  何人无路心  东山独此山  春城去水时  风风吹雪稠  黄车出海深  野与山山去    风灯出上清  山风水暮深  无年空水人  寒帆远雨稀  山碕不可归山云人月  日日风风月  谁知见不行  风事有何人  孤云入水深  白君山远人不人    寒风自如月  一子此人发  风雪夜云春  一暮春山人  白见楚山山  白事见云山  风日白山城  月上风华情  春山吹上深  春风思未归  江城宿水深  江城白雨人  风风入白深  沧风万雨归  孤城独北流   风草竹中深  不日白云人  江山江水深  山云草上山人山  谁有人山深  野  山雨思云下山风  风月宿云深  不日无山时  江云草上稀  高年独上云  春风月路深  青山不客深  山门思未多  江山日路稀  春山雨鼓流    朱光诵不枝  香花入印深  何山天水深  山风白上山  一人风水时长风  何此人人归  莫入白山人  高  江山有客深  高山见自心  江山草草多    月  一人不已雪  晨风不为月  何山独水时   风草上过    弱洒在枝时  不见不人归  野日上云人  山风万上人  江山雨上山  山城见雨人  白人江上山路人  旅马向人人  山花雨雨飞   风见有金尘  江帆出白流  云源秋路间  一寒秋鼓忧  相君见此归  风山宿水人    宿  何人无不歇  白尔不能人  山草出相心  山月一江山  一与风未心  夜云多不人  一山云水归  江江忆雨船  江云白水云  无云见草人  风山水上时不人  何君无日深  不人无路时  云花入露风    草  何人在水水云风  不日不陵人  秋夜有中时  一望山草过云云  不见不山城  山日入人城自山  江山橘水深  江山山上归  江帆草路归  江云汉雨间  山声水水深  客家归路心          春云转不人  春山日里深  白知风海深  东年不有心  江山水海云  江云见上风  春风到故云  山城山路深水归  何  风云白水生  野息吹海上山深  山月云林山    山  不月日人山  山首见山间人风水人风  江山草水归  江山月水云  何人草水山  月花云水归\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/checkpoints/i10000_l512.ckpt'\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 随着模型不断训练, 得到的语句越来越丰富, 越来越完整, 也就是说效果越来越好. 但是本质上这还是一个概率模型, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
